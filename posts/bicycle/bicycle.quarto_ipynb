{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Forecasting Bicycle Counts in Cologne\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "First, we import the modules we will use. Besides some modules from the Python standard library, we will use `polars` and `pandas` for data manipulation, `optuna` for hyperparameter optimization, and `sklearn`/`XGBoost` for the machine learning part. In the local `lookup` module, we have hardcoded some dictionaries that we will use to clean the data.\n"
      ],
      "id": "af1a091f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "import geopandas as gpd\n",
        "import humanize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "import requests\n",
        "# import shap\n",
        "\n",
        "from adjustText import adjust_text\n",
        "from geopy.geocoders import Nominatim\n",
        "from great_tables import GT\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import (mean_absolute_percentage_error,\n",
        "                             root_mean_squared_error)\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import (FunctionTransformer, MinMaxScaler,\n",
        "                                   OneHotEncoder, SplineTransformer)\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from lookup import months_lookup, names_lookup\n",
        "from postcodes import postcodes"
      ],
      "id": "9c3b4b17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading\n",
        "\n",
        "Let's define a function, that downloads the bicycle counting data from the open data portal of the city of Cologne. As the tables are not really consistent, we need to take care of the following cleaning steps:\n",
        "\n",
        "1. Ensure the German is correctly encoded. We have to rename them, because the encoding is not consistent.\n",
        "2. Replace the month names with integers to make conversion to `pl.Date` easier.\n",
        "3. Extract the year from the file name and add it as a column.\n",
        "4. Convert year and month to a `pl.Date` column.\n",
        "5. Remove rows that include yearly sums.\n",
        "6. Remove columns that have too few data points.\n",
        "7. Unpivot the data to have a long format.\n",
        "8. Cast the count column to `pl.Int64`.\n",
        "9. Drop rows with missing values. Some files even have blank rows in between.\n"
      ],
      "id": "657b4efb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def read_in(url):\n",
        "    expr = pl.col(\"month\")\n",
        "    for old, new in months_lookup.items():\n",
        "        expr = expr.str.replace_all(old, new)\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        url,\n",
        "        encoding='latin1',\n",
        "        thousands=\".\",\n",
        "        decimal=\",\",\n",
        "        sep=\";\", \n",
        "        index_col=False,\n",
        "    )\n",
        "\n",
        "    df.rename(columns={ df.columns[0]: \"month\" }, inplace = True)\n",
        "    \n",
        "    return (\n",
        "        pl.DataFrame(df)\n",
        "        # .filter(~pl.all_horizontal(pl.all().is_null()))\n",
        "        .rename(names_lookup, strict=False)\n",
        "        .filter(pl.col(\"month\") != \"Jahressumme\")\n",
        "        .with_columns(month = pl.col(\"month\").str.replace(r\"\\d+\", \"\").str.replace(\" \", \"\"))\n",
        "        .with_columns(expr)\n",
        "        .with_columns(date = pl.date(pl.lit(int(url[-9:-5])), pl.col(\"month\"), 1))\n",
        "        # .drop(\"month\", \"Hohe Pforte\", \"Neusser Straße\", strict=False)\n",
        "        .unpivot(cs.numeric(), index=\"date\", variable_name=\"location\", value_name=\"count\")\n",
        "        .cast({\"count\": pl.Int64})\n",
        "        .drop_nulls()\n",
        "    )\n",
        "\n",
        "def load_tables(urls):\n",
        "    return (\n",
        "        pl.concat([read_in(url) for url in urls], rechunk=True)\n",
        "        .with_columns(\n",
        "            location = pl.col(\"location\").cast(pl.Categorical),\n",
        "            # quarter = pl.col(\"date\").dt.quarter(),\n",
        "            # month = pl.col(\"date\").dt.month(),\n",
        "            # days_from_start = (pl.col(\"date\") - pl.col(\"date\").min()).dt.total_days(),\n",
        "            # sin_month = (pl.col(\"date\").dt.month() * 2 * math.pi / 24).sin()\n",
        "            )\n",
        "    )\n",
        "\n",
        "def get_yearly_sums(df):\n",
        "    return (\n",
        "        df\n",
        "        .with_columns(year = pl.col(\"date\").dt.year())\n",
        "        .group_by(\"location\", \"year\")\n",
        "        .agg(yearly_sum = pl.sum(\"count\"))\n",
        "        .group_by(\"location\")\n",
        "        .agg(yearly_sum_avg = pl.col(\"yearly_sum\").mean().cast(pl.Int64))\n",
        "        .sort(\"yearly_sum_avg\", descending=True)\n",
        "    )"
      ],
      "id": "fda429f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this in a list comprehension to download and concatenate all `.csv` files from the portal.\n",
        "The links are stored in a text file. So we will read it's lines first. The second part includes some feature engineering, that might get moved later on. The dataset then looks like this:\n"
      ],
      "id": "a5093b88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "working_dir_path = Path(\".\")\n",
        "links_file = working_dir_path / \"links/links.txt\"\n",
        "\n",
        "with open(links_file, \"r\") as f:\n",
        "    df = load_tables(f.readlines())\n",
        "\n",
        "yearly_sums = get_yearly_sums(df)\n",
        "\n",
        "GT(df.sample(10))"
      ],
      "id": "3f0e41d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Further Exploration\n",
        "Let's also have a look at the start dates of each location as well as the average yearly sums of bicycle counted at each location.\n"
      ],
      "id": "5788b451"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "GT(\n",
        "    (\n",
        "        df\n",
        "        .group_by(\"location\")\n",
        "        .agg(pl.min(\"date\").dt.strftime(\"%Y-%m\").alias(\"active_since\"))\n",
        "        .join(yearly_sums, on=\"location\")\n",
        "        .sort(\"active_since\")\n",
        "    )\n",
        ").cols_label(\n",
        "    location=\"Location\",\n",
        "    active_since=\"Active Since\",\n",
        "    yearly_sum_avg=\"Avg. Yearly Sum\"\n",
        ")"
      ],
      "id": "36d370e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 10))\n",
        "\n",
        "# create lollipop chart\n",
        "ax.hlines(y=yearly_sums[\"location\"], xmin=0, xmax=yearly_sums[\"yearly_sum_avg\"], color='navy')\n",
        "ax.plot(yearly_sums[\"yearly_sum_avg\"], yearly_sums[\"location\"], \"o\", markersize=8, color='navy')\n",
        "\n",
        "# set plot title and axis labels\n",
        "ax.set_title('Avg. Yearly Sums')\n",
        "ax.set_xlabel('Bicycles Counted in a Year')\n",
        "ax.set_ylabel('Location')\n",
        "\n",
        "# set y-axis limits and invert the y-axis\n",
        "ax.set_ylim(ax.get_ylim()[::-1])\n",
        "\n",
        "# hide top and right spines\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "# # add annotations for the counts\n",
        "for i, (count, loc) in enumerate(zip(yearly_sums[\"yearly_sum_avg\"], yearly_sums[\"location\"])):\n",
        "    ax.annotate(f\"{int(count):}\", xy=(count + 50000, i), va='center')\n",
        "\n",
        "# x-axis ticks\n",
        "ticks = [0, 500_000, 1_000_000, 1_500_000, 2_000_000]\n",
        "ticks_labels = [\"0\", \"500k\", \"1M\", \"1.5M\", \"2M\"]\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_xticklabels(ticks_labels, fontsize=12)\n",
        "ax.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "a9ef8085",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spotlight \"Zülpicher Straße\"\n",
        "\n",
        "Zülpicher Straße is one of the first locations equipped with a counter, and counts the second most bicycles evcery year. We will plot the data with `plotly` to make it interactive. A trendline helps understand the general development of the counts.\n"
      ],
      "id": "b2f22a9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zuelpicher = (\n",
        "    df\n",
        "    .filter(pl.col(\"location\") == \"Zülpicher Straße\").sort(\"date\")\n",
        "    .to_pandas()\n",
        ")\n",
        "\n",
        "fig = px.scatter(\n",
        "    zuelpicher, x=\"date\", y=\"count\", \n",
        "    title=\"Counts at Location Zülpicher Straße\", \n",
        "    trendline=\"ols\", \n",
        "    trendline_scope=\"overall\",\n",
        "    template=\"simple_white\")\n",
        "fig.add_scatter(x=zuelpicher[\"date\"], y=zuelpicher[\"count\"].rolling(1).mean(), mode='lines', name=\"Timeseries\")\n",
        "fig.update_xaxes(title_text='Date')\n",
        "fig.update_yaxes(title_text='Bicycles Counted')\n",
        "fig.show()"
      ],
      "id": "74f7de93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Draw Locations on a Map\n",
        "#### Scrape the Geolocation Data\n",
        "\n",
        "In order to draw the counting stations on a map, we need their locations. Since open data Cologne does not provide this information, we will use the `geopy` library to scrape the data from OpenStreetMap---specifically, we use the `Nominatim` geocoder. We will store the data in a dictionary and convert it to a `pd.DataFrame` for better handling. The call `geolocator.geocode(loc + \", Köln, Germany\")` makes a request to the Nominatim API and returns the location of the adress. So this is only approximate, as the exact counter location may differ from the address.\n"
      ],
      "id": "4234ca05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "geolocator = Nominatim(user_agent=\"myapp\")\n",
        "\n",
        "coordinates = {}\n",
        "for loc in yearly_sums[\"location\"].unique():\n",
        "    postcode = postcodes.get(loc, \"\")\n",
        "    tmp = geolocator.geocode(loc + f\",{postcode} Köln, Germany\")\n",
        "    coordinates[loc] = {\n",
        "        \"latitude\": tmp.latitude,\n",
        "        \"longitude\": tmp.longitude\n",
        "    }\n",
        "\n",
        "coord_df = pd.DataFrame(coordinates).T.reset_index().rename(columns={\"index\": \"location\"})\n",
        "\n",
        "counter_geo = pd.DataFrame(yearly_sums, columns=[\"location\", \"yearly_sums_avg\"]).merge(coord_df, on=\"location\")\n",
        "counter_geo[\"yearly_sums_avg\"] = counter_geo[\"yearly_sums_avg\"].astype(\"float64\")"
      ],
      "id": "9903a98a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting\n",
        "\n",
        "We can use the `geopandas` package to preprocess the data before plotting. The shape of Cologne is boring alone. We need the river Rhine! So we import the coordinates of the river Rhine as well as the shape of Cologne. We then calculate the total outer bounds of Cologne, which is a rectangle. We can use this to clip the coordinates of the river Rhine. We then plot the data with `matplotlib`. We will use the `adjustText` package to prevent the labels from overlapping.\n",
        "\n",
        "<!-- \n"
      ],
      "id": "895c86b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# points = gpd.GeoDataFrame(\n",
        "#     counter_geo, geometry=gpd.points_from_xy(counter_geo.longitude, counter_geo.latitude), crs=\"EPSG:4326\"\n",
        "# )\n",
        "# counter_geo[\"yearly_sums_avg\"] = counter_geo[\"yearly_sums_avg\"].astype(\"float64\")\n",
        "\n",
        "# cologne = gpd.read_file(\"data/cologne.geojson\")\n",
        "# rhine = gpd.read_file(\"data/rhine.geojson\").clip(cologne.total_bounds)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(4,4), dpi=200)\n",
        "# cologne.plot(ax=ax, color=\"white\", edgecolor=\"black\", zorder=0)\n",
        "# rhine.plot(ax=ax, zorder=1)\n",
        "# points.plot(ax=ax, color=\"darkred\", markersize=counter_geo.yearly_sums_avg * 10, zorder=2)\n",
        "\n",
        "# # add background\n",
        "# ax.set_facecolor('grey')\n",
        "\n",
        "# # remove axes\n",
        "# ax.set_yticklabels([])\n",
        "# ax.set_xticklabels([])\n",
        "# # remove tickmarks\n",
        "# ax.xaxis.set_ticks_position('none')\n",
        "# ax.yaxis.set_ticks_position('none')\n",
        "\n",
        "# # make annotation\n",
        "# texts = []\n",
        "# for x, y, label in zip(points.geometry.x, points.geometry.y, points[\"location\"]):\n",
        "#     texts.append(ax.text(x, y, label, color='black', fontsize=3))\n",
        "\n",
        "# # place text in the upper right corner of the plot\n",
        "# ax.text(0.99, 0.99, 'Cologne Bicycle Counter', verticalalignment='top', horizontalalignment='right',\n",
        "#         transform=ax.transAxes,\n",
        "#         color='black', fontsize=8)\n",
        "# adjust_text(texts, \n",
        "#             arrowprops=dict(arrowstyle=\"->\",\n",
        "#             color='r', \n",
        "#             lw=0.5), ax=ax)\n",
        "# plt.show()"
      ],
      "id": "84490f24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-->\n",
        "\n",
        "#### With Sliders\n"
      ],
      "id": "13abae07"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coord_pl = pl.DataFrame(coord_df)\n",
        "lon_mean = coord_pl[\"longitude\"].mean()\n",
        "lat_mean = coord_pl[\"latitude\"].mean()\n",
        "\n",
        "date_geo_df = (\n",
        "    df\n",
        "    .join(\n",
        "        coord_pl.with_columns(pl.col(\"location\").cast(pl.Categorical)),\n",
        "        on=\"location\", \n",
        "        how=\"left\"\n",
        "        )\n",
        "    .with_columns(\n",
        "        norm_count = 25 * pl.col(\"count\") / pl.col(\"count\").max(),\n",
        "        human_count = pl.col(\"count\").map_elements(lambda s: humanize.intword(s), return_dtype=pl.String)\n",
        "    )\n",
        "    .select(pl.exclude(\"month\", \"quarter\", \"days_from_start\", \"sin_month\"))\n",
        "    .sort(\"date\")\n",
        ")\n",
        "\n",
        "# csv_file = Path(\"data\") / \"date_geo.csv\"\n",
        "# if csv_file.exists():\n",
        "#     date_geo_df.write_csv(csv_file)\n",
        "\n",
        "n_dates = len(date_geo_df[\"date\"].unique())\n",
        "\n",
        "fig = go.Figure()\n",
        "mask = [False] * n_dates\n",
        "\n",
        "steps = []\n",
        "for i, (date, data) in enumerate(date_geo_df.group_by(\"date\")):\n",
        "\n",
        "    date = date[0].strftime(\"%Y-%m\")\n",
        "    mask = [j == i for j in range(n_dates)]\n",
        "\n",
        "    trace = go.Scattermapbox(\n",
        "        lon=data[\"longitude\"],\n",
        "        lat=data[\"latitude\"],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            color='darkred',\n",
        "            size=data['norm_count'],\n",
        "            sizemode='area',\n",
        "            sizeref=0.025,\n",
        "            sizemin=5\n",
        "        ),\n",
        "        hoverinfo='text',\n",
        "        hovertext=[f\"Location: {loc}<br>Date: {date}<br>Count: {val}\" \n",
        "                   for loc, val in zip(data['location'], data['human_count'])],\n",
        "        visible=(i == 0),  # Only first trace visible by default\n",
        "        showlegend=False\n",
        "    )\n",
        "    fig.add_trace(trace)\n",
        "    \n",
        "    # Create slider step\n",
        "    step = dict(\n",
        "        method='update',\n",
        "        args=[{'visible': mask},\n",
        "              {'title': f'Cologne Bicycle Counter - {date}'}],\n",
        "        label=date\n",
        "    )\n",
        "    steps.append(step)\n",
        "\n",
        "# Configure slider\n",
        "sliders = [dict(\n",
        "    active=0,\n",
        "    currentvalue={\"prefix\": \"Date: \"},\n",
        "    pad={\"t\": 10},\n",
        "    ticklen=0,\n",
        "    minorticklen=0,\n",
        "    transition={\n",
        "        'duration': 10000,\n",
        "        'easing': 'circle-in-out'},\n",
        "    steps=steps\n",
        ")]\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    mapbox_style=\"carto-positron\", #\"open-street-map\",\n",
        "    mapbox=dict(\n",
        "        center=dict(\n",
        "            lon=lon_mean, \n",
        "            lat=lat_mean\n",
        "        ),\n",
        "        zoom=11\n",
        "    ),\n",
        "    title='Cologne Bicycle Counter',\n",
        "    height=600,\n",
        "    width=600,\n",
        "    margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
        "    sliders=sliders\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "id": "cadaae73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Incorporating Weather Features\n",
        "\n",
        "We will scrape the weather data from the web and extract the files necessary. The data is then loaded into a `pl.DataFrame` and cleaned. We mainly do some renaming and type casting. Lastly, we deselect some columns that are not needed.\n"
      ],
      "id": "ac83975b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "weather_url = Path(\"links/weather.txt\").read_text()\n",
        "data_path = Path(\"data\")\n",
        "zip_path = data_path / \"weather.zip\"\n",
        "data_path.mkdir(exist_ok=True)\n",
        "\n",
        "response = requests.get(weather_url)\n",
        "with open(zip_path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(data_path)\n",
        "\n",
        "weather_csv = [os.path.join(data_path, f) for f in os.listdir(data_path) if 'produkt' in f][0]\n",
        "\n",
        "weather_df = (\n",
        "    pl.read_csv(\n",
        "        weather_csv, \n",
        "        separator=\";\", \n",
        "        has_header=True,\n",
        "        schema=pl.Schema(\n",
        "            {\n",
        "                \"station_id\": pl.Int64,\n",
        "                \"date_begin\": pl.String,\n",
        "                \"date_end\": pl.Int64,\n",
        "                \"qn_4\": pl.Int64,\n",
        "                \"sky_cov\": pl.Float64,\n",
        "                \"air_temp_daymean_month_mean\": pl.Float64,\n",
        "                \"air_temp_daymax_month_mean\": pl.Float64,\n",
        "                \"air_temp_daymin_month_mean\": pl.Float64,\n",
        "                \"air_temp_daymax_month_max\": pl.Float64,\n",
        "                \"air_temp_daymin_month_min\": pl.Float64,\n",
        "                \"wind_speed_month_mean\": pl.Float64,  # MO_FK\n",
        "                \"wind_speed_daymax_month_max\": pl.Float64,  # MX_FX\n",
        "                \"sunshine_duration\": pl.Float64,  # MO_SD_S\n",
        "                \"QN_6\": pl.Int64,\n",
        "                \"precipitation_month_sum\": pl.Float64,\n",
        "                \"precipitation_daymax_month_max\": pl.Float64,\n",
        "                \"eor\": pl.String\n",
        "            }\n",
        "        )\n",
        "        )\n",
        "        .with_columns(\n",
        "            date = pl.col(\"date_begin\").str.to_date(format=\"%Y%m%d\")\n",
        "        )\n",
        "        .select(pl.col(\"date\"), cs.numeric())\n",
        "        .select(cs.exclude(\"qn_4\", \"eor\", \"QN_6\", \"station_id\", \"date_end\"))\n",
        ")\n",
        "\n",
        "GT(weather_df.sample(10))"
      ],
      "id": "14bd9f7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "weather_detail = (\n",
        "    weather_df\n",
        "    .filter(pl.col(\"date\") >= zuelpicher[\"date\"].min())\n",
        "    .filter(pl.col(\"date\") <= zuelpicher[\"date\"].max())\n",
        "    .to_pandas()\n",
        ")\n",
        "\n",
        "fig = px.scatter(\n",
        "    weather_detail, x=\"date\", y=\"air_temp_daymean_month_mean\", \n",
        "    title=\"Daily Average Air Temperature (Monthly Mean) vs. Date\", \n",
        "    trendline=\"ols\", \n",
        "    trendline_scope=\"overall\",\n",
        "    template=\"simple_white\")\n",
        "fig.add_scatter(x=weather_detail[\"date\"], y=weather_detail[\"air_temp_daymean_month_mean\"].rolling(1).mean(), mode='lines', name=\"Timeseries\")\n",
        "# axes titles\n",
        "fig.update_xaxes(title_text='Date')\n",
        "fig.update_yaxes(title_text='Air Temp. [°C]')\n",
        "fig.show()"
      ],
      "id": "9f39efc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merging the Data\n",
        "\n",
        "We merge the weather data with the bicycle counts. We will use a left join, because we want to keep all the bicycle counts. We will also drop the `date` column, because it is not needed anymore.\n",
        "In a second step, we will split the data into a training and a test set. The data from year 2022\n",
        "will used for testing, only. We can split the data by filtering the `date` column. We call `to_pandas` to convert the data to a `pd.DataFrame` which is used in `sklearn`.\n"
      ],
      "id": "41c5957d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_data = df.join(weather_df, on=\"date\", how=\"left\")\n",
        "train_data = ml_data.filter(pl.col(\"date\") < datetime(2022, 1, 1))\n",
        "test_data = ml_data.filter(pl.col(\"date\") >= datetime(2022, 1, 1))"
      ],
      "id": "be7b64d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training\n",
        "\n",
        "Next up is the fun part: Actual Machine Learning! In this projectm, we will use the XGBoost regression model. As a tuning library, we will use Optuna. We will define a function that will be called by Optuna to optimize the hyperparameters of the model. We will use the `TimeSeriesSplit` to split the data into training and validation sets. The function will return the mean of the cross-validated RMSE.\n"
      ],
      "id": "77485a23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ts_cv = TimeSeriesSplit(\n",
        "    n_splits=5,\n",
        "    gap=2,\n",
        "    test_size=6\n",
        "    )\n",
        "\n",
        "class QuarterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, date_column='date'):\n",
        "        self.date_column = date_column\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X = pd.DataFrame(X)\n",
        "        dates = X[self.date_column]\n",
        "        dates = pd.to_datetime(dates)\n",
        "\n",
        "        quarter_columns = []\n",
        "        quarter_conditions = [\n",
        "            (dates.dt.month <= 3),\n",
        "            ((dates.dt.month > 3) & (dates.dt.month <= 6)),\n",
        "            ((dates.dt.month > 6) & (dates.dt.month <= 9)),\n",
        "            (dates.dt.month > 9)\n",
        "        ]\n",
        "        quarter_names = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "        \n",
        "        for name, condition in zip(quarter_names, quarter_conditions):\n",
        "            quarter_columns.append(condition.astype(int))\n",
        "\n",
        "        # Combine all new features\n",
        "        new_features = np.column_stack(quarter_columns)\n",
        "        \n",
        "\n",
        "        return new_features\n",
        "    \n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "\n",
        "\n",
        "class MonthTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, date_column='date'):\n",
        "        self.date_column = date_column\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        if isinstance(X, np.ndarray):\n",
        "            X = pd.DataFrame(X)\n",
        "        dates = X[self.date_column]\n",
        "        dates = pd.to_datetime(dates)\n",
        "\n",
        "        month_column = dates.dt.month\n",
        "        \n",
        "        # Combine all new features\n",
        "        new_features = month_column.to_numpy().reshape(-1, 1)\n",
        "        \n",
        "        return new_features\n",
        "    \n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return ['month']\n",
        "\n",
        "\n",
        "def sin_transformer(period=24):\n",
        "    return FunctionTransformer(lambda x: np.sin(x * 2 * np.pi / period))\n",
        "\n",
        "\n",
        "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
        "    if n_splines is None:\n",
        "        n_splines = period\n",
        "    n_knots = n_splines + 1  # periodic and include_bias is True\n",
        "    return SplineTransformer(\n",
        "        degree=degree,\n",
        "        n_knots=n_knots,\n",
        "        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
        "        extrapolation=\"periodic\",\n",
        "        include_bias=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_feature_transformer(X):\n",
        "    numerical_columns = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    spline_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            ('month_transformer', MonthTransformer()),\n",
        "            ('cyclic_hour', periodic_spline_transformer(12, n_splines=6))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    sin_pipeline = Pipeline(\n",
        "        steps=[\n",
        "            ('month_transformer', MonthTransformer()),\n",
        "            ('sine_transformer', sin_transformer(24))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    col_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('quarter_transformer', QuarterTransformer(), ['date']),\n",
        "            ('month_transformer', MonthTransformer(), ['date']),\n",
        "            ('sin_transformer', sin_pipeline, ['date']),\n",
        "            ('spline_transformer', spline_pipeline, ['date']),\n",
        "            ('num_scale', MinMaxScaler(), numerical_columns),\n",
        "            ('cat_encode', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
        "        ],\n",
        "        remainder=\"passthrough\",\n",
        "        verbose_feature_names_out=False\n",
        "    )\n",
        "    return col_transformer\n",
        "\n",
        "\n",
        "def objective(trial, X_train, y_train):\n",
        "\n",
        "    numerical_columns = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "    col_transformer = make_feature_transformer(X_train)\n",
        "\n",
        "    xgbr = XGBRegressor(\n",
        "            n_estimators=2000,\n",
        "            # n_estimators=trial.suggest_int(\"n_estimators\", 100, 2000),\n",
        "            learning_rate=trial.suggest_float(\"eta\", 0.0001, 1),\n",
        "            gamma=trial.suggest_int('gamma', 0, 1000),\n",
        "            max_depth=trial.suggest_int(\"max_depth\", 1, 50),\n",
        "            min_child_weight=trial.suggest_int('min_child_weight', 0, 100),\n",
        "            max_delta_step=trial.suggest_int('max_delta_step', 0, 100),\n",
        "            subsample=trial.suggest_float('subsample', 0, 1),\n",
        "            colsample_bytree=trial.suggest_float('colsample_bytree', 0, 1),\n",
        "            reg_alpha=trial.suggest_int('reg_alpha', 0, 1000),\n",
        "            reg_lambda=trial.suggest_int('reg_lambda', 0, 1000),\n",
        "            enable_categorical=True,\n",
        "            random_state=42,\n",
        "        )\n",
        "\n",
        "    pipeline = make_pipeline(col_transformer, xgbr)\n",
        "\n",
        "    cv_score = cross_val_score(\n",
        "        pipeline,\n",
        "        X=X_train,\n",
        "        y=y_train,\n",
        "        cv=ts_cv,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "    )\n",
        "\n",
        "    return -cv_score.mean()\n",
        "\n",
        "class BicyclePredictor():\n",
        "    def __init__(self, fixed_params=None, cache_dir='optimization_cache'):\n",
        "        if fixed_params is None:\n",
        "            fixed_params = {\n",
        "                \"n_estimators\": 2000,\n",
        "                \"enable_categorical\": True,\n",
        "                \"random_state\": 42\n",
        "            }\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        self.fixed_params = fixed_params\n",
        "        self.is_fitted = False\n",
        "    \n",
        "    def set_objective(self, objective):\n",
        "        self.objective = objective\n",
        "\n",
        "    def set_fixed_params(self, fixed_params):\n",
        "        self.fixed_params = fixed_params\n",
        "\n",
        "    def set_studies(self, studies):\n",
        "        self.studies = studies\n",
        "\n",
        "    def _make_writable_string(self, location):\n",
        "        return (\n",
        "            location\n",
        "            .lower()\n",
        "            .replace(\".\", \"\")\n",
        "            .replace(\" \", \"_\")\n",
        "            .replace(\"ü\", \"ue\")\n",
        "            .replace(\"ö\", \"oe\")\n",
        "            .replace(\"ä\", \"ae\")\n",
        "            .replace(\"ß\", \"ss\")\n",
        "        )\n",
        "\n",
        "    def _get_cache_path(self, location):\n",
        "        \"\"\"Generate a cache file path for a specific location.\"\"\"\n",
        "        return os.path.join(self.cache_dir, f\"{location}_study_cache.json\")\n",
        "    \n",
        "    def _save_study_cache(self, location, study_data):\n",
        "        \"\"\"Save study results to a JSON cache file.\"\"\"\n",
        "        cache_path = self._get_cache_path(location)\n",
        "        with open(cache_path, 'w') as f:\n",
        "            json.dump(study_data, f)\n",
        "    \n",
        "    def _load_study_cache(self, location):\n",
        "        \"\"\"Load cached study results for a location.\"\"\"\n",
        "        cache_path = self._get_cache_path(location)\n",
        "        if os.path.exists(cache_path):\n",
        "            with open(cache_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return None\n",
        "\n",
        "    def _save_failed_cache(self, failed_studies):\n",
        "        \"\"\"Save failed studies to a JSON cache file.\"\"\"\n",
        "        cache_path = os.path.join(self.cache_dir, \"failed_studies.json\")\n",
        "        with open(cache_path, 'w') as f:\n",
        "            json.dump(failed_studies, f)\n",
        "    \n",
        "    def _load_failed_cache(self):\n",
        "        \"\"\"Load failed studies from a JSON cache file.\"\"\"\n",
        "        cache_path = os.path.join(self.cache_dir, \"failed_studies.json\")\n",
        "        if os.path.exists(cache_path):\n",
        "            with open(cache_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def tune(self, train_data, n_trials=200, use_stored=False):\n",
        "        metrics = {}\n",
        "        succesful_studies = {}\n",
        "        failed_studies = {} if not use_stored else self._load_failed_cache()\n",
        "\n",
        "        for location, group in train_data.group_by('location'):\n",
        "            location = location[0]\n",
        "\n",
        "            if location in failed_studies.keys():\n",
        "                continue\n",
        "            \n",
        "            loc_str = self._make_writable_string(location)\n",
        "            # Check if we can use stored results\n",
        "            if use_stored:\n",
        "                cached_study = self._load_study_cache(loc_str)\n",
        "                if cached_study:\n",
        "                    inner_dict = cached_study[location]\n",
        "                    succesful_studies[location] = inner_dict['best_params']\n",
        "                    metrics[location] = inner_dict['best_value']\n",
        "                    continue\n",
        "            \n",
        "            X_train = group.drop(\"count\").to_pandas()\n",
        "            y_train = group[\"count\"]\n",
        "            objective_partial = lambda trial: self.objective(trial, X_train, y_train)\n",
        "            study = optuna.create_study(direction=\"minimize\")\n",
        "            \n",
        "            try:\n",
        "                study.optimize(objective_partial, n_trials=n_trials)\n",
        "                succesful_studies[location] = study.best_trial.params\n",
        "                metrics[location] = study.best_trial.value\n",
        "\n",
        "                self._save_study_cache(\n",
        "                    loc_str, \n",
        "                    {\n",
        "                        location:\n",
        "                        {\n",
        "                        'best_params': study.best_trial.params,\n",
        "                        'best_value': study.best_trial.value\n",
        "                        }\n",
        "                    }\n",
        "                    )\n",
        "            except ValueError as e:\n",
        "                failed_studies[location] = str(e)\n",
        "\n",
        "        self.succesful_studies = succesful_studies\n",
        "        self.metrics = metrics\n",
        "\n",
        "        # find location with the min mape in the dict\n",
        "        self.best_location = pd.Series(metrics).idxmin()\n",
        "        best_params = succesful_studies[self.best_location]\n",
        "\n",
        "        for location in failed_studies.keys():\n",
        "            self.succesful_studies[location] = best_params\n",
        "            self._save_study_cache(\n",
        "                self._make_writable_string(location),\n",
        "                {\n",
        "                    location: {\n",
        "                        'best_params': best_params,\n",
        "                        'best_value': metrics[self.best_location]\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "        self.failed_studies = failed_studies\n",
        "        self._save_failed_cache(failed_studies)\n",
        "\n",
        "\n",
        "    def _refit_inner(self, train_data, location):\n",
        "        X_train = train_data.drop(\"count\").to_pandas()\n",
        "        y_train = train_data[\"count\"]\n",
        "\n",
        "        pipe = make_pipeline(\n",
        "            make_feature_transformer(X_train),\n",
        "            XGBRegressor(\n",
        "                **self.succesful_studies[location],\n",
        "                **self.fixed_params\n",
        "            )\n",
        "        )\n",
        "        return pipe.fit(X_train, y_train)\n",
        "\n",
        "    def refit(self, train_data):\n",
        "        # first refit the baseline model\n",
        "        self._baseline_pipe = self._refit_inner(train_data, self.best_location)\n",
        "        self._pipeline_dict = {}\n",
        "        # fit location specific models\n",
        "        for location, data in self.succesful_studies.items():\n",
        "            self._pipeline_dict[location] = self._refit_inner(\n",
        "                train_data=train_data.filter(pl.col(\"location\") == location),\n",
        "                location=location\n",
        "            )\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def check_fitted(self):\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model has not been fitted yet.\")\n",
        "        return True\n",
        "\n",
        "    def predict(self, X_test, loc_str=\"\"):\n",
        "        self.check_fitted()\n",
        "        # return the specific model when available, otherwise the baseline model\n",
        "        # and make predictions\n",
        "        return self._pipeline_dict.get(loc_str, self._baseline_pipe).predict(X_test)\n",
        "\n",
        "    def eval(self, test_data):\n",
        "        self.check_fitted()\n",
        "        metrics_by_location = {}\n",
        "        for location, group in test_data.group_by('location'):\n",
        "            location = location[0]\n",
        "            X_test = group.drop(\"count\").to_pandas()\n",
        "            y_test = group[\"count\"]\n",
        "            y_pred = self.predict(X_test, location)\n",
        "\n",
        "            metrics_by_location[location] = {\n",
        "                \"rmse\": np.round(root_mean_squared_error(y_test, y_pred)),\n",
        "                \"mape\": np.round(mean_absolute_percentage_error(y_test, y_pred), 2)\n",
        "            }\n",
        "\n",
        "        return (\n",
        "            pl.DataFrame(\n",
        "                pd.DataFrame(metrics_by_location).T\n",
        "                .reset_index()\n",
        "                .rename(columns={\"level_0\": \"location\", \"index\": \"location\"})\n",
        "            )\n",
        "            .with_columns(\n",
        "                pl.col(\"location\").is_in(self.succesful_studies.keys()).not_().alias(\"baseline\"),\n",
        "                pl.col(\"location\").cast(pl.Categorical).alias(\"location\"),\n",
        "                pl.col(\"rmse\").cast(pl.Int64).alias(\"rmse\"),\n",
        "            )\n",
        "            .join(\n",
        "                test_data\n",
        "                .group_by(\"location\")\n",
        "                .agg(pl.col(\"count\").mean().cast(pl.Int64).alias(\"count\"))\n",
        "                .filter(pl.col(\"location\").is_in(metrics_by_location.keys()))\n",
        "                , on=\"location\"\n",
        "                )\n",
        "            .sort(\"mape\")\n",
        "        )\n",
        "\n",
        "    def get_pipeline(self, loc_str):\n",
        "        return self._pipeline_dict.get(loc_str, self._baseline_pipe)\n",
        "    \n",
        "    def plot_predictions(self, data, loc_str):\n",
        "        import os\n",
        "\n",
        "        if not os.path.exists(\"images\"):\n",
        "            os.mkdir(\"images\")\n",
        "\n",
        "        self.check_fitted()\n",
        "        X_test = data.filter(data[\"location\"] == loc_str).drop(\"count\").to_pandas()\n",
        "        y_test = data.filter(data[\"location\"] == loc_str)[\"count\"].to_pandas()\n",
        "        y_pred = self.predict(X_test, loc_str)\n",
        "\n",
        "        combined = pd.DataFrame({\"date\": X_test[\"date\"], \"count\": y_test, \"pred\": y_pred})\n",
        "\n",
        "        fig = px.line(\n",
        "            combined, x=\"date\", y=[\"count\", \"pred\"], \n",
        "            title=f\"Counter {loc_str}\", \n",
        "            template=\"simple_white\")\n",
        "        fig.update_xaxes(title_text='Date')\n",
        "        fig.update_yaxes(title_text='Count')\n",
        "        # fig.write_image(f\"images/{loc_str}.png\")\n",
        "        fig.show()\n",
        "\n",
        "\n",
        "\n",
        "# little test case to see if the pipeline works\n",
        "X_train = train_data.filter(train_data[\"location\"] == \"Zülpicher Straße\").drop(\"count\").to_pandas()\n",
        "y_train = train_data.filter(train_data[\"location\"] == \"Zülpicher Straße\")[\"count\"]\n",
        "\n",
        "trafo = make_feature_transformer(X_train)\n",
        "xgbr = XGBRegressor(**{\n",
        "    'eta': 0.27979397187537103,\n",
        "    'gamma': 210,\n",
        "    'max_depth': 14,\n",
        "    'min_child_weight': 43,\n",
        "    'max_delta_step': 22,\n",
        "    'subsample': 0.5140509086572217,\n",
        "    'colsample_bytree': 0.022917694406145328,\n",
        "    'reg_alpha': 961,\n",
        "    'reg_lambda': 184\n",
        "    },\n",
        "   random_state=42)\n",
        "pipeline = make_pipeline(trafo, xgbr)\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "id": "f352c25b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning/Training the Pipeline\n"
      ],
      "id": "3a8efc94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_pipeline = BicyclePredictor()\n",
        "ml_pipeline.set_objective(objective)\n",
        "ml_pipeline.set_fixed_params({\n",
        "    \"n_estimators\": 2000,\n",
        "    \"enable_categorical\": True,\n",
        "    \"random_state\": 42\n",
        "})\n",
        "ml_pipeline.tune(train_data, n_trials=200, use_stored=True)\n",
        "ml_pipeline.refit(train_data)"
      ],
      "id": "ce94a880",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation on Holdout Data\n"
      ],
      "id": "c9494e91"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics_df = ml_pipeline.eval(test_data)\n",
        "\n",
        "(\n",
        "    GT(metrics_df)\n",
        "    .tab_header(\"Metrics on Holdout Data\")\n",
        "    .cols_label(location=\"Location\", rmse=\"RMSE\", mape=\"MAPE\", count=\"Avg. Monthly Count\")\n",
        "    .cols_move(\"count\", after=\"location\")\n",
        ")"
      ],
      "id": "f04022d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot the Predictions vs Actual Counts\n"
      ],
      "id": "e0ba0930"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_pipeline.plot_predictions(test_data, \"Zülpicher Straße\")\n",
        "ml_pipeline.plot_predictions(test_data, \"Bonner Straße\")\n",
        "ml_pipeline.plot_predictions(test_data, \"Severinsbrücke\")\n",
        "ml_pipeline.plot_predictions(test_data, \"Hohe Pforte\")\n",
        "ml_pipeline.plot_predictions(test_data, \"Venloer Straße\")\n",
        "\n",
        "# def shap_single_loc(loc_str, data, pipeline_dict):\n",
        "#     data = data.filter(data[\"location\"] == loc_str)\n",
        "#     features = data.drop(\"count\").to_pandas()\n",
        "#     data = (\n",
        "#         data\n",
        "#         .with_columns(\n",
        "#             pl.Series(pipeline_dict[loc_str].predict(features)).alias(\"pred\")\n",
        "#         )\n",
        "#         .sort(\"date\")\n",
        "#     )\n",
        "\n",
        "#     pipe = pipeline_dict[loc_str]\n",
        "\n",
        "#     pred = model.predict(features, output_margin=True)\n",
        "\n",
        "#     explainer = shap.TreeExplainer(pipe[\"xgbregressor\"])\n",
        "#     explanation = explainer(pipe[\"columntransformer\"].transform(features))\n",
        "\n",
        "#     shap_values = explanation.values\n",
        "#     # make sure the SHAP values add up to marginal predictions\n",
        "#     print(f\"Max(Abs(ShapValues + BaseValues - Preds)) = {np.abs(shap_values.sum(axis=1) + explanation.base_values - pred).max()}\")\n",
        "#     shap.plots.beeswarm(explanation)\n",
        "\n",
        "# shap_single_loc(\"Zülpicher Straße\", test_data, pipeline_dict)\n",
        "# shap_single_loc(\"Bonner Straße\", test_data, pipeline_dict)\n",
        "# shap_single_loc(\"Severinsbrücke\", test_data, pipeline_dict)"
      ],
      "id": "d8ab679b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "D:\\BicycleCountingPolars\\bicycle_website\\.venv\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
@software{westphal_cases_2023,
	title = {cases: Stratified Evaluation of Subgroup Classification Accuracy},
	rights = {{MIT} + file {LICENSE}},
	url = {https://doi.org/10.32614/CRAN.package.cases},
	shorttitle = {cases},
	abstract = {Enables simultaneous statistical inference for the accuracy of multiple classifiers in multiple subgroups (strata). For instance, allows to perform multiple comparisons in diagnostic accuracy studies with co-primary endpoints sensitivity and specificity. (Westphal, Max, and Antonia Zapf. (2021). "Statistical Inference for Diagnostic Test Accuracy Studies with Multiple Comparisons." {\textless}doi:10.48550/{arXiv}.2105.13469{\textgreater}.)},
	version = {0.1.1},
	author = {Westphal, Max},
	urldate = {2024-08-07},
	date = {2023-05-18},
}

@article{qu_return_baseline_2022,
	title = {Return-to-baseline multiple imputation for missing values in clinical trials},
	volume = {21},
	rights = {© 2022 John Wiley \& Sons Ltd.},
	issn = {1539-1612},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2191},
	doi = {10.1002/pst.2191},
	abstract = {Return-to-baseline is an important method to impute missing values or unobserved potential outcomes when certain hypothetical strategies are used to handle intercurrent events in clinical trials. Current return-to-baseline approaches seen in literature and in practice inflate the variability of the “complete” dataset after imputation and lead to biased mean estimators when the probability of missingness depends on the observed baseline and/or postbaseline intermediate outcomes. In this article, we first provide a set of criteria a return-to-baseline imputation method should satisfy. Under this framework, we propose a novel return-to-baseline imputation method. Simulations show the completed data after the new imputation approach have the proper distribution, and the estimators based on the new imputation method outperform the traditional method in terms of both bias and variance, when missingness depends on the observed values. The new method can be implemented easily with the existing multiple imputation procedures in commonly used statistical packages.},
	pages = {641--653},
	number = {3},
	journaltitle = {Pharmaceutical Statistics},
	author = {Qu, Yongming and Dai, Biyue},
	urldate = {2024-08-06},
	date = {2022},
	langid = {english},
	keywords = {baseline observation carried forward, direct maximum likelihood estimation, estimand, ignorable missingness},
}

@article{zhang_missing_2022,
	title = {Missing Data Imputation With Baseline Information in Longitudinal Clinical Trials},
	volume = {14},
	issn = {null},
	url = {https://doi.org/10.1080/19466315.2020.1845234},
	doi = {10.1080/19466315.2020.1845234},
	abstract = {In longitudinal clinical trials, missing data are inevitable despite every effort made to retain patients in the trial. Missing data cause difficulty in the estimation and interpretation of the treatment effect. When the primary objective is to assess the treatment effect in a realistic setting, it is necessary to take into consideration the impact of noncompliance to the treatment regimen. For estimation following the intention-to-treat principle, a return-to-baseline ({RTB}) approach may be used for continuous endpoints in some longitudinal clinical trials. The {RTB} approach is based on the assumption that the unobserved outcomes at the end of the trial represent a return to the baseline value, that is, any change observed while on treatment can be expected to wash out after patients drop out. This article describes a statistical approach for {RTB} analyses. The method for calculating the sample size using {RTB} approach is presented. A detailed illustration using this {RTB} approach based on publicly available longitudinal antidepressant clinical trial data is provided. Extensive simulations are presented to evaluate the performance of this {RTB} approach under various missing data mechanisms. Important limitations regarding the appropriateness of the underlying assumptions of {RTB} are discussed.},
	pages = {242--248},
	number = {2},
	journaltitle = {Statistics in Biopharmaceutical Research},
	author = {Zhang, Yilong and Zimmer, Zachary and Xu, Lei and Lam, Raymond L. H. and Huyck, Susan and Golm, Gregory},
	urldate = {2024-08-06},
	date = {2022-04-03},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/19466315.2020.1845234},
	keywords = {Baseline observation carried forward, Longitudinal data, Multiple imputation, Return to baseline},
}


@article{royall_conditional_1985,
	title = {Conditional Coverage Properties of Finite Population Confidence Intervals},
	volume = {80},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1985.10478122},
	doi = {10.1080/01621459.1985.10478122},
	abstract = {Standardized errors (Ť — T)/v 1/2 were calculated for both ratio and regression estimators in each of 10,000 simple random samples of n = 32 from each of six populations, using four different variance estimators. Graphs show how the percentage of intervals T ± 1.96 v 1/2 that fail to contain T changes as a function of the average value of the auxiliary variable in the sample. They reveal that (a) intervals using the variance estimators from standard linear regression theory were hopelessly unreliable, (b) intervals using the conventional finite population variance estimators showed a striking excess of failures in badly balanced samples, and (c) none of the four variance estimators produced satisfactory confidence intervals in populations arising from badly skewed prediction models.},
	pages = {355--359},
	number = {390},
	journaltitle = {Journal of the American Statistical Association},
	author = {Royall, Richard M. and Cumberland, William G.},
	urldate = {2024-08-06},
	date = {1985-06-01},
	keywords = {Balanced samples, Conditional inference, Confidence interval, Prediction, Robustness, Standard error, Variance estimate},
}

@article{burch_estimating_2014,
	title = {Estimating kurtosis and confidence intervals for the variance under nonnormality},
	volume = {84},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2013.840628},
	doi = {10.1080/00949655.2013.840628},
	abstract = {Exact confidence intervals for variances rely on normal distribution assumptions. Alternatively, large-sample confidence intervals for the variance can be attained if one estimates the kurtosis of the underlying distribution. The method used to estimate the kurtosis has a direct impact on the performance of the interval and thus the quality of statistical inferences. In this paper the author considers a number of kurtosis estimators combined with large-sample theory to construct approximate confidence intervals for the variance. In addition, a nonparametric bootstrap resampling procedure is used to build bootstrap confidence intervals for the variance. Simulated coverage probabilities using different confidence interval methods are computed for a variety of sample sizes and distributions. A modification to a conventional estimator of the kurtosis, in conjunction with adjustments to the mean and variance of the asymptotic distribution of a function of the sample variance, improves the resulting coverage values for leptokurtically distributed populations.},
	pages = {2710--2720},
	number = {12},
	journaltitle = {Journal of Statistical Computation and Simulation},
	author = {Burch, Brent D.},
	urldate = {2024-08-06},
	date = {2014-12-02},
	keywords = {62F10, 62F12, 62F25, bootstrap, interval estimation, large-sample theory},
}


@article{saravanan_application_2020,
	title = {Application of the hierarchical bootstrap to multi-level data in neuroscience},
	volume = {3},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7906290/},
	number = {5},
	journaltitle = {Neurons, behavior, data analysis and theory},
	shortjournal = {Neuron Behav Data Anal Theory},
	author = {Saravanan, Varun and Berman, Gordon J and Sober, Samuel J},
	urldate = {2024-07-31},
	date = {2020},
	pmid = {33644783},
	pmcid = {PMC7906290},
}

@article{probst_tunability_2019,
	title = {Tunability: Importance of Hyperparameters of Machine Learning Algorithms},
	volume = {20},
	url = {http://jmlr.org/papers/v20/18-444.html},
	pages = {1--32},
	number = {53},
	journaltitle = {Journal of Machine Learning Research},
	author = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
	date = {2019},
}

@article{hanley_sampling_1997,
	title = {Sampling variability of nonparametric estimates of the areas under receiver operating characteristic curves: An update},
	volume = {4},
	issn = {1076-6332},
	url = {https://www.sciencedirect.com/science/article/pii/S1076633297801614},
	doi = {10.1016/S1076-6332(97)80161-4},
	shorttitle = {Sampling variability of nonparametric estimates of the areas under receiver operating characteristic curves},
	pages = {49--58},
	number = {1},
	journaltitle = {Academic Radiology},
	shortjournal = {Academic Radiology},
	author = {Hanley, James A. and Hajian-Tilaki, Karim O.},
	urldate = {2024-08-01},
	date = {1997-01-01},
	keywords = {Nonparametric {ROC} analysis, area under the curve, {DeLong} method, jackknife pseudovalues},
}

@article{burch_nonparametric_2012,
	title = {Nonparametric Bootstrap Confidence Intervals for Variance Components Applied to Interlaboratory Comparisons},
	volume = {17},
	issn = {1537-2693},
	url = {https://doi.org/10.1007/s13253-012-0087-9},
	doi = {10.1007/s13253-012-0087-9},
	pages = {228--245},
	number = {2},
	journaltitle = {Journal of Agricultural, Biological, and Environmental Statistics},
	shortjournal = {{JABES}},
	author = {Burch, Brent D.},
	urldate = {2024-07-31},
	date = {2012-06-01},
	langid = {english},
	keywords = {Bootstrap {BC}
                  a
                 method, Bootstrap standard method, One-way random effects model, Small-sample adjustment},
}

@article{braschel_comparison_2016,
	title = {A comparison of confidence interval methods for the intraclass correlation coefficient in community-based cluster randomization trials with a binary outcome},
	volume = {13},
	issn = {1740-7745},
	url = {https://doi.org/10.1177/1740774515606377},
	doi = {10.1177/1740774515606377},
	pages = {180--187},
	number = {2},
	journaltitle = {Clinical Trials},
	shortjournal = {Clinical Trials},
	author = {Braschel, Melissa C and Svec, Ivana and Darlington, Gerarda A and Donner, Allan},
	urldate = {2024-07-31},
	date = {2016-04-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
}

@article{field_bootstrapping_2007,
	title = {Bootstrapping Clustered Data},
	volume = {69},
	issn = {1369-7412},
	url = {https://www.jstor.org/stable/4623274},
	pages = {369--390},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Field, C. A. and Welsh, A. H.},
	urldate = {2024-07-31},
	date = {2007},
	note = {Publisher: [Royal Statistical Society, Wiley]},
}

@article{wang_bootstrap_2019,
	title = {Bootstrap {ICC} estimators in analysis of small clustered binary data},
	volume = {34},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-019-00885-z},
	doi = {10.1007/s00180-019-00885-z},
	pages = {1765--1778},
	number = {4},
	journaltitle = {Computational Statistics},
	shortjournal = {Comput Stat},
	author = {Wang, Bei and Zheng, Yi and Irimata, Kyle M. and Wilson, Jeffrey R.},
	urldate = {2024-07-31},
	date = {2019-12-01},
	langid = {english},
	keywords = {Generalized linear mixed model, Resampling scheme, Small sample inference},
}

@incollection{goldstein_bootstrapping_2010,
	title = {Bootstrapping in Multilevel Models},
	isbn = {978-0-203-84885-2},
	abstract = {Bootstrapping in Multilevel Models - 1},
	booktitle = {Handbook of Advanced Multilevel Analysis},
	publisher = {Routledge},
	author = {Goldstein, Harvey},
	date = {2010}
}

@online{noauthor_full_nodate,
	title = {Full article: Bootstrapped inference for variance parameters, measures of heterogeneity and random effects in multilevel logistic regression models},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2020.1797738#},
	urldate = {2024-07-30},
}

@article{austin_bootstrapped_2020,
	title = {Bootstrapped inference for variance parameters, measures of heterogeneity and random effects in multilevel logistic regression models},
	volume = {90},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2020.1797738},
	doi = {10.1080/00949655.2020.1797738},
	pages = {3175--3199},
	number = {17},
	journaltitle = {Journal of Statistical Computation and Simulation},
	author = {Austin, Peter C. and Leckie, George},
	urldate = {2024-07-30},
	date = {2020-11-21},
	keywords = {Bootstrap, Monte Carlo simulations, hierarchical model, multilevel model, random-effects model, variance},
}

@article{ukoumunne_non-parametric_2003,
	title = {Non-parametric bootstrap confidence intervals for the intraclass correlation coefficient},
	volume = {22},
	rights = {Copyright © 2003 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1643},
	doi = {10.1002/sim.1643},
	pages = {3805--3821},
	number = {24},
	journaltitle = {Statistics in Medicine},
	author = {Ukoumunne, Obioha C. and Davison, Anthony C. and Gulliford, Martin C. and Chinn, Susan},
	urldate = {2024-07-30},
	date = {2003},
	langid = {english},
	keywords = {bootstrap, confidence intervals, intraclass correlation coefficient},
}

@article{austin_bootstrapped_2020-1,
	title = {Bootstrapped inference for variance parameters, measures of heterogeneity and random effects in multilevel logistic regression models},
	volume = {90},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2020.1797738},
	doi = {10.1080/00949655.2020.1797738},
	pages = {3175--3199},
	number = {17},
	journaltitle = {Journal of Statistical Computation and Simulation},
	author = {Austin, Peter C. and Leckie, George},
	urldate = {2024-07-30},
	date = {2020-11-21},
	keywords = {Bootstrap, Monte Carlo simulations, hierarchical model, multilevel model, random-effects model, variance},
}

@article{field_bootstrapping_2007-1,
	title = {Bootstrapping clustered data},
	volume = {69},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00593.x},
	doi = {10.1111/j.1467-9868.2007.00593.x},
	pages = {369--390},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Field, C. A. and Welsh, A. H.},
	urldate = {2024-07-30},
	date = {2007},
	langid = {english},
	keywords = {Between and within sums of squares, Bootstrap, Clusters, Hierarchical data, One-way arrays},
}

@book{shalev-shwartz_understanding_2014,
	edition = {1},
	title = {Understanding Machine Learning: From Theory to Algorithms},
	rights = {https://www.cambridge.org/core/terms},
	url = {https://www.cambridge.org/core/product/identifier/9781107298019/type/book},
	shorttitle = {Understanding Machine Learning},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	urldate = {2024-07-22},
	date = {2014-05-19},
	langid = {english},
	doi = {10.1017/CBO9781107298019},
}

@book{hastie_elements_2009,
	location = {New York, {NY}},
	title = {The Elements of Statistical Learning},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	series = {Springer Series in Statistics},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	urldate = {2024-03-30},
	date = {2009},
	langid = {english},
	doi = {10.1007/978-0-387-84858-7},
}

@article{martin_small_1995,
	title = {Small sample statistics for classification error rates I: error rate measurements},
	url = {https://escholarship.org/uc/item/76g4v06v},
	shorttitle = {Small sample statistics for classification error rates I},
	abstract = {Several methods (independent subsamples, leave-one-out, cross-validation, and bootstrapping) have been proposed for estimating the error rates of classifiers. The rationale behind the various estimators and the causes of the sometimes conflicting claims regarding their bias and precision are explored in this paper. The biases and variances of each of the estimators are examined empirically. Cross-validation, 10-fold or greater, seems to be the best approach, the other methods are biased, have poorer precision, or are inconsistent. (Though unbiased for linear discriminant classifiers, the 632b bootstrap estimator isbiased for nearest neighbors classifiers, more so for single nearest neighbor than for three nearest neighbors. The 632b estimator is also biased for {CART}-style decision trees. Weiss {LOO}* estimator is unbiased and has better precision than cross-validation for discriminant and nearest neighbors classifiers, but its lack of bias and improved precision for those classifiers do not carry over to decision trees for nominal attributes.},
	author = {Martin, J. Kent and Hirschberg, D. S.},
	urldate = {2024-07-28},
	date = {1995-11-11},
	langid = {english},
}

@article{martin_small_1995-1,
	title = {Small sample statistics for classification error rates {II}: confidence intervals and significance tests},
	url = {https://escholarship.org/uc/item/3p38290h},
	shorttitle = {Small sample statistics for classification error rates {II}},
	abstract = {Several techniques for estimating the range of uncertainty of estimated error rates and for estimating the significance of observed differences in error rates are explored in this paper. Textbook formulas which assume a large test set (i.e., a normal distribution) are commonly used to approximate the confidence limits of error rates or as an approximate significance test for comparing error rates. Expressions for determining more exact limits and significance levels for small samples are given here, and criteria are also given for determining when these more exact methods should be used. The assumed normal distribution gives a poor approximation to the confidence interval in most cases, but is usually useful for significance tests when the proper mean and variance expressions are used. A commonly used ±2},
	author = {Martin, J. Kent and Hirschberg, D. S.},
	urldate = {2024-07-28},
	date = {1995-11-12},
	langid = {english},
}

@article{newcombe_logit_2001,
	title = {Logit Confidence Intervals and the Inverse Sinh Transformation},
	volume = {55},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2685799},
	abstract = {The logit Wald and Wilson score intervals for the binomial proportion are both symmetrical on the logit scale. They are closely related: the widths of the two intervals on the logit scale are shown to be related by a simple sinh function. Similarly, the Woolf logit Wald interval for the odds ratio and the analogous interval for the relative risk may be shortened by inverse sinh transformation. This modified interval can be calculated even when one of the cells contains a zero. It then takes a very simple form in which the appropriate centile of the chi-square distribution on one degree of freedom is substituted for the zero. These suggested modified intervals require evaluation of coverage probabilities.},
	pages = {200--202},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Newcombe, Robert G.},
	urldate = {2024-07-27},
	date = {2001},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
}

@book{shalev-shwartz_understanding_2014-1,
	edition = {1},
	title = {Understanding Machine Learning: From Theory to Algorithms},
	isbn = {978-1-107-05713-5 978-1-107-29801-9},
	shorttitle = {Understanding Machine Learning},
	abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the {PAC}-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	date = {2014-05-19},
	langid = {english},
	doi = {10.1017/CBO9781107298019},
}

@article{cleves_comparative_2002,
	title = {Comparative Assessment of Three Common Algorithms for Estimating the Variance of the Area under the Nonparametric Receiver Operating Characteristic Curve},
	volume = {2},
	doi = {10.1177/1536867X0200200304},
	abstract = {The area under the receiver operating characteristic ({ROC}) curve is often used to summarize and compare the discriminatory accuracy of a diagnostic test or modality, and to evaluate the predictive power of statistical models for binary outcomes. Parametric maximum likelihood methods for fitting of the {ROC} curve provide direct estimates of the area under the {ROC} curve and its variance. Nonparametric methods, on the other hand, provide estimates of the area under the {ROC} curve, but do not directly estimate its variance. Three algorithms for computing the variance for the area under the nonparametric {ROC} curve are commonly used, although ambiguity exists about their behavior under diverse study conditions. Using simulated data, we found similar asymptotic performance between these algorithms when the diagnostic test produces results on a continuous scale, but found notable differences in small samples, and when the diagnostic test yields results on a discrete diagnostic scale.},
	pages = {280--289},
	number = {3},
	journaltitle = {The Stata Journal},
	author = {Cleves, Mario A.},
	date = {2002},
}

@inproceedings{chen_xgboost_2016,
	location = {New York, {NY}, {USA}},
	title = {{XGBoost}: A Scalable Tree Boosting System},
	doi = {10.1145/2939672.2939785},
	series = {{KDD} '16},
	shorttitle = {{XGBoost}},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called {XGBoost}, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, {XGBoost} scales beyond billions of examples using far fewer resources than existing systems.},
	pages = {785--794},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Chen, Tianqi and Guestrin, Carlos},
	date = {2016-08-13},
	keywords = {large-scale machine learning},
}

@article{breslow_approximate_1993,
	title = {Approximate Inference in Generalized Linear Mixed Models},
	volume = {88},
	doi = {10.1080/01621459.1993.10594284},
	abstract = {Statistical approaches to overdispersion, correlated errors, shrinkage estimation, and smoothing of regression relationships may be encompassed within the framework of the generalized linear mixed model ({GLMM}). Given an unobserved vector of random effects, observations are assumed to be conditionally independent with means that depend on the linear predictor through a specified link function and conditional variances that are specified by a variance function, known prior weights and a scale factor. The random effects are assumed to be normally distributed with mean zero and dispersion matrix depending on unknown variance components. For problems involving time series, spatial aggregation and smoothing, the dispersion may be specified in terms of a rank deficient inverse covariance matrix. Approximation of the marginal quasi-likelihood using Laplace's method leads eventually to estimating equations based on penalized quasilikelihood or {PQL} for the mean parameters and pseudo-likelihood for the variances. Implementation involves repeated calls to normal theory procedures for {REML} estimation in variance components problems. By means of informal mathematical arguments, simulations and a series of worked examples, we conclude that {PQL} is of practical value for approximate inference on parameters and realizations of random effects in the hierarchical model. The applications cover overdispersion in binomial proportions of seed germination; longitudinal analysis of attack rates in epilepsy patients; smoothing of birth cohort effects in an age-cohort model of breast cancer incidence; evaluation of curvature of birth cohort effects in a case-control study of childhood cancer and obstetric radiation; spatial aggregation of lip cancer rates in Scottish counties; and the success of salamander matings in a complicated experiment involving crossing of male and female effects. {PQL} tends to underestimate somewhat the variance components and (in absolute value) fixed effects when applied to clustered binary data, but the situation improves rapidly for binomial observations having denominators greater than one.},
	pages = {9--25},
	number = {421},
	journaltitle = {Journal of the American Statistical Association},
	author = {Breslow, N. E. and Clayton, D. G.},
	date = {1993-03-01},
	keywords = {Longitudinal data, Overdispersion, Penalized quasi-likelihood, Spatial aggregation, Variance components},
}

@article{bradley_use_1997,
	title = {The use of the area under the {ROC} curve in the evaluation of machine learning algorithms},
	volume = {30},
	issn = {0031-3203},
	doi = {10.1016/S0031-3203(96)00142-2},
	abstract = {In this paper we investigate the use of the area under the receiver operating characteristic ({ROC}) curve ({AUC}) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of {AUC} to the more conventional overall accuracy and find that {AUC} exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance ({ANOVA}) tests; a standard error that decreased as both {AUC} and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that {AUC} be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.},
	pages = {1145--1159},
	number = {7},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Bradley, Andrew P.},
	date = {1997-07-01},
	keywords = {Accuracy measures, Cross-validation, Standard error, The {ROC} curve, The area under the {ROC} curve ({AUC}), Wilcoxon statistic},
}

@article{ling_auc_2003,
	title = {{AUC}: a Statistically Consistent and more Discriminating Measure than Accuracy},
	abstract = {Predictive accuracy has been used as the main and often only evaluation criterion for the predictive performance of classification learning algorithms. In recent years, the area under the {ROC} (Receiver Operating Characteristics) curve, or simply {AUC}, has been proposed as an alternative single-number measure for evaluating learning algorithms. In this paper, we prove that {AUC} is a better measure than accuracy. More specifically, we present rigourous definitions on consistency and discriminancy in comparing two evaluation measures for learning algorithms. We then present empirical evaluations and a formal proof to establish that {AUC} is indeed statistically consistent and more discriminating than accuracy. Our result is quite significant since we formally prove that, for the first time, {AUC} is a better measure than accuracy in the evaluation of learning algorithms.},
	author = {Ling, Charles X and Huang, Jin and Zhang, Harry},
	date = {2003},
	langid = {english},
}

@book{wickham_dplyr_2023,
	title = {dplyr: A Grammar of Data Manipulation},
	url = {https://CRAN.R-project.org/package=dplyr},
	author = {Wickham, Hadley and François, Romain and Henry, Lionel and Müller, Kirill and Vaughan, Davis},
	date = {2023},
}

@article{robin_proc_2011,
	title = {{pROC}: an open-source package for R and S+ to analyze and compare {ROC} curves},
	volume = {12},
	pages = {77},
	journaltitle = {{BMC} Bioinformatics},
	author = {Robin, Xavier and Turck, Natacha and Hainard, Alexandre and Tiberti, Natalia and Lisacek, Frédérique and Sanchez, Jean-Charles and Müller, Markus},
	date = {2011},
}

@article{bischl_batchjobs_2015,
	title = {{BatchJobs} and {BatchExperiments}: Abstraction Mechanisms for Using R in Batch Environments},
	volume = {64},
	url = {https://www.jstatsoft.org/v64/i11/},
	doi = {10.18637/jss.v064.i11},
	pages = {1--25},
	number = {11},
	journaltitle = {Journal of Statistical Software},
	author = {Bischl, Bernd and Lang, Michel and Mersmann, Olaf and Rahnenführer, Jörg and Weihs, Claus},
	date = {2015},
}

@book{barrett_datatable_2024,
	title = {data.table: Extension of `data.frame`},
	url = {https://CRAN.R-project.org/package=data.table},
	author = {Barrett, Tyson and Dowle, Matt and Srinivasan, Arun and Gorecki, Jan and Chirico, Michael and Hocking, Toby},
	date = {2024},
}

@book{r_core_team_r_2024,
	location = {Vienna, Austria},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	date = {2024},
}

@article{lang_mlr3_2019,
	title = {mlr3: A modern object-oriented machine learning framework in R},
	url = {https://joss.theoj.org/papers/10.21105/joss.01903},
	doi = {10.21105/joss.01903},
	journaltitle = {Journal of Open Source Software},
	author = {Lang, Michel and Binder, Martin and Richter, Jakob and Schratz, Patrick and Pfisterer, Florian and Coors, Stefan and Au, Quay and Casalicchio, Giuseppe and Kotthoff, Lars and Bischl, Bernd},
	date = {2019-12},
}

@misc{goodrich_rstanarm_2024,
	title = {rstanarm: Bayesian applied regression modeling via Stan.},
	url = {https://mc-stan.org/rstanarm/},
	author = {Goodrich, Ben and Gabry, Jonah and Ali, Imad and Brilleman, Sam},
	date = {2024},
}

@misc{stan_development_team_stan_2022,
	title = {Stan Modeling Language Users Guide and Reference Manual: Runtime warnings and convergence problems},
	url = {https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup},
	author = {Stan Development Team},
	urldate = {2024-07-24},
	date = {2022-03},
	note = {Publication Title: Runtime warnings and Convergence Problems},
}

@incollection{dekking_confidence_2005,
	location = {London},
	title = {Confidence intervals for the mean},
	isbn = {978-1-84628-168-6},
	url = {https://doi.org/10.1007/1-84628-168-7_23},
	pages = {341--360},
	booktitle = {A Modern Introduction to Probability and Statistics: Understanding Why and How},
	publisher = {Springer},
	author = {Dekking, Frederik Michel and Kraaikamp, Cornelis and Lopuhaä, Hendrik Paul and Meester, Ludolf Erwin},
	editor = {Dekking, Frederik Michel and Kraaikamp, Cornelis and Lopuhaä, Hendrik Paul and Meester, Ludolf Erwin},
	urldate = {2024-07-22},
	date = {2005},
	langid = {english},
	doi = {10.1007/1-84628-168-7_23},
}

@online{noauthor_understanding_nodate,
	title = {Understanding Machine Learning},
	url = {https://www.cambridge.org/core/books/understanding-machine-learning/3059695661405D25673058E43C8BE2A6},
	urldate = {2024-07-22},
}

@article{sun_fast_2014,
	title = {Fast Implementation of {DeLong}’s Algorithm for Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},
	volume = {21},
	issn = {1558-2361},
	url = {https://ieeexplore.ieee.org/document/6851192},
	doi = {10.1109/LSP.2014.2337313},
	abstract = {Among algorithms for comparing the areas under two or more correlated receiver operating characteristic ({ROC}) curves, {DeLong}'s algorithm is perhaps the most widely used one due to its simplicity of implementation in practice. Unfortunately, however, the time complexity of {DeLong}'s algorithm is of quadratic order (the product of sample sizes), thus making it time-consuming and impractical when the sample sizes are large. Based on an equivalent relationship between the Heaviside function and mid-ranks of samples, we improve {DeLong}'s algorithm by reducing the order of time complexity from quadratic down to linearithmic (the product of sample size and its logarithm). Monte Carlo simulations verify the computational efficiency of our algorithmic findings in this work.},
	pages = {1389--1393},
	number = {11},
	journaltitle = {{IEEE} Signal Processing Letters},
	author = {Sun, Xu and Xu, Weichao},
	urldate = {2024-07-21},
	date = {2014-11},
	note = {Conference Name: {IEEE} Signal Processing Letters},
	keywords = {Area under the curve ({AUC}), {DeLong}’s method, Manganese, Monte Carlo methods, Receivers, Signal processing algorithms, Sun, Time complexity, Vectors, mid-rank, receiver operating characteristic ({ROC})},
}

@article{delong_comparing_1988,
	title = {Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach},
	volume = {44},
	issn = {0006-341X},
	shorttitle = {Comparing the areas under two or more correlated receiver operating characteristic curves},
	abstract = {Methods of evaluating and comparing the performance of diagnostic tests are of increasing importance as new tests are developed and marketed. When a test is based on an observed variable that lies on a continuous or graded scale, an assessment of the overall value of the test can be made through the use of a receiver operating characteristic ({ROC}) curve. The curve is constructed by varying the cutpoint used to determine which values of the observed variable will be considered abnormal and then plotting the resulting sensitivities against the corresponding false positive rates. When two or more empirical curves are constructed based on tests performed on the same individuals, statistical analysis on differences between curves must take into account the correlated nature of the data. This paper presents a nonparametric approach to the analysis of areas under correlated {ROC} curves, by using the theory on generalized U-statistics to generate an estimated covariance matrix.},
	pages = {837--845},
	number = {3},
	journaltitle = {Biometrics},
	shortjournal = {Biometrics},
	author = {{DeLong}, E. R. and {DeLong}, D. M. and Clarke-Pearson, D. L.},
	date = {1988-09},
	pmid = {3203132},
	keywords = {Algorithms, Analysis of Variance, Female, Humans, Intestinal Obstruction, Models, Statistical, Ovarian Neoplasms, Predictive Value of Tests, {ROC} Curve},
}

@online{noauthor_comparative_nodate,
	title = {Comparative Assessment of Three Common Algorithms for Estimating the Variance of the Area under the Nonparametric Receiver Operating Characteristic Curve - Mario A. Cleves, 2002},
	url = {https://journals.sagepub.com/doi/abs/10.1177/1536867X0200200304},
	urldate = {2024-07-21},
}

@online{noauthor_comparative_nodate-1,
	title = {Comparative Assessment of Three Common Algorithms for Estimating the Variance of the Area under the Nonparametric Receiver Operating Characteristic Curve},
	url = {https://journals.sagepub.com/doi/epdf/10.1177/1536867X0200200304},
	urldate = {2024-07-21},
	langid = {english},
	doi = {10.1177/1536867X0200200304},
}

@article{tobi_small_2005,
	title = {Small proportions: what to report for confidence intervals?},
	volume = {14},
	issn = {1053-8569},
	doi = {10.1002/pds.1081},
	shorttitle = {Small proportions},
	abstract = {{PURPOSE}: It is generally agreed that a confidence interval ({CI}) is usually more informative than a point estimate or p-value, but we rarely encounter small proportions with {CI} in the pharmaco-epidemiological literature. When a {CI} is given it is sporadically reported, how it was calculated. This incorrectly suggests one single method to calculate {CIs}. To identify the method best suited for small proportions, seven approximate methods and the Clopper-Pearson Exact method to calculate {CIs} were compared.
{METHODS}: In a simulation study for 90-, 95- and 99\%{CIs}, with sample size 1000 and proportions ranging from 0.001 to 0.01, were evaluated systematically. Main quality criteria were coverage and interval width. The methods are illustrated using data from pharmaco-epidemiology studies.
{RESULTS}: Simulations showed that standard Wald methods have insufficient coverage probability regardless of how the desired coverage is perceived. Overall, the Exact method and the Score method with continuity correction ({CC}) performed best. Real life examples showed the methods to yield different results too.
{CONCLUSIONS}: For {CIs} for small proportions (pi {\textless} or = 0.01), the use of the Exact method and the Score method with {CC} are advocated based on this study.},
	pages = {239--247},
	number = {4},
	journaltitle = {Pharmacoepidemiology and Drug Safety},
	shortjournal = {Pharmacoepidemiol Drug Saf},
	author = {Tobi, Hilde and van den Berg, Paul B. and de Jong-van den Berg, Lolkje T. W.},
	date = {2005-04},
	pmid = {15719354},
	keywords = {Confidence Intervals, Epidemiologic Factors, Models, Statistical, Sample Size},
}

@online{noauthor_small_nodate,
	title = {Small proportions: what to report for confidence intervals? - {PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/15719354/},
	urldate = {2024-07-21},
}

@article{reed_better_2007,
	title = {Better Binomial Confidence Intervals},
	volume = {6},
	rights = {Copyright (c) 2007},
	issn = {1538-9472},
	url = {https://jmasm.com/index.php/jmasm/article/view/290},
	doi = {10.56801/10.56801/v6.i.290},
	abstract = {The construction of a confidence interval for a binomial parameter is a basic analysis in statistical inference. Most introductory statistics textbook authors present the binomial confidence interval based on the asymptotic normality of the sample proportion and estimating the standard error - the Wald method. For the one sample binomial confidence interval the Clopper-Pearson exact method has been regarded as definitive as it eliminates both overshoot and zero width intervals. The Clopper-Pearson exact method is the most conservative and is unquestionably a better alternative to the Wald method. Other viable alternatives include Wilson's Score, the Agresti-Coull method, and the Borkowf {SAIFS}-z.},
	pages = {153--161},
	journaltitle = {Journal of Modern Applied Statistical Methods},
	author = {Reed, James F. {III}},
	urldate = {2024-07-21},
	date = {2007-05-01},
	langid = {english},
	keywords = {Agresti-Coull method., Binomial distribution, {ClopperPearson} Method, Score Method, Wald method, confidence intervals, coverage probability},
}

@online{noauthor_view_nodate,
	title = {View of Better Binomial Confidence Intervals},
	url = {https://jmasm.com/index.php/jmasm/article/view/290/288},
	urldate = {2024-07-21},
}

@article{riley_minimum_2021,
	title = {Minimum sample size for external validation of a clinical prediction model with a binary outcome},
	volume = {40},
	rights = {© 2021 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9025},
	doi = {10.1002/sim.9025},
	abstract = {In prediction model research, external validation is needed to examine an existing model's performance using data independent to that for model development. Current external validation studies often suffer from small sample sizes and consequently imprecise predictive performance estimates. To address this, we propose how to determine the minimum sample size needed for a new external validation study of a prediction model for a binary outcome. Our calculations aim to precisely estimate calibration (Observed/Expected and calibration slope), discrimination (C-statistic), and clinical utility (net benefit). For each measure, we propose closed-form and iterative solutions for calculating the minimum sample size required. These require specifying: (i) target {SEs} (confidence interval widths) for each estimate of interest, (ii) the anticipated outcome event proportion in the validation population, (iii) the prediction model's anticipated (mis)calibration and variance of linear predictor values in the validation population, and (iv) potential risk thresholds for clinical decision-making. The calculations can also be used to inform whether the sample size of an existing (already collected) dataset is adequate for external validation. We illustrate our proposal for external validation of a prediction model for mechanical heart valve failure with an expected outcome event proportion of 0.018. Calculations suggest at least 9835 participants (177 events) are required to precisely estimate the calibration and discrimination measures, with this number driven by the calibration slope criterion, which we anticipate will often be the case. Also, 6443 participants (116 events) are required to precisely estimate net benefit at a risk threshold of 8\%. Software code is provided.},
	pages = {4230--4251},
	number = {19},
	journaltitle = {Statistics in Medicine},
	author = {Riley, Richard D. and Debray, Thomas P. A. and Collins, Gary S. and Archer, Lucinda and Ensor, Joie and van Smeden, Maarten and Snell, Kym I. E.},
	urldate = {2024-07-18},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9025},
	keywords = {binary outcomes, calibration, discrimination, external validation, minimum sample size, multivariable prediction model, net benefit},
}

@article{etz_small_2015,
	title = {Small Sample Research: Considerations Beyond Statistical Power},
	volume = {16},
	issn = {1573-6695},
	url = {https://doi.org/10.1007/s11121-015-0585-4},
	doi = {10.1007/s11121-015-0585-4},
	shorttitle = {Small Sample Research},
	abstract = {Small sample research presents a challenge to current standards of design and analytic approaches and the underlying notions of what constitutes good prevention science. Yet, small sample research is critically important as the research questions posed in small samples often represent serious health concerns in vulnerable and underrepresented populations. This commentary considers the Special Section on small sample research and also highlights additional challenges that arise in small sample research not considered in the Special Section, including generalizability, determining what constitutes knowledge, and ensuring that research designs match community desires. It also points to opportunities afforded by small sample research, such as a focus on and increased understanding of context and the emphasis it may place on alternatives to the randomized clinical trial. The commentary urges the development and adoption of innovative strategies to conduct research with small samples.},
	pages = {1033--1036},
	number = {7},
	journaltitle = {Prevention Science},
	shortjournal = {Prev Sci},
	author = {Etz, Kathleen E. and Arroyo, Judith A.},
	urldate = {2024-07-18},
	date = {2015-10-01},
	langid = {english},
}

@misc{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	url = {http://arxiv.org/abs/1611.03530},
	doi = {10.48550/arXiv.1611.03530},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	number = {{arXiv}:1611.03530},
	publisher = {{arXiv}},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	urldate = {2024-07-18},
	date = {2017-02-26},
	eprinttype = {arxiv},
	eprint = {1611.03530 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{mckinney_international_2020,
	title = {International evaluation of an {AI} system for breast cancer screening},
	volume = {577},
	issn = {1476-4687},
	doi = {10.1038/s41586-019-1799-6},
	abstract = {Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence ({AI}) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the {UK} and a large enriched dataset from the {USA}. We show an absolute reduction of 5.7\% and 1.2\% ({USA} and {UK}) in false positives and 9.4\% and 2.7\% in false negatives. We provide evidence of the ability of the system to generalize from the {UK} to the {USA}. In an independent study of six radiologists, the {AI} system outperformed all of the human readers: the area under the receiver operating characteristic curve ({AUC}-{ROC}) for the {AI} system was greater than the {AUC}-{ROC} for the average radiologist by an absolute margin of 11.5\%. We ran a simulation in which the {AI} system participated in the double-reading process that is used in the {UK}, and found that the {AI} system maintained non-inferior performance and reduced the workload of the second reader by 88\%. This robust assessment of the {AI} system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
	pages = {89--94},
	number = {7788},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{McKinney}, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg S. and Darzi, Ara and Etemadi, Mozziyar and Garcia-Vicente, Florencia and Gilbert, Fiona J. and Halling-Brown, Mark and Hassabis, Demis and Jansen, Sunny and Karthikesalingam, Alan and Kelly, Christopher J. and King, Dominic and Ledsam, Joseph R. and Melnick, David and Mostofi, Hormuz and Peng, Lily and Reicher, Joshua Jay and Romera-Paredes, Bernardino and Sidebottom, Richard and Suleyman, Mustafa and Tse, Daniel and Young, Kenneth C. and De Fauw, Jeffrey and Shetty, Shravya},
	date = {2020-01},
	pmid = {31894144},
	keywords = {Artificial Intelligence, Breast Neoplasms, Early Detection of Cancer, Female, Humans, Mammography, Reproducibility of Results, United Kingdom, United States},
}

@article{wenzel_automatic_2019,
	title = {Automatic classification of dopamine transporter {SPECT}: deep convolutional neural networks can be trained to be robust with respect to variable image characteristics},
	volume = {46},
	issn = {1619-7089},
	url = {https://doi.org/10.1007/s00259-019-04502-5},
	doi = {10.1007/s00259-019-04502-5},
	shorttitle = {Automatic classification of dopamine transporter {SPECT}},
	abstract = {This study investigated the potential of deep convolutional neural networks ({CNN}) for automatic classification of {FP}-{CIT} {SPECT} in multi-site or multi-camera settings with variable image characteristics.},
	pages = {2800--2811},
	number = {13},
	journaltitle = {European Journal of Nuclear Medicine and Molecular Imaging},
	shortjournal = {Eur J Nucl Med Mol Imaging},
	author = {Wenzel, Markus and Milletari, Fausto and Krüger, Julia and Lange, Catharina and Schenk, Michael and Apostolova, Ivayla and Klutmann, Susanne and Ehrenburg, Marcus and Buchert, Ralph},
	urldate = {2024-07-18},
	date = {2019-12-01},
	langid = {english},
	keywords = {Convolutional neural network, Deep learning, Domain adaption, Dopamine transporter, {FP}-{CIT}, {SPECT}},
}

@inproceedings{ambroladze_cnn-based_2023,
	location = {Wiesbaden},
	title = {{CNN}-based Whole Breast Segmentation in Longitudinal High-risk {MRI} Study},
	isbn = {978-3-658-41657-7},
	doi = {10.1007/978-3-658-41657-7_35},
	abstract = {Segmentation of breast {MRI} in a longitudinal high-risk cohort is challenging due to considerable variability in image quality and artefacts, imaging protocols, scanner types and field strengths, and not least anatomy. Automated segmentation of the whole breast is a prerequisite for quantitative tissue characterization in early detection as well as image-based risk modeling and predictive machine learning. We investigated the behavior of a 2D U-net architecture when being faced with a large set of error-prone training input and compared the results to the training on a smaller but human-corrected high-quality annotation set, both when being used for fine-tuning and training from scratch. Our dataset consists of a total of 876 pre-contrast axial T1 weighted {MRI} volumes from 166 subjects acquired between 2006 and 2021 in a longitudinal high-risk breast cancer screening cohort study. All images were previously segmented using a fully automated heuristic algorithm, resulting in error-prone segmentation masks, whichwere used in an initial ‘human-free’ experiment. We randomly separated on a per-subject level 102 volumes from 23 subjects, for which an expert radiologist manually corrected these segmentation masks, providing the basis for the additional two experiments. Our results indicate a subclass of input errors can be compensated for by the regularization capacity of standard deep convolutional neural networks while other errors are learnt or even newly introduced. In particular, tissue boundaries in axial directions were missed in our experiment. In the inner region, the median Dice coefficient of our fine-tuning U-net exceeded 98\% at a reasonable robustness and consistency, which is promising given the simplicity of our approach. Future work will address efficient learning schemes, aiming at boosting the segmentation quality with minimal human input, and the boundary issue.},
	pages = {159--164},
	booktitle = {Bildverarbeitung für die Medizin 2023},
	publisher = {Springer Fachmedien},
	author = {Ambroladze, Ani and Hahn, Horst K. and Amer, Heba and Ingrisch, Michael and Gerken, Annika and Wenzel, Markus and Püsken, Michael and Mittermeier, Andreas and Engel, Christoph and Schmutzler, Rita and Fallenberg, Eva M.},
	editor = {Deserno, Thomas M. and Handels, Heinz and Maier, Andreas and Maier-Hein, Klaus and Palm, Christoph and Tolxdorff, Thomas},
	date = {2023},
	langid = {german},
}

@article{venkatesh_going_2015,
	title = {Going beyond a First Reader: A Machine Learning Methodology for Optimizing Cost and Performance in Breast Ultrasound Diagnosis},
	volume = {41},
	issn = {1879-291X},
	doi = {10.1016/j.ultrasmedbio.2015.07.020},
	shorttitle = {Going beyond a First Reader},
	abstract = {The goal of this study was to devise a machine learning methodology as a viable low-cost alternative to a second reader to help augment physicians' interpretations of breast ultrasound images in differentiating benign and malignant masses. Two independent feature sets consisting of visual features based on a radiologist's interpretation of images and computer-extracted features when used as first and second readers and combined by adaptive boosting ({AdaBoost}) and a pruning classifier resulted in a very high level of diagnostic performance (area under the receiver operating characteristic curve = 0.98) at a cost of pruning a fraction (20\%) of the cases for further evaluation by independent methods. {AdaBoost} also improved the diagnostic performance of the individual human observers and increased the agreement between their analyses. Pairing {AdaBoost} with selective pruning is a principled methodology for achieving high diagnostic performance without the added cost of an additional reader for differentiating solid breast masses by ultrasound.},
	pages = {3148--3162},
	number = {12},
	journaltitle = {Ultrasound in Medicine \& Biology},
	shortjournal = {Ultrasound Med Biol},
	author = {Venkatesh, Santosh S. and Levenback, Benjamin J. and Sultan, Laith R. and Bouzghar, Ghizlane and Sehgal, Chandra M.},
	date = {2015-12},
	pmid = {26354997},
	keywords = {Adaptive boosting, Area Under Curve, Artificial intelligence, Breast Neoplasms, Breast cancer, Breast ultrasound, Computer-aided diagnosis, Female, Humans, Image Processing, Computer-Assisted, Machine Learning, Middle Aged, Observer Variation, Sensitivity and Specificity, Ultrasonography, Mammary},
}

@inproceedings{kohli_application_2018,
	title = {Application of Machine Learning in Disease Prediction},
	url = {https://ieeexplore.ieee.org/document/8777449},
	doi = {10.1109/CCAA.2018.8777449},
	abstract = {The application of machine learning in the field of medical diagnosis is increasing gradually. This can be contributed primarily to the improvement in the classification and recognition systems used in disease diagnosis which is able to provide data that aids medical experts in early detection of fatal diseases and therefore, increase the survival rate of patients significantly. In this paper, we apply different classification algorithms, each with its own advantage on three separate databases of disease (Heart, Breast cancer, Diabetes) available in {UCI} repository for disease prediction. The feature selection for each dataset was accomplished by backward modeling using the p-value test. The results of the study strengthen the idea of the application of machine learning in early detection of diseases.},
	eventtitle = {2018 4th International Conference on Computing Communication and Automation ({ICCCA})},
	pages = {1--4},
	booktitle = {2018 4th International Conference on Computing Communication and Automation ({ICCCA})},
	author = {Kohli, Pahulpreet Singh and Arora, Shriya},
	urldate = {2024-07-18},
	date = {2018-12},
	note = {{ISSN}: 2642-7354},
	keywords = {Classification algorithms, Diabetes, Disease Prediction, Diseases, Heart, Heart Disease Dataset, Machine Learning, Machine learning algorithms, Pima Indians Diabetes dataset, Predictive models, Support vector machines, Wisconsin Breast Cancer Dataset},
}

@article{volovici_steps_2022,
	title = {Steps to avoid overuse and misuse of machine learning in clinical research},
	volume = {28},
	rights = {2022 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-022-01961-6},
	doi = {10.1038/s41591-022-01961-6},
	abstract = {Machine learning algorithms are a powerful tool in healthcare, but sometimes perform no better than traditional statistical techniques. Steps should be taken to ensure that algorithms are not overused or misused, in order to provide genuine benefit for patients.},
	pages = {1996--1999},
	number = {10},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Volovici, Victor and Syn, Nicholas L. and Ercole, Ari and Zhao, Joseph J. and Liu, Nan},
	urldate = {2024-07-18},
	date = {2022-10},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Epidemiology, Outcomes research, Policy},
}

@article{grote_uncertainty_2023,
	title = {Uncertainty, Evidence, and the Integration of Machine Learning into Medical Practice},
	volume = {48},
	issn = {0360-5310},
	url = {https://doi.org/10.1093/jmp/jhac034},
	doi = {10.1093/jmp/jhac034},
	abstract = {In light of recent advances in machine learning for medical applications, the automation of medical diagnostics is imminent. That said, before machine learning algorithms find their way into clinical practice, various problems at the epistemic level need to be overcome. In this paper, we discuss different sources of uncertainty arising for clinicians trying to evaluate the trustworthiness of algorithmic evidence when making diagnostic judgments. Thereby, we examine many of the limitations of current machine learning algorithms (with deep learning in particular) and highlight their relevance for medical diagnostics. Among the problems we inspect are the theoretical foundations of deep learning (which are not yet adequately understood), the opacity of algorithmic decisions, and the vulnerabilities of machine learning models, as well as concerns regarding the quality of medical data used to train the models. Building on this, we discuss different desiderata for an uncertainty amelioration strategy that ensures that the integration of machine learning into clinical settings proves to be medically beneficial in a meaningful way.},
	pages = {84--97},
	number = {1},
	journaltitle = {The Journal of Medicine and Philosophy: A Forum for Bioethics and Philosophy of Medicine},
	shortjournal = {The Journal of Medicine and Philosophy: A Forum for Bioethics and Philosophy of Medicine},
	author = {Grote, Thomas and Berens, Philipp},
	urldate = {2024-07-17},
	date = {2023-02-01},
}

@article{valen_quantifying_2022,
	title = {Quantifying uncertainty in machine learning classifiers for medical imaging},
	volume = {17},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-022-02578-3},
	doi = {10.1007/s11548-022-02578-3},
	abstract = {Machine learning ({ML}) models in medical imaging ({MI}) can be of great value in computer aided diagnostic systems, but little attention is given to the confidence (alternatively, uncertainty) of such models, which may have significant clinical implications. This paper applied, validated, and explored a technique for assessing uncertainty in convolutional neural networks ({CNNs}) in the context of {MI}.},
	pages = {711--718},
	number = {4},
	journaltitle = {International Journal of Computer Assisted Radiology and Surgery},
	shortjournal = {Int J {CARS}},
	author = {Valen, John and Balki, Indranil and Mendez, Mauro and Qu, Wendi and Levman, Jacob and Bilbily, Alexander and Tyrrell, Pascal N.},
	urldate = {2024-07-17},
	date = {2022-04-01},
	langid = {english},
	keywords = {Confidence, Medical imaging, Neural network, Uncertainty},
}

@article{haug_artificial_2023,
	title = {Artificial Intelligence and Machine Learning in Clinical Medicine, 2023},
	volume = {388},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMra2302038},
	doi = {10.1056/NEJMra2302038},
	abstract = {This first article in a series describes the history of artificial intelligence in medicine; the use of {AI} in image analysis, identification of disease outbreaks, and diagnosis; and the use of chatbots.},
	pages = {1201--1208},
	number = {13},
	journaltitle = {New England Journal of Medicine},
	author = {Haug, Charlotte J. and Drazen, Jeffrey M.},
	urldate = {2024-07-17},
	date = {2023-03-29},
	note = {Publisher: Massachusetts Medical Society},
}

@article{wallis_binomial_2013,
	title = {Binomial Confidence Intervals and Contingency Tests: Mathematical Fundamentals and the Evaluation of Alternative Methods},
	volume = {20},
	issn = {0929-6174, 1744-5035},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09296174.2013.799918},
	doi = {10.1080/09296174.2013.799918},
	shorttitle = {Binomial Confidence Intervals and Contingency Tests},
	abstract = {Many statistical methods rely on an underlying mathematical model of probability which is based on a simple approximation, one that is simultaneously well-known and yet frequently poorly understood. This approximation is the Normal approximation to the Binomial distribution, and it underpins a range of statistical tests and methods, including the calculation of accurate confidence intervals, performing goodness of fit and contingency tests, line- and model-fitting, and computational methods based upon these.},
	pages = {178--208},
	number = {3},
	journaltitle = {Journal of Quantitative Linguistics},
	shortjournal = {Journal of Quantitative Linguistics},
	author = {Wallis, Sean},
	urldate = {2024-07-17},
	date = {2013-08},
	langid = {english},
}

@article{shao_bootstrap_1996,
	title = {Bootstrap Model Selection},
	volume = {91},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476934},
	doi = {10.1080/01621459.1996.10476934},
	abstract = {In a regression problem, typically there are p explanatory variables possibly related to a response variable, and we wish to select a subset of the p explanatory variables to fit a model between these variables and the response. A bootstrap variable/model selection procedure is to select the subset of variables by minimizing bootstrap estimates of the prediction error, where the bootstrap estimates are constructed based on a data set of size n. Although the bootstrap estimates have good properties, this bootstrap selection procedure is inconsistent in the sense that the probability of selecting the optimal subset of variables does not converge to 1 as n → ∞. This inconsistency can be rectified by modifying the sampling method used in drawing bootstrap observations. For bootstrapping pairs (response, explanatory variable), it is found that instead of drawing n bootstrap observations (a customary bootstrap sampling plan), much less bootstrap observations should be sampled: The bootstrap selection procedure becomes consistent if we draw m bootstrap observations with m → ∞ and m/n → 0. For bootstrapping residuals, we modify the bootstrap sampling procedure by increasing the variability among the bootstrap observations. The consistency of the modified bootstrap selection procedures is established in various situations, including linear models, nonlinear models, generalized linear models, and autoregressive time series. The choice of the bootstrap sample size m and some computational issues are also discussed. Some empirical results are presented.},
	pages = {655--665},
	number = {434},
	journaltitle = {Journal of the American Statistical Association},
	author = {Shao, Jun},
	urldate = {2024-07-11},
	date = {1996-06-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476934},
	keywords = {Autoregressive time series, Bootstrap sample size, Generalized linear model, Nonlinear regression, Prediction error},
}

@article{shao_linear_1993,
	title = {Linear Model Selection by Cross-Validation},
	volume = {88},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2290328},
	doi = {10.2307/2290328},
	abstract = {We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion ({AIC}), the C$_{\textrm{p}}$, and the bootstrap, is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n → ∞. We show that the inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n$_{\textrm{ν}}$-out cross-validation with n$_{\textrm{ν}}$, the number of observations reserved for validation, satisfying n$_{\textrm{ν}}$/n → 1 as n → ∞. This is a somewhat shocking discovery, because n$_{\textrm{ν}}$/n → 1 is totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some practical aspects of the use of the leave-n$_{\textrm{ν}}$-out cross-validation method are provided, and results from a simulation study are presented.},
	pages = {486--494},
	number = {422},
	journaltitle = {Journal of the American Statistical Association},
	author = {Shao, Jun},
	urldate = {2024-07-11},
	date = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
}

@article{daszykowski_representative_2002,
	title = {Representative subset selection},
	volume = {468},
	issn = {0003-2670},
	url = {https://www.sciencedirect.com/science/article/pii/S0003267002006517},
	doi = {10.1016/S0003-2670(02)00651-7},
	abstract = {Fast development of analytical techniques enable to acquire huge amount of data. Large data sets are difficult to handle and therefore, there is a big interest in designing a subset of the original data set, which preserves the information of the original data set and facilitates the computations. There are many subset selection methods and their choice depends on the problem at hand. The two most popular groups of subset selection methods are uniform designs and cluster-based designs. Among the methods considered in this paper there are uniform designs, such as those proposed by Kennard and Stone, {OptiSim}, and cluster-based designs applying K-means technique and density based spatial clustering of applications with noise ({DBSCAN}). Additionally, a new concept of the subset selection with K-means is introduced.},
	pages = {91--103},
	number = {1},
	journaltitle = {Analytica Chimica Acta},
	shortjournal = {Analytica Chimica Acta},
	author = {Daszykowski, M. and Walczak, B. and Massart, D. L.},
	urldate = {2024-07-11},
	date = {2002-09-10},
	keywords = {Data mining, Subset selection, Uniform design},
}

@article{harrington_multiple_2018,
	title = {Multiple Versus Single Set Validation of Multivariate Models to Avoid Mistakes},
	volume = {48},
	issn = {1547-6510},
	doi = {10.1080/10408347.2017.1361314},
	abstract = {Validation of multivariate models is of current importance for a wide range of chemical applications. Although important, it is neglected. The common practice is to use a single external validation set for evaluation. This approach is deficient and may mislead investigators with results that are specific to the single validation set of data. In addition, no statistics are available regarding the precision of a derived figure of merit ({FOM}). A statistical approach using bootstrapped Latin partitions is advocated. This validation method makes an efficient use of the data because each object is used once for validation. It was reviewed a decade earlier but primarily for the optimization of chemometric models this review presents the reasons it should be used for generalized statistical validation. Average {FOMs} with confidence intervals are reported and powerful, matched-sample statistics may be applied for comparing models and methods. Examples demonstrate the problems with single validation sets.},
	pages = {33--46},
	number = {1},
	journaltitle = {Critical Reviews in Analytical Chemistry},
	shortjournal = {Crit Rev Anal Chem},
	author = {Harrington, Peter de Boves},
	date = {2018-01-02},
	pmid = {28777019},
	keywords = {Algorithms, Bootstrap Latin partition, Datasets as Topic, Least-Squares Analysis, Multivariate Analysis, calibration, chemometrics, classification, dataset sampling, statistical validation},
}

@article{krzywinski_error_2013,
	title = {Error bars},
	volume = {10},
	rights = {2013 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2659},
	doi = {10.1038/nmeth.2659},
	abstract = {The meaning of error bars is often misinterpreted, as is the statistical significance of their overlap.},
	pages = {921--922},
	number = {10},
	journaltitle = {Nature Methods},
	author = {Krzywinski, Martin and Altman, Naomi},
	urldate = {2024-07-10},
	date = {2013-10-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Publishing, Research data, Statistical methods},
}

@article{altman_predicting_2018,
	title = {Predicting with confidence and tolerance},
	volume = {15},
	rights = {2018 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0196-7},
	doi = {10.1038/s41592-018-0196-7},
	abstract = {I abhor averages. I like the individual case. –J.D. Brandeis.},
	pages = {843--845},
	number = {11},
	journaltitle = {Nature Methods},
	author = {Altman, Naomi and Krzywinski, Martin},
	urldate = {2024-07-10},
	date = {2018-11-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Biological Microscopy, Biological Techniques, Biomedical Engineering/Biotechnology, Life Sciences, Proteomics, general},
}

@article{bzdok_statistics_2018,
	title = {Statistics versus machine learning},
	volume = {15},
	rights = {2018 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4642},
	doi = {10.1038/nmeth.4642},
	abstract = {Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns.},
	pages = {233--234},
	number = {4},
	journaltitle = {Nature Methods},
	author = {Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin},
	urldate = {2024-07-10},
	date = {2018-04-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Biological Microscopy, Biological Techniques, Biomedical Engineering/Biotechnology, Life Sciences, Proteomics, general},
}

@article{kulesa_sampling_2015,
	title = {Sampling distributions and the bootstrap},
	volume = {12},
	rights = {http://www.springer.com/tdm},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/nmeth.3414},
	doi = {10.1038/nmeth.3414},
	pages = {477--478},
	number = {6},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Kulesa, Anthony and Krzywinski, Martin and Blainey, Paul and Altman, Naomi},
	urldate = {2024-07-10},
	date = {2015-06},
	langid = {english},
}

@article{krzywinski_importance_2013,
	title = {Importance of being uncertain},
	volume = {10},
	rights = {2013 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2613},
	doi = {10.1038/nmeth.2613},
	abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
	pages = {809--810},
	number = {9},
	journaltitle = {Nature Methods},
	author = {Krzywinski, Martin and Altman, Naomi},
	urldate = {2024-07-10},
	date = {2013-09-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Publishing, Research data, Statistical methods},
}

@inproceedings{hendrycks_many_2021,
	title = {The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization},
	doi = {10.1109/ICCV48922.2021.00823},
	pages = {8320--8329},
	booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
	date = {2021},
	keywords = {Benchmark testing, Computational modeling, Computer vision, Degradation, Gain measurement, Market research, Recognition and classification, Robustness, Transfer/Low-shot/Semi/Unsupervised Learning},
}

@article{arlot_survey_2010,
	title = {A survey of cross-validation procedures for model selection},
	volume = {4},
	issn = {1935-7516},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full},
	doi = {10.1214/09-SS054},
	abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplicity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
	pages = {40--79},
	issue = {none},
	journaltitle = {Statistics Surveys},
	author = {Arlot, Sylvain and Celisse, Alain},
	urldate = {2024-07-10},
	date = {2010-01},
	note = {Publisher: Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
	keywords = {62G05, 62G08, 62G09, Model selection, cross-validation, leave-one-out},
}

@article{angelopoulos_gentle_2021,
	title = {A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification},
	volume = {abs/2107.07511},
	url = {https://api.semanticscholar.org/CorpusID:235899036},
	journaltitle = {{ArXiv}},
	author = {Angelopoulos, Anastasios Nikolas and Bates, Stephen},
	date = {2021},
}

@article{troyer_monte_nodate,
	title = {Monte Carlo simulations and error analysis},
	author = {Troyer, Matthias},
	langid = {english},
}

@article{koehler_assessment_2009,
	title = {On the Assessment of Monte Carlo Error in Simulation-Based Statistical Analyses},
	volume = {63},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/25652244},
	abstract = {Statistical experiments, more commonly referred to as Monte Carlo or simulation studies, are used to study the behavior of statistical methods and measures under controlled situations. Whereas recent computing and methodological advances have permitted increased efficiency in the simulation process, known as variance reduction, such experiments remain limited by their finite nature and hence are subject to uncertainty; when a simulation is run more than once, different results are obtained. However, virtually no emphasis has been placed on reporting the uncertainty, referred to here as Monte Carlo error, associated with simulation results in the published literature, or on justifying the number of replications used. These deserve broader consideration. Here we present a series of simple and practical methods for estimating Monte Carlo error as well as determining the number of replications required to achieve a desired level of accuracy. The issues and methods are demonstrated with two simple examples, one evaluating operating characteristics of the maximum likelihood estimator for the parameters in logistic regression and the other in the context of using the bootstrap to obtain 95\% confidence intervals. The results suggest that in many settings, Monte Carlo error may be more substantial than traditionally thought.},
	pages = {155--162},
	number = {2},
	journaltitle = {The American Statistician},
	author = {Koehler, Elizabeth and Brown, Elizabeth and Haneuse, Sebastien J.-P. A.},
	urldate = {2024-06-26},
	date = {2009},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
}

@inproceedings{gillespie_efficient_2016,
	title = {Efficient R Programming - A Practical Guide to Smarter Programming},
	url = {https://api.semanticscholar.org/CorpusID:10271333},
	author = {Gillespie, Colin S. and Lovelace, Robin},
	date = {2016},
}

@online{noauthor_efficient_nodate,
	title = {Efficient R Programming [Book]},
	url = {https://www.oreilly.com/library/view/efficient-r-programming/9781491950777/},
	abstract = {There are many excellent R resources for visualization, data science, and package development. Hundreds of scattered vignettes, web pages, and forums explain how to use R in particular domains. But … - Selection from Efficient R Programming [Book]},
	urldate = {2024-06-17},
	langid = {english},
	note = {{ISBN}: 9781491950784},
}

@online{heiss_guide_2021,
	title = {A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models},
	url = {https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/},
	abstract = {How to calculate grand means, conditional group means, and hypothetical group means of posterior predictions from multilevel brms models.},
	titleaddon = {Andrew Heiss},
	author = {Heiss, Andrew},
	urldate = {2024-06-13},
	date = {2021-11-10},
	langid = {english},
}

@online{heiss_guide_2021-1,
	title = {A Guide to Correctly Calculating Posterior Predictions and Average Marginal Effects with Multilievel Bayesian Models},
	url = {https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide},
	author = {Heiss, Andrew},
	date = {2021-11-10},
	doi = {10.59350/wbn93-edb02},
	note = {Pages: undefined},
}

@article{binder_collecting_2020,
	title = {Collecting Empirical Data About Hyperparameters for Data Driven {AutoML}},
	abstract = {All optimization needs some kind of prior over the functions it is optimizing over. We used a large computing cluster to collect empirical data about the behavior of {ML} performance, by randomly sampling hyperparameter values and performing cross-validation. We also collected information about cross-validation error by performing some evaluations multiple times, and information about progression of performance with respect to training data size by performing some evaluations on data subsets. We present how we collected data, make some preliminary analyses on the surrogate models that can be built with them, and give an outlook over interesting analysis this should enable.},
	journaltitle = {7th {ICML} Workshop on Automated Machine Learning},
	author = {Binder, Martin and Pﬁsterer, Florian and Bischl, Bernd},
	date = {2020},
	langid = {english},
}

@book{efron_introduction_1994,
	location = {New York},
	title = {An Introduction to the Bootstrap},
	isbn = {978-0-429-24659-3},
	abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
	pagetotal = {456},
	publisher = {Chapman and Hall/{CRC}},
	author = {Efron, Bradley and Tibshirani, R. J.},
	date = {1994-05-15},
	doi = {10.1201/9780429246593},
}

@book{davison_bootstrap_1997,
	title = {Bootstrap Methods and their Application},
	series = {Cambridge Series in Statistical and Probabilistic Mathematics},
	publisher = {Cambridge University Press},
	author = {Davison, A. C. and Hinkley, D. V.},
	date = {1997},
}

@online{noauthor_bootstrap_nodate,
	title = {Bootstrap Methods and their Application},
	url = {https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/ED2FD043579F27952363566DC09CBD6A},
	urldate = {2024-06-12},
}

@article{kulesa_sampling_2015-1,
	title = {Sampling distributions and the bootstrap},
	volume = {12},
	rights = {http://www.springer.com/tdm},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/nmeth.3414},
	doi = {10.1038/nmeth.3414},
	pages = {477--478},
	number = {6},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Kulesa, Anthony and Krzywinski, Martin and Blainey, Paul and Altman, Naomi},
	urldate = {2024-06-12},
	date = {2015-06},
	langid = {english},
}

@book{efron_introduction_1994-1,
	location = {New York},
	title = {An Introduction to the Bootstrap},
	isbn = {978-0-429-24659-3},
	abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
	pagetotal = {456},
	publisher = {Chapman and Hall/{CRC}},
	author = {Efron, Bradley and Tibshirani, R. J.},
	date = {1994-05-15},
	doi = {10.1201/9780429246593},
}

@article{gamerman_sampling_1997,
	title = {Sampling from the posterior distribution in generalized linear mixed models},
	volume = {7},
	issn = {1573-1375},
	url = {https://doi.org/10.1023/A:1018509429360},
	doi = {10.1023/A:1018509429360},
	abstract = {Generalized linear mixed models provide a unified framework for treatment of exponential family regression models, overdispersed data and longitudinal studies. These problems typically involve the presence of random effects and this paper presents a new methodology for making Bayesian inference about them. The approach is simulation-based and involves the use of Markov chain Monte Carlo techniques. The usual iterative weighted least squares algorithm is extended to include a sampling step based on the Metropolis–Hastings algorithm thus providing a unified iterative scheme. Non-normal prior distributions for the regression coefficients and for the random effects distribution are considered. Random effect structures with nesting required by longitudinal studies are also considered. Particular interests concern the significance of regression coefficients and assessment of the form of the random effects. Extensions to unknown scale parameters, unknown link functions, survival and frailty models are outlined.},
	pages = {57--68},
	number = {1},
	journaltitle = {Statistics and Computing},
	shortjournal = {Statistics and Computing},
	author = {Gamerman, Dani},
	urldate = {2024-06-12},
	date = {1997-03-01},
	langid = {english},
	keywords = {Bayesian, Markov chain Monte Carlo, blocking, longitudinal studies, random effects, weighted least squares},
}

@article{fong_bayesian_2010,
	title = {Bayesian inference for generalized linear mixed models},
	volume = {11},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxp053},
	doi = {10.1093/biostatistics/kxp053},
	abstract = {Generalized linear mixed models ({GLMMs}) continue to grow in popularity due to their ability to directly acknowledge multiple levels of dependency and model different data types. For small sample sizes especially, likelihood-based inference can be unreliable with variance components being particularly difficult to estimate. A Bayesian approach is appealing but has been hampered by the lack of a fast implementation, and the difficulty in specifying prior distributions with variance components again being particularly problematic. Here, we briefly review previous approaches to computation in Bayesian implementations of {GLMMs} and illustrate in detail, the use of integrated nested Laplace approximations in this context. We consider a number of examples, carefully specifying prior distributions on meaningful quantities in each case. The examples cover a wide range of data types including those requiring smoothing over time and a relatively complicated spline model for which we examine our prior specification in terms of the implied degrees of freedom. We conclude that Bayesian inference is now practically feasible for {GLMMs} and provides an attractive alternative to likelihood-based approaches such as penalized quasi-likelihood. As with likelihood-based approaches, great care is required in the analysis of clustered binary data since approximation strategies may be less accurate for such data.},
	pages = {397--412},
	number = {3},
	journaltitle = {Biostatistics},
	shortjournal = {Biostatistics},
	author = {Fong, Youyi and Rue, Håvard and Wakefield, Jon},
	urldate = {2024-06-12},
	date = {2010-07-01},
}

@book{gelman_data_2006,
	title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
	series = {Analytical Methods for Social Research},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	date = {2006},
}

@online{gelman_data_2006-1,
	title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
	url = {https://www.cambridge.org/highereducation/books/data-analysis-using-regression-and-multilevel-hierarchical-models/32A29531C7FD730C3A68951A17C9D983},
	abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
	titleaddon = {Higher Education from Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	urldate = {2024-06-12},
	date = {2006-12-18},
	langid = {english},
	doi = {10.1017/CBO9780511790942},
	note = {{ISBN}: 9780511790942
Publisher: Cambridge University Press},
}

@online{noauthor_home_nodate,
	title = {Home page for the book, "Data Analysis Using Regression and Multilevel/Hierarchical Models"},
	url = {http://www.stat.columbia.edu/~gelman/arm/},
	urldate = {2024-06-12},
}

@article{williams_extra-binomial_1982,
	title = {Extra-Binomial Variation in Logistic Linear Models},
	volume = {31},
	issn = {0035-9254},
	url = {https://www.jstor.org/stable/2347977},
	doi = {10.2307/2347977},
	abstract = {The logistic-linear model, and its maximum likelihood estimation by iterated reweighted least squares, can be simply modified to incorporate a component of extra-binomial variation. The modifications are very easily effected if the {GLIM} program is used.},
	pages = {144--148},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Williams, D. A.},
	urldate = {2024-06-12},
	date = {1982},
	note = {Publisher: [Wiley, Royal Statistical Society]},
}

@online{noauthor_introduction_nodate,
	title = {Introduction to Generalized Linear Mixed Models},
	url = {https://stats.oarc.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/},
}

@book{mccullagh_generalized_2019,
	location = {New York},
	edition = {2},
	title = {Generalized Linear Models},
	isbn = {978-0-203-75373-6},
	abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and ot},
	pagetotal = {532},
	publisher = {Routledge},
	author = {{McCullagh}, P.},
	date = {2019-01-31},
	doi = {10.1201/9780203753736},
}

@article{pinheiro_efficient_2006,
	title = {Efficient Laplacian and Adaptive Gaussian Quadrature Algorithms for Multilevel Generalized Linear Mixed Models},
	volume = {15},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1198/106186006X96962},
	doi = {10.1198/106186006X96962},
	pages = {58--81},
	number = {1},
	journaltitle = {Journal of Computational and Graphical Statistics},
	shortjournal = {Journal of Computational and Graphical Statistics},
	author = {Pinheiro, José C and Chao, Edward C},
	urldate = {2024-06-12},
	date = {2006-03},
	langid = {english},
}

@incollection{dodge_randomized_2008,
	location = {New York, {NY}},
	title = {Randomized Block Design},
	isbn = {978-0-387-32833-1},
	url = {https://doi.org/10.1007/978-0-387-32833-1_344},
	pages = {447--448},
	booktitle = {The Concise Encyclopedia of Statistics},
	publisher = {Springer},
	author = {Dodge, Yadolah},
	urldate = {2024-06-12},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-0-387-32833-1_344},
}

@article{hothorn_simultaneous_2008,
	title = {Simultaneous inference in general parametric models},
	volume = {50},
	issn = {1521-4036},
	doi = {10.1002/bimj.200810425},
	abstract = {Simultaneous inference is a common problem in many areas of application. If multiple null hypotheses are tested simultaneously, the probability of rejecting erroneously at least one of them increases beyond the pre-specified significance level. Simultaneous inference procedures have to be used which adjust for multiplicity and thus control the overall type I error rate. In this paper we describe simultaneous inference procedures in general parametric models, where the experimental questions are specified through a linear combination of elemental model parameters. The framework described here is quite general and extends the canonical theory of multiple comparison procedures in {ANOVA} models to linear regression problems, generalized linear models, linear mixed effects models, the Cox model, robust linear models, etc. Several examples using a variety of different statistical models illustrate the breadth of the results. For the analyses we use the R add-on package multcomp, which provides a convenient interface to the general approach adopted here.},
	pages = {346--363},
	number = {3},
	journaltitle = {Biometrical Journal. Biometrische Zeitschrift},
	shortjournal = {Biom J},
	author = {Hothorn, Torsten and Bretz, Frank and Westfall, Peter},
	date = {2008-06},
	pmid = {18481363},
	keywords = {Data Interpretation, Statistical, Humans, Models, Statistical},
}

@incollection{pinheiro_theory_2000,
	location = {New York, {NY}},
	title = {Theory and Computational Methods for Linear Mixed-Effects Models},
	isbn = {978-0-387-22747-4},
	url = {https://doi.org/10.1007/0-387-22747-4_2},
	pages = {57--96},
	booktitle = {Mixed-Effects Models in S and S-{PLUS}},
	publisher = {Springer},
	editor = {Pinheiro, José C. and Bates, Douglas M.},
	urldate = {2024-06-12},
	date = {2000},
	langid = {english},
	doi = {10.1007/0-387-22747-4_2},
}

@thesis{eugster_benchmark_2011,
	location = {München},
	title = {Benchmark Experiments – A Tool for Analyzing Statistical Learning Algorithms},
	url = {https://edoc.ub.uni-muenchen.de/12990/1/Eugster_Manuel_J_A.pdf},
	institution = {{LMU} München},
	type = {phdthesis},
	author = {Eugster, Manuel J A},
	urldate = {2024-06-12},
	date = {2011-01-25},
	langid = {english},
}

@article{eugster_benchmark_nodate,
	title = {Benchmark Experiments – A Tool for Analyzing Statistical Learning Algorithms},
	author = {Eugster, Manuel J A},
	langid = {english},
}

@thesis{noauthor_notitle_nodate,
	url = {https://edoc.ub.uni-muenchen.de/12990/1/Eugster_Manuel_J_A.pdf},
	type = {phdthesis},
}

@article{efron_bootstrap_1979,
	title = {Bootstrap Methods: Another Look at the Jackknife},
	volume = {7},
	url = {https://doi.org/10.1214/aos/1176344552},
	doi = {10.1214/aos/1176344552},
	pages = {1 -- 26},
	number = {1},
	journaltitle = {The Annals of Statistics},
	author = {Efron, B.},
	date = {1979},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Nonlinear regression, Resampling, bootstrap, discriminant analysis, error rate estimation, jackknife, nonparametric variance estimation, subsample values},
}

@online{noauthor_bootstrap_nodate-1,
	title = {Bootstrap Methods: Another Look at the Jackknife},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full},
	urldate = {2024-06-11},
}

@article{nadeau_inference_2003,
	title = {Inference for the Generalization Error},
	volume = {52},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1024068626366},
	doi = {10.1023/A:1024068626366},
	abstract = {In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.},
	pages = {239--281},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Nadeau, Claude and Bengio, Yoshua},
	urldate = {2024-06-10},
	date = {2003-09-01},
	langid = {english},
	keywords = {cross-validation, generalization error, hypothesis tests, power, size, variance estimation},
}

@article{binder_collecting_nodate,
	title = {Collecting Empirical Data About Hyperparameters for Data Driven {AutoML}},
	abstract = {All optimization needs some kind of prior over the functions it is optimizing over. We used a large computing cluster to collect empirical data about the behavior of {ML} performance, by randomly sampling hyperparameter values and performing cross-validation. We also collected information about cross-validation error by performing some evaluations multiple times, and information about progression of performance with respect to training data size by performing some evaluations on data subsets. We present how we collected data, make some preliminary analyses on the surrogate models that can be built with them, and give an outlook over interesting analysis this should enable.},
	author = {Binder, Martin and Pﬁsterer, Florian and Bischl, Bernd},
	langid = {english},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: Open and Efficient Foundation Language Models},
	url = {http://arxiv.org/abs/2302.13971},
	shorttitle = {{LLaMA}},
	abstract = {We introduce {LLaMA}, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, {LLaMA}-13B outperforms {GPT}-3 (175B) on most benchmarks, and {LLaMA}65B is competitive with the best models, Chinchilla-70B and {PaLM}-540B. We release all our models to the research community1.},
	number = {{arXiv}:2302.13971},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	urldate = {2024-05-10},
	date = {2023-02-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision ({IJCV})},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015},
}

@online{hugging_face_open_nodate,
	title = {Open {LLM} Leaderboard - a Hugging Face Space by {HuggingFaceH}4},
	url = {https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	titleaddon = {Open {LLM} Leaderboard},
	author = {{Hugging Face}},
	urldate = {2024-05-10},
}

@misc{bischl_hyperparameter_2021,
	title = {Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges},
	url = {http://arxiv.org/abs/2107.05847},
	doi = {10.48550/arXiv.2107.05847},
	shorttitle = {Hyperparameter Optimization},
	abstract = {Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization ({HPO}) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing {HPO} from a general perspective, this paper reviews important {HPO} methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting {HPO}, including the {HPO} algorithms themselves, performance evaluation, how to combine {HPO} with {ML} pipelines, runtime improvements, and parallelization. This work is accompanied by an appendix that contains information on specific software packages in R and Python, as well as information and recommended hyperparameter search spaces for specific learning algorithms. We also provide notebooks that demonstrate concepts from this work as supplementary files.},
	number = {{arXiv}:2107.05847},
	publisher = {{arXiv}},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	urldate = {2024-05-09},
	date = {2021-11-24},
	eprinttype = {arxiv},
	eprint = {2107.05847 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{friedman_greedy_2001,
	title = {Greedy function approximation: A gradient boosting machine.},
	volume = {29},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
	doi = {10.1214/aos/1013203451},
	shorttitle = {Greedy function approximation},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such {TreeBoost} models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	number = {5},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Friedman, Jerome H.},
	urldate = {2024-05-09},
	date = {2001-10-01},
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	pages = {273--297},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	urldate = {2024-05-09},
	date = {1995-09-01},
	langid = {english},
	keywords = {efficient learning algorithms, neural networks, pattern recognition, polynomial classifiers, radial basis function classifiers},
}

@article{wright_ranger_2017,
	title = {ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R},
	volume = {77},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v077i01},
	doi = {10.18637/jss.v077.i01},
	abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
	pages = {1--17},
	number = {1},
	journaltitle = {Journal of Statistical Software},
	author = {Wright, Marvin N. and Ziegler, Andreas},
	date = {2017},
}

@article{breiman_random_2001,
	title = {Random Forests},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Breiman, Leo},
	date = {2001-10-01},
}

@article{friedman_regularization_2010,
	title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
	volume = {33},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
	doi = {10.18637/jss.v033.i01},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include ℓ\&amp;lt;sub\&amp;gt;1\&amp;lt;/sub\&amp;gt; (the lasso), ℓ\&amp;lt;sub\&amp;gt;2\&amp;lt;/sub\&amp;gt; (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	pages = {1--22},
	number = {1},
	journaltitle = {Journal of Statistical Software},
	author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
	date = {2010},
}

@article{saito_precision-recall_2015,
	title = {The Precision-Recall Plot Is More Informative than the {ROC} Plot When Evaluating Binary Classifiers on Imbalanced Datasets},
	volume = {10},
	url = {https://doi.org/10.1371/journal.pone.0118432},
	doi = {10.1371/journal.pone.0118432},
	abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics ({ROC}) plots. Alternative measures such as positive predictive value ({PPV}) and the associated Precision/Recall ({PRC}) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While {ROC} plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether {ROC} plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of {ROC} plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. {PRC} plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use {ROC} plots on imbalanced datasets.},
	pages = {1--21},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	date = {2015-03},
	note = {Publisher: Public Library of Science},
}

@online{noauthor_precision-recall_nodate,
	title = {The Precision-Recall Plot Is More Informative than the {ROC} Plot When Evaluating Binary Classifiers on Imbalanced Datasets {\textbar} {PLOS} {ONE}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
	urldate = {2024-05-09},
}

@article{benkeser_improved_2020,
	title = {Improved small-sample estimation of nonlinear cross-validated prediction metrics},
	volume = {115},
	issn = {0162-1459},
	doi = {10.1080/01621459.2019.1668794},
	abstract = {When predicting an outcome is the scientific goal, one must decide on a metric by which to evaluate the quality of predictions. We consider the problem of measuring the performance of a prediction algorithm with the same data that were used to train the algorithm. Typical approaches involve bootstrapping or cross-validation. However, we demonstrate that bootstrap-based approaches often fail and standard cross-validation estimators may perform poorly. We provide a general study of cross-validation-based estimators that highlights the source of this poor performance, and propose an alternative framework for estimation using techniques from the efficiency theory literature. We provide a theorem establishing the weak convergence of our estimators. The general theorem is applied in detail to two specific examples and we discuss possible extensions to other parameters of interest. For the two explicit examples that we consider, our estimators demonstrate remarkable finite-sample improvements over standard approaches.},
	pages = {1917--1932},
	number = {532},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {J Am Stat Assoc},
	author = {Benkeser, David and Petersen, Maya and van der Laan, Mark J.},
	date = {2020},
	pmid = {33716360},
	pmcid = {PMC7954141},
	keywords = {{AUC}, cross-validation, estimating equations, machine learning, prediction, targeted minimum loss-based estimation},
}

@article{ledell_computationally_2015,
	title = {Computationally efficient confidence intervals for cross-validated area under the {ROC} curve estimates},
	volume = {9},
	issn = {1935-7524},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4533123/},
	doi = {10.1214/15-EJS1035},
	abstract = {In binary classification problems, the area under the {ROC} curve ({AUC}) is commonly used to evaluate the performance of a prediction model. Often, it is combined with cross-validation in order to assess how the results will generalize to an independent data set. In order to evaluate the quality of an estimate for cross-validated {AUC}, we obtain an estimate of its variance. For massive data sets, the process of generating a single performance estimate can be computationally expensive. Additionally, when using a complex prediction method, the process of cross-validating a predictive model on even a relatively small data set can still require a large amount of computation time. Thus, in many practical settings, the bootstrap is a computationally intractable approach to variance estimation. As an alternative to the bootstrap, we demonstrate a computationally efficient influence curve based approach to obtaining a variance estimate for cross-validated {AUC}.},
	pages = {1583--1607},
	number = {1},
	journaltitle = {Electronic journal of statistics},
	shortjournal = {Electron J Stat},
	author = {{LeDell}, Erin and Petersen, Maya and van der Laan, Mark},
	urldate = {2024-05-02},
	date = {2015},
	pmid = {26279737},
	pmcid = {PMC4533123},
}

@book{ross_first_2010,
	location = {Upper Saddle River, N.J},
	edition = {8th ed},
	title = {A first course in probability},
	isbn = {978-0-13-603313-4},
	pagetotal = {530},
	publisher = {Pearson Prentice Hall},
	author = {Ross, Sheldon M.},
	date = {2010},
	langid = {english},
	note = {{OCLC}: ocn237199460},
	keywords = {Probabilities, Textbooks},
}

@inproceedings{ross_first_1979,
	title = {A First Course in Probability},
	volume = {86},
	url = {https://www.jstor.org/stable/2321968?origin=crossref},
	doi = {10.2307/2321968},
	abstract = {1. Combinatorial Analysis. 2. Axioms of Probability. 3. Conditional Probability and Independence. 4. Random Variables. 5. Continuous Random Variables. 6. Jointly Distributed Random Variables. 7. Properties of Expectation. 8. Limit Theorems. 9. Additional Topics in Probability. 10. Simulation. Appendix A. Answers to Selected Problems. Appendix B. Solutions to Self-Test Problems and Exercises. Index.},
	pages = {137},
	booktitle = {The American Mathematical Monthly},
	author = {Ross, Sheldon},
	urldate = {2024-05-02},
	date = {1979-02},
	note = {{ISSN}: 00029890
Issue: 2
Journal Abbreviation: The American Mathematical Monthly},
}

@article{muschelli_roc_2020,
	title = {{ROC} and {AUC} with a Binary Predictor: a Potentially Misleading Metric},
	volume = {37},
	issn = {1432-1343},
	url = {https://doi.org/10.1007/s00357-019-09345-1},
	doi = {10.1007/s00357-019-09345-1},
	shorttitle = {{ROC} and {AUC} with a Binary Predictor},
	abstract = {In analysis of binary outcomes, the receiver operator characteristic ({ROC}) curve is heavily used to show the performance of a model or algorithm. The {ROC} curve is informative about the performance over a series of thresholds and can be summarized by the area under the curve ({AUC}), a single number. When a predictor is categorical, the {ROC} curve has one less than number of categories as potential thresholds; when the predictor is binary, there is only one threshold. As the {AUC} may be used in decision-making processes on determining the best model, it important to discuss how it agrees with the intuition from the {ROC} curve. We discuss how the interpolation of the curve between thresholds with binary predictors can largely change the {AUC}. Overall, we show using a linear interpolation from the {ROC} curve with binary predictors corresponds to the estimated {AUC}, which is most commonly done in software, which we believe can lead to misleading results. We compare R, Python, Stata, and {SAS} software implementations. We recommend using reporting the interpolation used and discuss the merit of using the step function interpolator, also referred to as the “pessimistic” approach by Fawcett (2006).},
	pages = {696--708},
	number = {3},
	journaltitle = {Journal of Classification},
	shortjournal = {J Classif},
	author = {Muschelli, John},
	urldate = {2024-04-30},
	date = {2020-10-01},
	langid = {english},
	keywords = {{AUC}, Area under the curve, R, {ROC}},
}

@article{fawcett_introduction_2006,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
	doi = {10.1016/j.patrec.2005.10.010},
	series = {{ROC} Analysis in Pattern Recognition},
	abstract = {Receiver operating characteristics ({ROC}) graphs are useful for organizing classifiers and visualizing their performance. {ROC} graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although {ROC} graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to {ROC} graphs and as a guide for using them in research.},
	pages = {861--874},
	number = {8},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Fawcett, Tom},
	urldate = {2024-04-30},
	date = {2006-06-01},
	keywords = {Classifier evaluation, Evaluation metrics, {ROC} analysis},
}

@article{markatou_analysis_2005,
	title = {Analysis of Variance of Cross-Validation Estimators of the Generalization Error},
	volume = {6},
	url = {http://jmlr.org/papers/v6/markatou05a.html},
	pages = {1127--1168},
	number = {39},
	journaltitle = {Journal of Machine Learning Research},
	author = {Markatou, Marianthi and Tian, Hong and Biswas, Shameek and Hripcsak, George},
	date = {2005},
}

@article{breiman_heuristics_1996,
	title = {Heuristics of instability and stabilization in model selection},
	volume = {24},
	url = {https://doi.org/10.1214/aos/1032181158},
	doi = {10.1214/aos/1032181158},
	pages = {2350 -- 2383},
	number = {6},
	journaltitle = {The Annals of Statistics},
	author = {Breiman, Leo},
	date = {1996},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {cross-validation, prediction error, predictive loss, regression, subset selection},
}

@article{hajjem_mixed-effects_2010,
	title = {Mixed-effects random forest for clustered data},
	volume = {84},
	pages = {1313 -- 1328},
	journaltitle = {Journal of Statistical Computation and Simulation},
	author = {Hajjem, Ahlem and Bellavance, François and Larocque, Denis},
	date = {2010},
}

@book{steyerberg_clinical_2019,
	location = {Cham},
	title = {Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating},
	isbn = {978-3-030-16398-3 978-3-030-16399-0},
	url = {http://link.springer.com/10.1007/978-3-030-16399-0},
	series = {Statistics for Biology and Health},
	shorttitle = {Clinical Prediction Models},
	publisher = {Springer International Publishing},
	author = {Steyerberg, Ewout W.},
	urldate = {2024-03-29},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-16399-0},
}

@article{barnard_small-sample_2024,
	title = {Small-Sample Degrees of Freedom with Multiple Imputation},
	abstract = {An appealing feature of multiple imputation is the simplicity of the rules for combining the multiple complete-data inferences into a final inference, the repeated-imputation inference (Rubin, 1987). This inference is based on a t distribution and is derived from a Bayesian paradigm under the assumption that the complete-data degrees of freedom, vcom, are infinite, but the number of imputations, m, is finite. When vcom is small and there is only a modest proportion of missing data, the calculated repeated-imputation degrees of freedom, vit, for the t reference distribution can be much larger than vcor, which is clearly inappropriate. Following the Bayesian paradigm, we derive an adjusted degrees of freedom, Vir with the following three properties: for fixed m and estimated fraction of missing information, vn monotonically increases in vcom; Vi is always less than or equal to vcom; and vm equals v. when vcom is infinite. A small simulation study demonstrates the superior frequentist performance when using Vi, rather than v7n.},
	author = {Barnard, John and Rubin, Donald B},
	date = {2024},
	langid = {english},
}

@article{marshall_combining_2009,
	title = {Combining estimates of interest in prognostic modelling studies after multiple imputation: current practice and guidelines},
	volume = {9},
	issn = {1471-2288},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-9-57},
	doi = {10.1186/1471-2288-9-57},
	shorttitle = {Combining estimates of interest in prognostic modelling studies after multiple imputation},
	abstract = {Background: Multiple imputation ({MI}) provides an effective approach to handle missing covariate data within prognostic modelling studies, as it can properly account for the missing data uncertainty. The multiply imputed datasets are each analysed using standard prognostic modelling techniques to obtain the estimates of interest. The estimates from each imputed dataset are then combined into one overall estimate and variance, incorporating both the within and between imputation variability. Rubin's rules for combining these multiply imputed estimates are based on asymptotic theory. The resulting combined estimates may be more accurate if the posterior distribution of the population parameter of interest is better approximated by the normal distribution. However, the normality assumption may not be appropriate for all the parameters of interest when analysing prognostic modelling studies, such as predicted survival probabilities and model performance measures.
Methods: Guidelines for combining the estimates of interest when analysing prognostic modelling studies are provided. A literature review is performed to identify current practice for combining such estimates in prognostic modelling studies.
Results: Methods for combining all reported estimates after {MI} were not well reported in the current literature. Rubin's rules without applying any transformations were the standard approach used, when any method was stated.
Conclusion: The proposed simple guidelines for combining estimates after {MI} may lead to a wider and more appropriate use of {MI} in future prognostic modelling studies.},
	pages = {57},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Med Res Methodol},
	author = {Marshall, Andrea and Altman, Douglas G and Holder, Roger L and Royston, Patrick},
	urldate = {2024-03-29},
	date = {2009-12},
	langid = {english},
}

@article{pigott_missing_2016,
	title = {\textit{Missing Data Analysis in Practice} , by Trivellore Raghunathan: Boca Raton, {FL}: {CRC} Press, 2016, \$63.96, {ISBN} 978-1-4822-1192-4},
	volume = {26},
	issn = {1054-3406, 1520-5711},
	url = {https://www.tandfonline.com/doi/full/10.1080/10543406.2016.1226147},
	doi = {10.1080/10543406.2016.1226147},
	shorttitle = {\textit{Missing Data Analysis in Practice} , by Trivellore Raghunathan},
	pages = {1146--1147},
	number = {6},
	journaltitle = {Journal of Biopharmaceutical Statistics},
	shortjournal = {Journal of Biopharmaceutical Statistics},
	author = {Pigott, Terri D.},
	urldate = {2024-03-29},
	date = {2016-11},
	langid = {english},
}

@book{buuren_flexible_2018,
	location = {New York},
	edition = {2},
	title = {Flexible Imputation of Missing Data, Second Edition},
	isbn = {978-0-429-49225-9},
	abstract = {Missing data pose challenges to real-life data analysis. Simple ad-hoc fixes, like deletion or mean imputation, only work under highly restrictive conditions, which are often not met in practice. Multiple imputation replaces each missing value by multiple plausible values. The variability between these replacements reflects our ignorance of the true (but missing) value. Each of the completed data set is then analyzed by standard methods, and the results are pooled to obtain unbiased estimates with correct confidence intervals. Multiple imputation is a general approach that also inspires novel solutions to old problems by reformulating the task at hand as a missing-data problem.

This is the second edition of a popular book on multiple imputation, focused on explaining the application of methods through detailed worked examples using the {MICE} package as developed by the author. This new edition incorporates the recent developments in this fast-moving field.
This class-tested book avoids mathematical and technical details as much as possible: formulas are accompanied by verbal statements that explain the formula in accessible terms. The book sharpens the reader’s intuition on how to think about missing data, and provides all the tools needed to execute a well-grounded quantitative analysis in the presence of missing data.},
	pagetotal = {444},
	publisher = {Chapman and Hall/{CRC}},
	author = {Buuren, Stef van},
	date = {2018-07-12},
	doi = {10.1201/9780429492259},
}

@article{bates_cross-validation_2023,
	title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
	rights = {© 2023 American Statistical Association},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686},
	shorttitle = {Cross-Validation},
	abstract = {1. When deploying a predictive model, it is important to understand its prediction accuracy on future test points, so both good point estimates and accurate confidence intervals for prediction erro...},
	journaltitle = {Journal of the American Statistical Association},
	author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
	urldate = {2024-03-26},
	date = {2023-05-11},
	note = {Publisher: Taylor \& Francis},
}


@online{noauthor_rosenbrock_nodate,
	title = {Rosenbrock Function},
	url = {https://www.sfu.ca/~ssurjano/rosen.html},
	urldate = {2024-03-23},
}

@inproceedings{kohavi_study_1995,
	location = {San Francisco, {CA}, {USA}},
	title = {A study of cross-validation and bootstrap for accuracy estimation and model selection},
	isbn = {1-55860-363-8},
	series = {{IJCAI}'95},
	abstract = {We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment–over half a million runs of C4.5 and a Naive-Bayes algorithm–to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds.},
	pages = {1137--1143},
	booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Kohavi, Ron},
	date = {1995},
	note = {event-place: Montreal, Quebec, Canada},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
}

@online{noauthor_scikit-learn_nodate,
	title = {scikit-learn: machine learning in Python — scikit-learn 1.4.1 documentation},
	url = {https://scikit-learn.org/stable/index.html},
	urldate = {2024-03-21},
}

@online{noauthor_neptuneai_nodate,
	title = {neptune.ai {\textbar} The {MLOps} stack component for experiment tracking},
	url = {https://neptune.ai/},
	abstract = {Log, organize, compare, register, and share all your {ML} model metadata in a single place.},
	titleaddon = {neptune.ai},
	urldate = {2024-03-21},
	langid = {american},
}

@article{kohavi_study_nodate,
	title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
	abstract = {We review accuracy estimation methods and compare the two most common methods: crossvalidation and bootstrap. Recent experimental results on arti cial data and theoretical results in restricted settings have shown that for selecting a good classi er from a set of classiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment{\textbar}over half a million runs of C4.5 and a Naive-Bayes algorithm{\textbar}to estimate the e ects of di erent parameters on these algorithms on real-world datasets. For crossvalidation, we vary the number of folds and whether the folds are strati ed or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold strati ed cross validation, even if computation power allows using more folds.},
	author = {Kohavi, Ron},
	langid = {english},
}

@article{stone_crossvalidatory_1974,
	title = {Cross‐Validatory Choice and Assessment of Statistical Predictions},
	volume = {36},
	issn = {0035-9246, 2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1974.tb00994.x},
	doi = {10.1111/j.2517-6161.1974.tb00994.x},
	abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
	pages = {111--133},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	shortjournal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Stone, M.},
	urldate = {2024-03-16},
	date = {1974-01},
	langid = {english},
}

@inproceedings{astola_small-sample_2005,
	location = {San Diego, California, {USA}},
	title = {Small-sample error estimation: mythology versus mathematics},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.619331},
	doi = {10.1117/12.619331},
	shorttitle = {Small-sample error estimation},
	abstract = {Error estimation is a key aspect of statistical pattern recognition. The true classiﬁcation error rate is usually unavailable since it depends on the unknown feature-label distribution. Hence, one needs to estimate the error rate from the available sample data. This paper presents a concise, mathematically rigorous review of the subject of error estimation in statistical pattern recognition, pointing to the pitfalls that arise in small-sample settings due to the use of “rules of thumb” and a neglect for proper mathematical understanding of the problem.},
	eventtitle = {Optics \& Photonics 2005},
	pages = {59160V},
	author = {Braga-Neto, Ulisses},
	editor = {Astola, Jaakko T. and Tabus, Ioan and Barrera, Junior},
	urldate = {2024-03-16},
	date = {2005-08-18},
	langid = {english},
}

@article{stone_cross-validatory_1974,
	title = {Cross-Validatory Choice and Assessment of Statistical Predictions},
	volume = {36},
	rights = {© 1974 The Authors},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1974.tb00994.x},
	doi = {10.1111/j.2517-6161.1974.tb00994.x},
	abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
	pages = {111--133},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Stone, M.},
	urldate = {2024-03-16},
	date = {1974},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1974.tb00994.x},
	keywords = {analysis of variance, choice of variables, crossvalidation, doublecross, modelmix, multiple regression, prediction, prescription, univariate estimation},
}

@article{rodriguez_general_2013,
	title = {A general framework for the statistical analysis of the sources of variance for classification error estimators},
	volume = {46},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320312003998},
	doi = {10.1016/j.patcog.2012.09.007},
	abstract = {Estimating the prediction error of classiﬁers induced by supervised learning algorithms is important not only to predict its future error, but also to choose a classiﬁer from a given set (model selection). If the goal is to estimate the prediction error of a particular classiﬁer, the desired estimator should have low bias and low variance. However, if the goal is the model selection, in order to make fair comparisons the chosen estimator should have low variance assuming that the bias term is independent from the considered classiﬁer.},
	pages = {855--864},
	number = {3},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Rodríguez, Juan D. and Pérez, Aritz and Lozano, Jose A.},
	urldate = {2024-03-16},
	date = {2013-03},
	langid = {english},
}

@article{rodriguez_sensitivity_2010,
	title = {Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation},
	volume = {32},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/5342427/},
	doi = {10.1109/TPAMI.2009.187},
	pages = {569--575},
	number = {3},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Rodriguez, J.D. and Perez, A. and Lozano, J.A.},
	urldate = {2024-03-16},
	date = {2010-03},
	langid = {english},
}

@article{bengio_no_2004,
	title = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
	abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don’t take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is conﬁrmed by numerical experiments in which the three components of the variance are compared when the difﬁculty of the learning problem and the number of folds are varied.},
	number = {5},
	journaltitle = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Grandvalet, Yves},
	date = {2004},
	langid = {english},
}

@article{wong_dependency_2017,
	title = {Dependency Analysis of Accuracy Estimates in k-Fold Cross Validation},
	volume = {29},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/8012491/},
	doi = {10.1109/TKDE.2017.2740926},
	abstract = {A standard procedure for evaluating the performance of classiﬁcation algorithms is k-fold cross validation. Since the training sets for any pair of iterations in k-fold cross validation are overlapping when the number of folds is larger than two, the resulting accuracy estimates are considered to be dependent. In this paper, the overlapping of training sets is shown to be irrelevant in determining whether two fold accuracies are dependent or not. Then a statistical method is proposed to test the appropriateness of assuming independence for the accuracy estimates in k-fold cross validation. This method is applied on 20 data sets, and the experimental results suggest that it is generally appropriate to assume that the fold accuracies are independent. The cross validation of non-overlapping training sets can make fold accuracies to be dependent. However, this dependence almost has no impact on estimating the sample variance of fold accuracies, and hence they can generally be assumed to be independent.},
	pages = {2417--2427},
	number = {11},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Wong, Tzu-Tsung and Yang, Nai-Yu},
	urldate = {2024-03-06},
	date = {2017-11-01},
	langid = {english},
}

@article{xu_splitting_2018,
	title = {On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning},
	volume = {2},
	issn = {2096-241X, 2509-4696},
	url = {http://link.springer.com/10.1007/s41664-018-0068-2},
	doi = {10.1007/s41664-018-0068-2},
	shorttitle = {On Splitting Training and Validation Set},
	abstract = {Model validation is the most important part of building a supervised model. For building a model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The {MixSim} model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample set partitioning based on joint X–Y distances algorithm ({SPXY}). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and {SPXY} generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
	pages = {249--262},
	number = {3},
	journaltitle = {Journal of Analysis and Testing},
	shortjournal = {J. Anal. Test.},
	author = {Xu, Yun and Goodacre, Royston},
	urldate = {2024-03-06},
	date = {2018-07},
	langid = {english},
}

@article{wong_reliable_2020,
	title = {Reliable Accuracy Estimates from \textit{k} -Fold Cross Validation},
	volume = {32},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/8698831/},
	doi = {10.1109/TKDE.2019.2912815},
	abstract = {It is popular to evaluate the performance of classiﬁcation algorithms by k-fold cross validation. A reliable accuracy estimate will have a relatively small variance, and several studies therefore suggested to repeatedly perform k-fold cross validation. Most of them did not consider the correlation among the replications of k-fold cross validation, and hence the variance could be underestimated. The purpose of this study is to explore whether k-fold cross validation should be repeatedly performed for obtaining reliable accuracy estimates. The dependency relationships between the predictions of the same instance in two replications of k-fold cross validation are ﬁrst analyzed for k-nearest neighbors with k ¼ 1. Then, statistical methods are proposed to test the strength of the dependency level between the accuracy estimates resulting from two replications of k-fold cross validation. The experimental results on 20 data sets show that the accuracy estimates obtained from various replications of k-fold cross validation are generally highly correlated, and the correlation will be higher as the number of folds increases. The k-fold cross validation with a large number of folds and a small number of replications should be adopted for performance evaluation of classiﬁcation algorithms.},
	pages = {1586--1594},
	number = {8},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Wong, Tzu-Tsung and Yeh, Po-Yang},
	urldate = {2024-03-06},
	date = {2020-08-01},
	langid = {english},
}

@misc{varoquaux_cross-validation_2017,
	title = {Cross-validation failure: small sample sizes lead to large error bars},
	url = {http://arxiv.org/abs/1706.07581},
	shorttitle = {Cross-validation failure},
	abstract = {Predictive models ground many state-of-the-art developments in statistical brain image analysis: decoding, {MVPA}, searchlight, or extraction of biomarkers. The principled approach to establish their validity and usefulness is crossvalidation, testing prediction on unseen data. Here, I would like to raise awareness on error bars of cross-validation, which are often underestimated. Simple experiments show that sample sizes of many neuroimaging studies inherently lead to large error bars, eg ±10\% for 100 samples. The standard error across folds strongly underestimates them. These large error bars compromise the reliability of conclusions drawn with predictive models, such as biomarkers or methods developments where, unlike with cognitive neuroimaging {MVPA} approaches, more samples cannot be acquired by repeating the experiment across many subjects. Solutions to increase sample size must be investigated, tackling possible increases in heterogeneity of the data.},
	number = {{arXiv}:1706.07581},
	publisher = {{arXiv}},
	author = {Varoquaux, Gaël},
	urldate = {2024-03-06},
	date = {2017-06-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.07581 [q-bio, stat]},
	keywords = {Quantitative Biology - Quantitative Methods, Statistics - Machine Learning, Statistics - Methodology},
}

@article{santos_cross-validation_2018,
	title = {Cross-Validation for Imbalanced Datasets: Avoiding Overoptimistic and Overfitting Approaches [Research Frontier]},
	volume = {13},
	issn = {1556-603X, 1556-6048},
	url = {https://ieeexplore.ieee.org/document/8492368/},
	doi = {10.1109/MCI.2018.2866730},
	shorttitle = {Cross-Validation for Imbalanced Datasets},
	pages = {59--76},
	number = {4},
	journaltitle = {{IEEE} Computational Intelligence Magazine},
	shortjournal = {{IEEE} Comput. Intell. Mag.},
	author = {Santos, Miriam Seoane and Soares, Jastin Pompeu and Abreu, Pedro Henrigues and Araujo, Helder and Santos, Joao},
	urldate = {2024-03-06},
	date = {2018-11},
	langid = {english},
}

@article{wong_reliable_2020-1,
	title = {Reliable Accuracy Estimates from \textit{k} -Fold Cross Validation},
	volume = {32},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/8698831/},
	doi = {10.1109/TKDE.2019.2912815},
	abstract = {It is popular to evaluate the performance of classiﬁcation algorithms by k-fold cross validation. A reliable accuracy estimate will have a relatively small variance, and several studies therefore suggested to repeatedly perform k-fold cross validation. Most of them did not consider the correlation among the replications of k-fold cross validation, and hence the variance could be underestimated. The purpose of this study is to explore whether k-fold cross validation should be repeatedly performed for obtaining reliable accuracy estimates. The dependency relationships between the predictions of the same instance in two replications of k-fold cross validation are ﬁrst analyzed for k-nearest neighbors with k ¼ 1. Then, statistical methods are proposed to test the strength of the dependency level between the accuracy estimates resulting from two replications of k-fold cross validation. The experimental results on 20 data sets show that the accuracy estimates obtained from various replications of k-fold cross validation are generally highly correlated, and the correlation will be higher as the number of folds increases. The k-fold cross validation with a large number of folds and a small number of replications should be adopted for performance evaluation of classiﬁcation algorithms.},
	pages = {1586--1594},
	number = {8},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Wong, Tzu-Tsung and Yeh, Po-Yang},
	urldate = {2024-03-06},
	date = {2020-08-01},
	langid = {english},
}

@article{de_rooij_cross-validation_2020,
	title = {Cross-Validation: A Method Every Psychologist Should Know},
	volume = {3},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245919898466},
	doi = {10.1177/2515245919898466},
	shorttitle = {Cross-Validation},
	abstract = {Cross-validation is a statistical procedure that every psychologist should know. Most are possibly familiar with the procedure in a global way but have not used it for the analysis of their own data. We introduce cross-validation for the purpose of model selection in a general sense, as well as an R package we have developed for this kind of analysis, and we present examples illustrating the use of this package for types of research problems that are often encountered in the social sciences. Cross-validation can be an easy-to-use alternative to null-hypothesis testing, and it has the benefit that it does not make as many assumptions.},
	pages = {248--263},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	shortjournal = {Advances in Methods and Practices in Psychological Science},
	author = {De Rooij, Mark and Weeda, Wouter},
	urldate = {2024-03-06},
	date = {2020-06},
	langid = {english},
}

@misc{austern_asymptotics_2020,
	title = {Asymptotics of Cross-Validation},
	url = {http://arxiv.org/abs/2001.11111},
	abstract = {Cross validation is a central tool in evaluating the performance of machine learning and statistical models. However, despite its ubiquitous role, its theoretical properties are still not well understood. We study the asymptotic properties of the cross validated-risk for a large class of models. Under stability conditions, we establish a central limit theorem and Berry-Esseen bounds, which enable us to compute asymptotically accurate conﬁdence intervals. Using our results, we paint a big picture for the statistical speed-up of cross validation compared to a train-test split procedure. A corollary of our results is that parametric M-estimators (or empirical risk minimizers) beneﬁt from the “full” speed-up when performing cross-validation under the training loss. In other common cases, such as when the training is performed using a surrogate loss or a regularizer, we show that the behavior of the cross-validated risk is complex with a variance reduction which may be smaller or larger than the “full” speed-up, depending on the model and the underlying distribution. We allow the number of folds Kn to grow with the number of observations at any rate.},
	number = {{arXiv}:2001.11111},
	publisher = {{arXiv}},
	author = {Austern, Morgane and Zhou, Wenda},
	urldate = {2024-03-06},
	date = {2020-06-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.11111 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@thesis{westphal_model_2019,
	title = {Model Selection and Evaluation in Supervised Machine Learning},
	rights = {Attribution 3.0 Germany,},
	url = {https://media.suub.uni-bremen.de/handle/elib/4231},
	abstract = {In this thesis, we propose new model evaluation strategies for supervised machine learning. Our main goal is to reliably and efficiently infer the generalization performance of one or multiple prediction models based on limited data. So far, a strict separation of model selection and performance assessment has been recommended. While this approach is valid, it lacks flexibility as a flawed model selection can usually not be corrected without compromising the statistical inference. We suggest to evaluate multiple promising models on the test dataset, thereby taking into account more observations for the final selection process. We employ a parametric simultaneous test procedure to adjust the inferences (test decisions, point estimates) for multiple comparisons. We extend this method to enable a simultaneous evaluation of multiple binary classifiers with regard to sensitivity and specificity as co-primary endpoints. In both cases, approximate control of the family-wise error rate is warranted. Besides this established frequentist procedure, we propose a new multivariate Beta-binomial model for the analysis of multiple proportions with general correlation structure. This Bayesian approach allows to incorporate prior knowledge to the inference task. Finally, we derive a new decision rule for subset selection problems. Our method is developed in the framework of Bayesian decision theory by employing a novel utility function. Compared to previous approaches, this method is computationally more complex but hyperparameter-free. We illustrate in extensive simulation studies that our framework can improve the expected final model performance and statistical power, the probability to correctly identify a sufficiently good model. While an unbiased point estimation is no longer possible, the selection-induced bias can be corrected in a conservative manner. The family-wise error rate is controlled under realistic parameter configurations given that a moderate number of observations is available. We conclude that the test data can be used for model selection when suitable adjustments for multiple comparisons are applied. This increases the flexibility and statistical efficiency compared to traditional approaches. Our framework can help to prevent the implementation of flawed models into sensitive and large-scale applications while at the same time reliably identifying truly capable solutions.},
	institution = {Universität Bremen},
	type = {phdthesis},
	author = {Westphal, Max},
	editora = {{Universität Bremen} and {Universität Bremen} and Brannath, Werner},
	editoratype = {collaborator},
	urldate = {2024-03-30},
	date = {2019-12-16},
	langid = {english},
	doi = {10.26092/ELIB/16},
	keywords = {510, Bayesian inference, artificial intelligence, bias, classification, co-primary endpoints, decision theory, diagnosis, diagnostic accuracy, hypothesis testing, multiple comparisons, performance assessment, predictive modelling, prognosis, regulation, subset selection, uncertainty quantification},
}

@article{yousef_estimating_2021,
	title = {Estimating the standard error of cross-Validation-Based estimators of classifier performance},
	volume = {146},
	issn = {01678655},
	url = {http://arxiv.org/abs/1908.00325},
	doi = {10.1016/j.patrec.2021.02.022},
	abstract = {First, we analyze the variance of the Cross Validation ({CV})-based estimators used for estimating the performance of classiﬁcation rules. Second, we propose a novel estimator to estimate this variance using the Inﬂuence Function ({IF}) approach that had been used previously very successfully to estimate the variance of the bootstrap-based estimators. The motivation for this research is that, as the best of our knowledge, the literature lacks a rigorous method for estimating the variance of the {CV}-based estimators. What is available is a set of ad-hoc procedures that have no mathematical foundation since they ignore the covariance structure among dependent random variables. The conducted experiments show that the {IF} proposed method has small {RMS} error with some bias. However, surprisingly, the ad-hoc methods still work better than the {IF}-based method. Unfortunately, this is due to the lack of enough smoothness if compared to the bootstrap estimator. This opens the research for three points: (1) more comprehensive simulation study to clarify when the {IF} method win or loose; (2) more mathematical analysis to ﬁgure out why the ad-hoc methods work well; and (3) more mathematical treatment to ﬁgure out the connection between the appropriate amount of “smoothness” and decreasing the bias of the {IF} method.},
	pages = {115--125},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Yousef, Waleed A.},
	urldate = {2024-03-30},
	date = {2021-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1908.00325 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{efron_improvements_1997,
	title = {Improvements on Cross-Validation: The .632+ Bootstrap Method},
	volume = {92},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2965703?origin=crossref},
	doi = {10.2307/2965703},
	shorttitle = {Improvements on Cross-Validation},
	abstract = {proposals to estimate the standard error of the (bootstrap) point estimates of prediction error with methods based on influence functions},
	pages = {548},
	number = {438},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Efron, Bradley and Tibshirani, Robert},
	urldate = {2024-03-28},
	date = {1997-06},
	langid = {english},
}

@misc{rosset_fixed-x_2017,
	title = {From Fixed-X to Random-X Regression: Bias-Variance Decompositions, Covariance Penalties, and Prediction Error Estimation},
	url = {http://arxiv.org/abs/1704.08160},
	shorttitle = {From Fixed-X to Random-X Regression},
	abstract = {In statistical prediction, classical approaches for model selection and model evaluation based on covariance penalties are still widely used. Most of the literature on this topic is based on what we call the “Fixed-X” assumption, where covariate values are assumed to be nonrandom. By contrast, it is often more reasonable to take a “Random-X” view, where the covariate values are independently drawn for both training and prediction. To study the applicability of covariance penalties in this setting, we propose a decomposition of Random-X prediction error in which the randomness in the covariates contributes to both the bias and variance components. This decomposition is general, but we concentrate on the fundamental case of least squares regression. We prove that in this setting the move from Fixed-X to Random-X prediction results in an increase in both bias and variance. When the covariates are normally distributed and the linear model is unbiased, all terms in this decomposition are explicitly computable, which yields an extension of Mallows’ Cp that we call {RCp}. {RCp} also holds asymptotically for certain classes of nonnormal covariates. When the noise variance is unknown, plugging in the usual unbiased estimate leads to an approach that we call {RCp}, which is closely related to Sp (Tukey 1967), and {GCV} (Craven and Wahba 1978). For excess bias, we propose an estimate based on the “shortcut-formula” for ordinary cross-validation ({OCV}), resulting in an approach we call {RCp}+. Theoretical arguments and numerical simulations suggest that {RCp}+ is typically superior to {OCV}, though the diﬀerence is small. We further examine the Random-X error of other popular estimators. The surprising result we get for ridge regression is that, in the heavily-regularized regime, Random-X variance is smaller than Fixed-X variance, which can lead to smaller overall Random-X error.},
	number = {{arXiv}:1704.08160},
	publisher = {{arXiv}},
	author = {Rosset, Saharon and Tibshirani, Ryan J.},
	urldate = {2024-03-30},
	date = {2017-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1704.08160 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{wager_cross-validation_2019,
	title = {Cross-Validation, Risk Estimation, and Model Selection},
	url = {http://arxiv.org/abs/1909.11696},
	abstract = {Cross-validation is a popular non-parametric method for evaluating the accuracy of a predictive rule. The usefulness of cross-validation depends on the task we want to employ it for. In this note, I discuss a simple non-parametric setting, and ﬁnd that cross-validation is asymptotically uninformative about the expected test error of any given predictive rule, but allows for asymptotically consistent model selection. The reason for this phenomenon is that the leading-order error term of cross-validation doesn’t depend on the model being evaluated, and so cancels out when we compare two models. This note was prepared as a comment on a paper by Rosset and Tibshirani, forthcoming in the Journal of the American Statistical Association.},
	number = {{arXiv}:1909.11696},
	publisher = {{arXiv}},
	author = {Wager, Stefan},
	urldate = {2024-03-30},
	date = {2019-09-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.11696 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{sivula_uncertainty_2023,
	title = {Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison},
	url = {http://arxiv.org/abs/2008.10296},
	abstract = {Leave-one-out cross-validation ({LOO}-{CV}) is a popular method for comparing Bayesian models based on their estimated predictive performance on new, unseen, data. As leave-oneout cross-validation is based on finite observed data, there is uncertainty about the expected predictive performance on new data. By modeling this uncertainty when comparing two models, we can compute the probability that one model has a better predictive performance than the other. Modeling this uncertainty well is not trivial, and for example, it is known that the commonly used standard error estimate is often too small. We study the properties of the Bayesian {LOO}-{CV} estimator and the related uncertainty estimates when comparing two models. We provide new results of the properties both theoretically in the linear regression case and empirically for multiple different models and discuss the challenges of modeling the uncertainty. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection in the skewness of the individual leave-one-out terms and the distribution of the error of the Bayesian {LOO}-{CV} estimator. We show that it is possible that the problematic skewness of the error distribution, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide practical recommendations for the users of Bayesian {LOO}-{CV} for model comparison.},
	number = {{arXiv}:2008.10296},
	publisher = {{arXiv}},
	author = {Sivula, Tuomas and Magnusson, Måns and Matamoros, Asael Alonzo and Vehtari, Aki},
	urldate = {2024-03-24},
	date = {2023-10-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.10296 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{sivula_unbiased_2022,
	title = {Unbiased estimator for the variance of the leave-one-out cross-validation estimator for a Bayesian normal model with fixed variance},
	url = {http://arxiv.org/abs/2008.10859},
	abstract = {When evaluating and comparing models using leave-one-out cross-validation ({LOO}-{CV}), the uncertainty of the estimate is typically assessed using the variance of the sampling distribution. Considering the uncertainty is important, as the variability of the estimate can be high in some cases. An important result by Bengio and Grandvalet (2004) states that no general unbiased variance estimator can be constructed, that would apply for any utility or loss measure and any model. We show that it is possible to construct an unbiased estimator considering a speciﬁc predictive performance measure and model. We demonstrate an unbiased sampling distribution variance estimator for the Bayesian normal model with ﬁxed model variance using the expected log pointwise predictive density (elpd) utility score. This example demonstrates that it is possible to obtain improved, problem-speciﬁc, unbiased estimators for assessing the uncertainty in {LOO}-{CV} estimation.},
	number = {{arXiv}:2008.10859},
	publisher = {{arXiv}},
	author = {Sivula, Tuomas and Magnusson, Måns and Vehtari, Aki},
	urldate = {2024-03-24},
	date = {2022-02-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.10859 [stat]},
	keywords = {Statistics - Methodology},
}

@article{gelman_what_2023,
	title = {What is a standard error?},
	volume = {237},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407623002324},
	doi = {10.1016/j.jeconom.2023.105516},
	abstract = {The appropriate standard error depends not just on the data and sampling model but also on the generalization of interest, and the model of variation across units and over time corresponding to the uses to which the estimate will be put. Deciding on a generalization of interest in a sampling or regression problem is similar to the problem of focusing on a particular average treatment eﬀect in causal inference: thinking seriously about your replications (for the goal of getting the right standard error) and inferential goals, you might well get a better understanding of what you’re trying to do with your model.},
	pages = {105516},
	number = {1},
	journaltitle = {Journal of Econometrics},
	shortjournal = {Journal of Econometrics},
	author = {Gelman, Andrew},
	urldate = {2024-03-24},
	date = {2023-11},
	langid = {english},
}

@misc{kuh_using_2022,
	title = {Using leave-one-out cross-validation ({LOO}) in a multilevel regression and poststratification ({MRP}) workflow: A cautionary tale},
	url = {http://arxiv.org/abs/2209.01773},
	shorttitle = {Using leave-one-out cross-validation ({LOO}) in a multilevel regression and poststratification ({MRP}) workflow},
	abstract = {In recent decades, multilevel regression and poststratiﬁcation ({MRP}) has surged in popularity for population inference. However, the validity of the estimates can depend on details of the model, and there is currently little research on validation. We explore how leave-one-out cross-validation ({LOO}) can be used to compare Bayesian models for {MRP}. We investigate two approximate calculations of {LOO}, the Pareto smoothed importance sampling ({PSIS}-{LOO}) and a survey-weighted alternative ({WTD}-{PSIS}-{LOO}). Using two simulation designs, we examine how accurately these two criteria recover the correct ordering of model goodness at predicting population and small area level estimands. Focusing ﬁrst on variable selection, we ﬁnd that neither {PSIS}-{LOO} nor {WTD}-{PSIS}-{LOO} correctly recovers the models’ order for an {MRP} population estimand (although both criteria correctly identify the best and worst model). When considering small-area estimation, the best model differs for different small areas, highlighting the complexity of {MRP} validation. When considering different priors, the models’ order seems slightly better at smaller area levels. These ﬁndings suggest that while not terrible, {PSISLOO}-based ranking techniques may not be suitable to evaluate {MRP} as a method. We suggest this is due to the aggregation stage of {MRP}, where individual-level prediction errors average out. These results show that in practice, {PSIS}-{LOO}-based model validation tools need to be used with caution and might not convey the full story when validating {MRP} as a method.},
	number = {{arXiv}:2209.01773},
	publisher = {{arXiv}},
	author = {Kuh, Swen and Kennedy, Lauren and Chen, Qixuan and Gelman, Andrew},
	urldate = {2024-03-24},
	date = {2022-09-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.01773 [stat]},
	keywords = {Statistics - Methodology},
}

@article{yao_using_2018,
	title = {Using Stacking to Average Bayesian Predictive Distributions (with Discussion)},
	volume = {13},
	issn = {1936-0975},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Using-Stacking-to-Average-Bayesian-Predictive-Distributions-with-Discussion/10.1214/17-BA1091.full},
	doi = {10.1214/17-BA1091},
	abstract = {Bayesian model averaging is ﬂawed in the M-open setting in which the true data-generating process is not one of the candidate models being ﬁt. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to eﬃciently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging ({BMA}), Pseudo-{BMA}, and a variant of Pseudo-{BMA} that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-{BMA} as an approximate alternative when computation cost is an issue.},
	number = {3},
	journaltitle = {Bayesian Analysis},
	shortjournal = {Bayesian Anal.},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	urldate = {2024-03-24},
	date = {2018-09-01},
	langid = {english},
}

@article{gelman_are_2019,
	title = {Are confidence intervals better termed “uncertainty intervals”?},
	issn = {0959-8138, 1756-1833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.l5381},
	doi = {10.1136/bmj.l5381},
	pages = {l5381},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Gelman, Andrew and Greenland, Sander},
	urldate = {2024-03-24},
	date = {2019-09-10},
	langid = {english},
}

@article{ball_shadow_1990,
	title = {In the shadow of Vesuvius},
	volume = {345},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/345479a0},
	doi = {10.1038/345479a0},
	pages = {479--479},
	number = {6275},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Ball, Philip},
	urldate = {2024-03-24},
	date = {1990-06},
	langid = {english},
}

@article{davison_recent_2003,
	title = {Recent Developments in Bootstrap Methodology},
	volume = {18},
	url = {http://www.jstor.org/stable/3182844},
	abstract = {Ever sinceits introductionth, ebootstraphas providedbotha powerfuslet of solutionsforpracticalstatisticianas,nd a richsourceof {theoreticaalndmethodologicaplroblemsforstatisticsI}.n thisarticles,ome recentdevelopmentisnbootstramp ethodologayrereviewedanddiscussed. Aftera briefintroductiotnothebootstrapw, e considerthefollowingtopics at varyinglevels of detail:theuse of bootstrappinfgorhighlyaccurate parametriicnferencet;heoreticaplropertieosf nonparametrbioc otstrapping withunequalprobabilitiess;ubsamplingand the m out of n bootstrap; bootstrapfailuresand remediesforsuperefficieensttimatorsr;ecenttopics in significancteestingb; ootstrapimprovemenotfs unstableclassifierasnd resamplingfordependentdata. The treatmenits telegraphircatherthan exhaustive.},
	pages = {141--157},
	number = {2},
	journaltitle = {Statistical Science},
	author = {Davison, A. C. and Hinkley, D. V. and Young, G. A.},
	date = {2003},
	langid = {english},
	keywords = {bootstrap},
}

@misc{raschka_model_2020,
	title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
	url = {http://arxiv.org/abs/1811.12808},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different ﬂavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to conﬁdence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F -test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	number = {{arXiv}:1811.12808},
	publisher = {{arXiv}},
	author = {Raschka, Sebastian},
	urldate = {2024-03-11},
	date = {2020-11-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1811.12808 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, bootstrap},
}

@article{ren_nonparametric_2010,
	title = {Nonparametric bootstrapping for hierarchical data},
	volume = {37},
	issn = {0266-4763, 1360-0532},
	url = {http://www.tandfonline.com/doi/full/10.1080/02664760903046102},
	doi = {10.1080/02664760903046102},
	pages = {1487--1498},
	number = {9},
	journaltitle = {Journal of Applied Statistics},
	shortjournal = {Journal of Applied Statistics},
	author = {Ren, Shiquan and Lai, Hong and Tong, Wenjing and Aminzadeh, Mostafa and Hou, Xuezhang and Lai, Shenghan},
	urldate = {2024-03-11},
	date = {2010-09},
	langid = {english},
	keywords = {bootstrap},
}

@article{psaros_uncertainty_2023,
	title = {Uncertainty Quantification in Scientific Machine Learning: Methods, Metrics, and Comparisons},
	volume = {477},
	issn = {00219991},
	url = {http://arxiv.org/abs/2201.07766},
	doi = {10.1016/j.jcp.2022.111902},
	shorttitle = {Uncertainty Quantification in Scientific Machine Learning},
	abstract = {Neural networks ({NNs}) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in {NN}-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to {NN} hyperparameters, overparametrization, optimization and sampling errors as well as model misspeciﬁcation. Although there are some recent works on uncertainty quantiﬁcation ({UQ}) in {NNs}, there is no systematic investigation of suitable methods towards quantifying the total uncertainty eﬀectively and eﬃciently even for function approximation, and there is even less work on solving partial diﬀerential equations and learning operator mappings between inﬁnite-dimensional function spaces using {NNs}. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the {UQ} methods employed, which we will make available as open-source library of all codes included in this framework.},
	pages = {111902},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Psaros, Apostolos F. and Meng, Xuhui and Zou, Zongren and Guo, Ling and Karniadakis, George Em},
	urldate = {2024-03-23},
	date = {2023-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2201.07766 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{vovk_conditional_2012,
	title = {Conditional validity of inductive conformal predictors},
	url = {http://arxiv.org/abs/1209.2673},
	abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given conﬁdence level. Inductive conformal predictors are a computationally eﬃcient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have been only known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modiﬁcations.},
	number = {{arXiv}:1209.2673},
	publisher = {{arXiv}},
	author = {Vovk, Vladimir},
	urldate = {2024-03-23},
	date = {2012-09-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1209.2673 [cs]},
	keywords = {68T05, 62G15, Computer Science - Machine Learning},
}

@misc{barber_limits_2020,
	title = {The limits of distribution-free conditional predictive inference},
	url = {http://arxiv.org/abs/1903.04684},
	abstract = {We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally rather than marginally. Existing methods such as conformal prediction offer marginal coverage guarantees, where predictive coverage holds on average over all possible test points, but this is not suﬃcient for many practical applications where we would like to know that our predictions are valid for a given individual, not merely on average over a population. On the other hand, exact conditional inference guarantees are known to be impossible without imposing assumptions on the underlying distribution. In this work we aim to explore the space in between these two, and examine what types of relaxations of the conditional coverage property would alleviate some of the practical concerns with marginal coverage guarantees while still being possible to achieve in a distribution-free setting.},
	number = {{arXiv}:1903.04684},
	publisher = {{arXiv}},
	author = {Barber, Rina Foygel and Candès, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	urldate = {2024-03-23},
	date = {2020-04-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.04684 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@online{noauthor_putting_nodate,
	title = {Putting clear bounds on uncertainty},
	url = {https://news.mit.edu/2023/putting-clear-bounds-uncertainty-0123},
}

@online{noauthor_measuring_nodate,
	title = {Measuring Models' Uncertainty: Conformal Prediction},
	url = {https://blog.dataiku.com/measuring-models-uncertainty-conformal-prediction},
}

@unpublished{zaffran_conformal_2023,
	location = {Valencia},
	title = {Conformal Prediction: How to quantify uncertainty of machine learning models?},
	note = {{ECAS}-{ENBIS} Course},
	author = {Zaffran, Margaux},
	date = {2023},
	langid = {english},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
	volume = {76},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253521001081},
	doi = {10.1016/j.inffus.2021.05.008},
	shorttitle = {A review of uncertainty quantification in deep learning},
	abstract = {Uncertainty quantiﬁcation ({UQ}) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two of the most widely-used types of uncertainty quantiﬁcation ({UQ}) methods. In this regard, researchers have proposed diﬀerent {UQ} methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classiﬁcation and segmentation), natural language processing (e.g., text classiﬁcation, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in {UQ} methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights the fundamental research challenges and directions associated with the {UQ} ﬁeld.},
	pages = {243--297},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	urldate = {2024-03-22},
	date = {2021-12},
	langid = {english},
}

@misc{sensoy_evidential_2018,
	title = {Evidential Deep Learning to Quantify Classification Uncertainty},
	url = {http://arxiv.org/abs/1806.01768},
	abstract = {Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction conﬁdence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classiﬁcation problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of outof-distribution queries and endurance against adversarial perturbations.},
	number = {{arXiv}:1806.01768},
	publisher = {{arXiv}},
	author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
	urldate = {2024-03-22},
	date = {2018-10-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.01768 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{buddenkotte_calibrating_2023,
	title = {Calibrating ensembles for scalable uncertainty quantification in deep learning-based medical image segmentation},
	volume = {163},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482523005619},
	doi = {10.1016/j.compbiomed.2023.107096},
	abstract = {Uncertainty quantification in automated image analysis is highly desired in many applications. Typically, machine learning models in classification or segmentation are only developed to provide binary answers; however, quantifying the uncertainty of the models can play a critical role for example in active learning or machine human interaction. Uncertainty quantification is especially difficult when using deep learning-based models, which are the state-of-the-art in many imaging applications. The current uncertainty quantification approaches do not scale well in high-dimensional real-world problems. Scalable solutions often rely on classical techniques, such as dropout, during inference or training ensembles of identical models with different random seeds to obtain a posterior distribution. In this paper, we present the following contributions. First, we show that the classical approaches fail to approximate the classification probability. Second, we propose a scalable and intuitive framework for uncertainty quantification in medical image segmentation that yields measurements that approximate the classification probability. Third, we suggest the usage of ��-fold crossvalidation to overcome the need for held out calibration data. Lastly, we motivate the adoption of our method in active learning, creating pseudo-labels to learn from unlabeled images and human–machine collaboration.},
	pages = {107096},
	journaltitle = {Computers in Biology and Medicine},
	shortjournal = {Computers in Biology and Medicine},
	author = {Buddenkotte, Thomas and Escudero Sanchez, Lorena and Crispin-Ortuzar, Mireia and Woitek, Ramona and {McCague}, Cathal and Brenton, James D. and Öktem, Ozan and Sala, Evis and Rundo, Leonardo},
	urldate = {2024-03-22},
	date = {2023-09},
	langid = {english},
}

@misc{ovadia_can_2019,
	title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
	url = {http://arxiv.org/abs/1906.02530},
	shorttitle = {Can You Trust Your Model's Uncertainty?},
	abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model’s output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and {nonBayesian} methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classiﬁcation problems and investigate the effect of dataset shift on accuracy and calibration. We ﬁnd that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
	number = {{arXiv}:1906.02530},
	publisher = {{arXiv}},
	author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
	urldate = {2024-03-22},
	date = {2019-12-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.02530 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gupta_distribution-free_nodate,
	title = {Distribution-free binary classiﬁcation: prediction sets, conﬁdence intervals and calibration},
	abstract = {We study three notions of uncertainty quantiﬁcation—calibration, conﬁdence intervals and prediction sets—for binary classiﬁcation in the distribution-free setting, that is without making any distributional assumptions on the data. With a focus towards calibration, we establish a ‘tripod’ of theorems that connect these three notions for score-based classiﬁers. A direct implication is that distributionfree calibration is only possible, even asymptotically, using a scoring function whose level sets partition the feature space into at most countably many sets. Parametric calibration schemes such as variants of Platt scaling do not satisfy this requirement, while nonparametric schemes based on binning do. To close the loop, we derive distribution-free conﬁdence intervals for binned probabilities for both ﬁxed-width and uniform-mass binning. As a consequence of our ‘tripod’ theorems, these conﬁdence intervals for binned probabilities lead to distributionfree calibration. We also derive extensions to settings with streaming data and covariate shift.},
	author = {Gupta, Chirag and Podkopaev, Aleksandr and Ramdas, Aaditya},
	langid = {english},
}

@misc{angelopoulos_gentle_2022,
	title = {A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification},
	url = {http://arxiv.org/abs/2107.07511},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantiﬁcation to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-speciﬁed probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the ﬁelds of computer vision, natural language processing, deep reinforcement learning, and so on.},
	number = {{arXiv}:2107.07511},
	publisher = {{arXiv}},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	urldate = {2024-03-22},
	date = {2022-12-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2107.07511 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@book{lofstrom_trustworthy_2023,
	location = {Jönköping},
	title = {Trustworthy explanations: Improved decision support through well-calibrated uncertainty quantification},
	isbn = {978-91-7914-031-1},
	shorttitle = {Trustworthy explanations},
	publisher = {Jönköping University, Jönköping International Business School},
	author = {Löfström, Helena},
	date = {2023},
	langid = {english},
	note = {{OCLC}: 1408765849},
}

@misc{vovk_cross-conformal_2012,
	title = {Cross-conformal predictors},
	url = {http://arxiv.org/abs/1208.0806},
	abstract = {This note introduces the method of cross-conformal prediction, which is a hybrid of the methods of inductive conformal prediction and crossvalidation, and studies its validity and predictive eﬃciency empirically.},
	number = {{arXiv}:1208.0806},
	publisher = {{arXiv}},
	author = {Vovk, Vladimir},
	urldate = {2024-03-22},
	date = {2012-08-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1208.0806 [cs, stat]},
	keywords = {62G15, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{akiba_optuna_2019,
	title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	date = {2019},
}

@article{westphal_improving_2019,
	title = {Improving Model Selection by Employing the Test Data},
	author = {Westphal, Max and Brannath, Werner},
	date = {2019},
	langid = {english},
}

@article{westphal_evaluation_2020,
	title = {Evaluation of multiple prediction models: A novel view on model selection and performance assessment},
	volume = {29},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280219854487},
	doi = {10.1177/0962280219854487},
	shorttitle = {Evaluation of multiple prediction models},
	abstract = {Model selection and performance assessment for prediction models are important tasks in machine learning, e.g. for the development of medical diagnosis or prognosis rules based on complex data. A common approach is to select the best model via cross-validation and to evaluate this final model on an independent dataset. In this work, we propose to instead evaluate several models simultaneously. These may result from varied hyperparameters or completely different learning algorithms. Our main goal is to increase the probability to correctly identify a model that performs sufficiently well. In this case, adjusting for multiplicity is necessary in the evaluation stage to avoid an inflation of the family wise error rate. We apply the so-called {maxT}-approach which is based on the joint distribution of test statistics and suitable to (approximately) control the family-wise error rate for a wide variety of performance measures. We conclude that evaluating only a single final model is suboptimal. Instead, several promising models should be evaluated simultaneously, e.g. all models within one standard error of the best validation model. This strategy has proven to increase the probability to correctly identify a good model as well as the final model performance in extensive simulation studies.},
	pages = {1728--1745},
	number = {6},
	journaltitle = {Statistical Methods in Medical Research},
	shortjournal = {Stat Methods Med Res},
	author = {Westphal, Max and Brannath, Werner},
	urldate = {2024-03-16},
	date = {2020-06},
	langid = {english},
}


@article{westphal_applied_2023,
	title = {Applied Statistics {MSc} Thesis Topics at Fraunhofer {MEVIS} (Bremen)},
	author = {Westphal, Max},
	date = {2023},
	langid = {english},
}

@article{singh_significance_2016,
	title = {Significance of non-parametric statistical tests for comparison of classifiers over multiple datasets},
	abstract = {In machine learning, generation of new algorithms or, in most cases, minor amendment of the existing ones is a common task. In such cases, a rigorous and correct statistical analysis of the results of different algorithms is necessary in order to select the exact technique(s) depending on the problem to be solved. The main inconvenience related to this necessity is the absence of proper compilation of statistical techniques. In this paper, we propose the use of two important non-parametric statistical tests, namely, Wilcoxon signed rank test for comparison of two classifiers and Friedman test with the corresponding post-hoc tests for comparison of multiple classifiers over multiple datasets. We also introduce a new variant of non-parametric test known as Scheffe’s test for locating unequal pairs of means of performances of multiple classifiers when the given datasets are of unequal sizes. The parametric tests, which were previously being used for comparing multiple classifiers, have also been described in brief. The proposed non-parametric tests have also been applied on the classification results on ten real-problem datasets taken from the {UCI} Machine Learning Database Repository (http://www.ics.uci.edu/mlearn) (Valdovinos and Sanchez, 2009) as case studies.},
	author = {Singh, Pawan Kumar and Sarkar, Ram and Nasipuri, Mita},
	date = {2016},
	langid = {english},
}

@article{bouckaert_choosing_2003,
	title = {Choosing Between Two Learning Algorithms Based on Calibrated Tests},
	abstract = {Designing a hypothesis test to determine the best of two machine learning algorithms with only a small data set available is not a simple task. Many popular tests suﬀer from low power (5x2 cv [2]), or high Type I error (Weka’s 10x10 cross validation [11]). Furthermore, many tests show a low level of replicability, so that tests performed by different scientists with the same pair of algorithms, the same data sets and the same hypothesis test still may present diﬀerent results. We show that 5x2 cv, resampling and 10 fold cv suﬀer from low replicability.},
	author = {Bouckaert, Remco R},
	date = {2003},
	langid = {english},
}

@book{tikhonov_numerical_1995,
	location = {Dordrecht},
	title = {Numerical Methods for the Solution of Ill-Posed Problems},
	isbn = {978-0-7923-3583-2},
	abstract = {Many problems in science, technology and engineering are posed in the form of operator equations of the first kind, with the operator and {RHS} approximately known. But such problems often turn out to be ill-posed, having no solution, or a non-unique solution, and/or an unstable solution. Non-existence and non-uniqueness can usually be overcome by settling for `generalised' solutions, leading to the need to develop regularising algorithms.  The theory of ill-posed problems has advanced greatly since A. N. Tikhonov laid its foundations, the Russian original of this book (1990) rapidly becoming a classical monograph on the topic. The present edition has been completely updated to consider linear ill-posed problems with or without a priori constraints (non-negativity, monotonicity, convexity, etc.).  Besides the theoretical material, the book also contains a {FORTRAN} program library.  Audience: Postgraduate students of physics, mathematics, chemistry, economics, engineering. Engineers and scientists interested in data processing and the theory of ill-posed problems.},
	pagetotal = {274},
	publisher = {Springer Science \& Business Media},
	author = {Tikhonov, A. N. and Goncharsky, A. and Stepanov, V. V. and Yagola, Anatoly G.},
	date = {1995-06-30},
	langid = {english},
	keywords = {Computers / Programming / Algorithms, Mathematics / Applied, Mathematics / Calculus, Mathematics / Counting \& Numeration, Mathematics / Functional Analysis, Mathematics / Mathematical Analysis, Mathematics / Numerical Analysis, Mathematics / Optimization, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@book{srivastava_theory_2013,
	location = {Dordrecht},
	title = {Theory and applications of convolution integral equations},
	volume = {79},
	publisher = {Springer Science \& Business Media},
	author = {Srivastava, Hari M and Buschman, Robert G},
	date = {2013},
	keywords = {Mathematics / Applied, Mathematics / Calculus, Mathematics / Mathematical Analysis, Science / Physics / Quantum Theory},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	issn = {0277-6715, 1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.8086},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo‐random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data‐generating mechanisms, estimands, methods, and performance measures (“{ADEMP}”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of
              Statistics in Medicine
              , which included at least one simulation study and identify areas for improvement.},
	pages = {2074--2102},
	number = {11},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	urldate = {2024-03-11},
	date = {2019-05-20},
	langid = {english},
}


@inproceedings{hatziantoniou_real-time_2004,
	location = {Berlin},
	title = {Real-Time Room Equalization based on Complex Smoothing: Robustness Results},
	abstract = {The aim of this study is to investigate the robustness of room acoustics real-time equalization using inverse filters derived from the Complex Smoothing of the Transfer Function using perceptual criteria. The robustness of the method is assessed by real-time tests which compare the performance of Complex Smoothing-based equalization (for different filter lengths) with the traditional, ideal inverse filtering, over a range of room locations, which differ to the ones where response measurements were taken. Objective measurements and audio examples will show that the Complex Smoothing-based equalization performance is largely immune to position changes and does not introduce processing artifacts, problems affecting the traditional ideal inversion.},
	booktitle = {Proc. of the 116th {AES} Convention, preprint 6070},
	publisher = {{AES}},
	author = {Hatziantoniou, Panagiotis D. and Mourjopoulos, John N.},
	date = {2004},
	keywords = {6070},
}

@article{dietterich_approximate_1998,
	title = {Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms},
	volume = {10},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/10/7/1895-1923/6224},
	doi = {10.1162/089976698300017197},
	abstract = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, {McNemar}'s test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5×2 cv test is shown to be slightly more powerful than {McNemar}'s test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
	pages = {1895--1923},
	number = {7},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Dietterich, Thomas G.},
	urldate = {2024-03-11},
	date = {1998-10-01},
	langid = {english},
}


@misc{fry_unbiased_2023,
	title = {Unbiased Estimation of Structured Prediction Error},
	url = {http://arxiv.org/abs/2310.10740},
	abstract = {Many modern datasets, such as those in ecology and geology, are composed of samples with spatial structure and dependence. With such data violating the usual independent and identically distributed ({IID}) assumption in machine learning and classical statistics, it is unclear a priori how one should measure the performance and generalization of models. Several authors have empirically investigated cross-validation ({CV}) methods in this setting, reaching mixed conclusions. We provide a class of unbiased estimation methods for general quadratic errors, correlated Gaussian response, and arbitrary prediction function g, for a noise-elevated version of the error. Our approach generalizes the coupled bootstrap ({CB}) of Oliveira et al. [19] from the normal means problem to general normal data, allowing correlation both within and between the training and test sets. {CB} relies on creating bootstrap samples that are intelligently decoupled, in the sense of being statistically independent. Specifically, the key to {CB} lies in generating two independent “views” of our data and using them as stand-ins for the usual independent training and test samples. Beginning with Mallows’ Cp [15], we generalize the estimator to develop our generalized Cp estimators ({GC}). We show at under only a moment condition on g, this noise-elevated error estimate converges smoothly to the noiseless error estimate. We show that when Stein’s unbiased risk estimator ({SURE}) applies, {GC} converges to {SURE} as in the normal means problem. Further, we use these same tools to analyze {CV} and provide some theoretical analysis to help understand when {CV} will provide good estimates of error. Simulations align with our theoretical results, demonstrating the effectiveness of {GC} and illustrating the behavior of {CV} methods. Lastly, we apply our estimator to a model selection task on geothermal data in Nevada.},
	number = {{arXiv}:2310.10740},
	publisher = {{arXiv}},
	author = {Fry, Kevin and Taylor, Jonathan E.},
	urldate = {2024-03-18},
	date = {2023-10-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.10740 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{rink_post-selection_2023,
	title = {Post-Selection Confidence Bounds for Prediction Performance},
	url = {http://arxiv.org/abs/2210.13206},
	abstract = {In machine learning, the selection of a promising model from a potentially large number of competing models and the assessment of its generalization performance are critical tasks that need careful consideration. Typically, model selection and evaluation are strictly separated endeavors, splitting the sample at hand into a training, validation, and evaluation set, and only compute a single conﬁdence interval for the prediction performance of the ﬁnal selected model. We however propose an algorithm how to compute valid lower conﬁdence bounds for multiple models that have been selected based on their prediction performances in the evaluation set by interpreting the selection problem as a simultaneous inference problem. We use bootstrap tilting and a {maxT}-type multiplicity correction. The approach is universally applicable for any combination of prediction models, any model selection strategy, and any prediction performance measure that accepts weights. We conducted various simulation experiments which show that our proposed approach yields lower conﬁdence bounds that are at least comparably good as bounds from standard approaches, and that reliably reach the nominal coverage probability. In addition, especially when sample size is small, our proposed approach yields better performing prediction models than the default selection of only one model for evaluation does.},
	number = {{arXiv}:2210.13206},
	publisher = {{arXiv}},
	author = {Rink, Pascal and Brannath, Werner},
	urldate = {2024-03-15},
	date = {2023-02-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.13206 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yu_blocked_2014,
	title = {Blocked 3×2 Cross-Validated \textit{t} -Test for Comparing Supervised Classification Learning Algorithms},
	volume = {26},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/26/1/208-235/7938},
	doi = {10.1162/NECO_a_00532},
	abstract = {In the research of machine learning algorithms for classification tasks, the comparison of the performances of algorithms is extremely important, and a statistical test of significance for generalization error is often used to perform it in the machine learning literature. In view of the randomness of partitions in cross-validation, a new blocked 3×2 cross-validation is proposed to estimate generalization error in this letter. We then conduct an analysis of variance of the blocked 3×2 cross-validated estimator. A relatively conservative variance estimator that considers the correlation between any two two-fold cross-validations, and was previously neglected in 5×2 cross-validated t and F-tests is put forward. A corresponding test using this variance estimator is presented to compare the performances of algorithms. Simulated results show that the performance of our test is comparable with that of 5×2 cross-validated tests but with less computation complexity.},
	pages = {208--235},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Yu, Wang and Ruibo, Wang and Huichen, Jia and Jihong, Li},
	urldate = {2024-03-15},
	date = {2014-01},
	langid = {english},
}

@article{wong_parametric_2017,
	title = {Parametric methods for comparing the performance of two classification algorithms evaluated by k-fold cross validation on multiple data sets},
	volume = {65},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316304290},
	doi = {10.1016/j.patcog.2016.12.018},
	abstract = {A popular procedure for identifying which one of two classification algorithms has a better performance is to test them on multiple data sets, and the accuracies resulting from k-fold cross validation are aggregated to draw a conclusion. Several nonparametric methods have been proposed for this purpose, while parametric methods will be a better choice to determine the superior algorithm when the assumptions for deriving sampling distributions can be satisfied. In this paper, we consider every accuracy estimate resulting from the instances in a fold or a data set as a point estimator instead of a fixed value to derive the sampling distribution of the point estimator for comparing the performance of two classification algorithms. The test statistics for both data-set and fold averaging levels are proposed, and the ways to calculate their degrees of freedom are also presented. Twelve data sets are chosen to demonstrate that our parametric methods can be used to effectively compare the performance of two classification algorithms on multiple data sets. Several critical issues in using our parametric methods and the nonparametric ones proposed in a previous study are then discussed.},
	pages = {97--107},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Wong, Tzu-Tsung},
	urldate = {2024-03-15},
	date = {2017-05},
	langid = {english},
}

@misc{sziklai_testing_2022,
	title = {Testing Rankings with Cross-Validation},
	url = {http://arxiv.org/abs/2105.11939},
	abstract = {This research investigates how to determine whether two rankings come from the same distribution. We evaluate three hybrid tests: Wilcoxon’s, Dietterich’s, and Alpaydin’s statistical tests combined with cross-validation ({CV}), each operating with folds ranging from 5 to 10, thus altogether 18 variants. We have applied these tests in the framework of a popular comparative statistical test, the Sum of Ranking Diﬀerences that builds upon the Manhattan distance between the rankings. The introduced methodology is widely applicable from machine learning through social sciences. To compare these methods, we have followed an innovative approach borrowed from Economics. We designed nine scenarios for testing type I and {II} errors. These represent typical situations (that is, diﬀerent data structures) that {CV} tests face routinely. The optimal {CV} method depends on the preferences regarding the minimization of type I/{II} errors, size of the input, and expected patterns in the data. The Wilcoxon method with eight folds proved to be the best for all three investigated input sizes. Although the Dietterich and Alpaydin methods are the best in type I situations, they fail badly in type {II} cases. We demonstrate our results on real-world data, borrowed from chess and chemistry. Overall we cannot recommend either Alpaydin or Dietterich as an alternative to Wilcoxon cross-validation.},
	number = {{arXiv}:2105.11939},
	publisher = {{arXiv}},
	author = {Sziklai, Balázs R. and Baranyi, Máté and Héberger, Károly},
	urldate = {2024-03-15},
	date = {2022-02-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2105.11939 [math, stat]},
	keywords = {G.3, I.6, Mathematics - Statistics Theory, Statistics - Methodology},
}

@article{stapor_how_2021,
	title = {How to design the fair experimental classifier evaluation},
	volume = {104},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494621001423},
	doi = {10.1016/j.asoc.2021.107219},
	abstract = {Many researchers working on classification problems evaluate the quality of developed algorithms based on computer experiments. The conclusions drawn from them are usually supported by the statistical analysis and chosen experimental protocol. Statistical tests are widely used to confirm whether considered methods significantly outperform reference classifiers. Usually, the tests are applied to stratified datasets, which could raise the question of whether data folds used for classification are really randomly drawn and how the statistical analysis supports robust conclusions. Unfortunately, some scientists do not realize the real meaning of the obtained results and overinterpret them. They do not see that inappropriate use of such analytical tools may lead them into a trap. This paper aims to show the commonly used experimental protocols’ weaknesses and discuss if we really can trust in such evaluation methodology, if all presented evaluations are fair and if it is possible to manipulate the experimental results using well-known statistical evaluation methods. We will present that it is possible to choose only such results, confirming the experimenter’s expectation. We will try to show what could be done to avoid such likely unethical behavior. At the end of this work, we will formulate recommendations on improving an experimental protocol to design fair experimental classifier evaluation.},
	pages = {107219},
	journaltitle = {Applied Soft Computing},
	shortjournal = {Applied Soft Computing},
	author = {Stapor, Katarzyna and Ksieniewicz, Paweł and García, Salvador and Woźniak, Michał},
	urldate = {2024-03-15},
	date = {2021-06},
	langid = {english},
}

@incollection{el_naqa_performance_2015,
	location = {Cham},
	title = {Performance Evaluation in Machine Learning},
	isbn = {978-3-319-18304-6 978-3-319-18305-3},
	url = {https://link.springer.com/10.1007/978-3-319-18305-3_4},
	abstract = {Performance evaluation is an important aspect of the machine learning process. However, it is a complex task. It, therefore, needs to be conducted carefully in order for the application of machine learning to radiation oncology or other domains to be reliable. This chapter introduces the issue and discusses some of the most commonly used techniques that have been applied to it. The focus is on the three main subtasks of evaluation: measuring performance, resampling the data, and assessing the statistical significance of the results. In the context of the first subtask, the chapter discusses some of the confusion matrix-based measures (accuracy, precision, recall or sensitivity, and false alarm rate) as well as receiver operating characteristic ({ROC}) analysis; several error estimation or resampling techniques belonging to the cross-validation family as well as bootstrapping are involved in the context of the second subtask. Finally, a number of nonparametric statistical tests including {McNemar}’s test, Wilcoxon’s signed-rank test, and Friedman’s test are covered in the context of the third subtask. The chapter concludes with a discussion of the limitations of the evaluation process.},
	pages = {41--56},
	booktitle = {Machine Learning in Radiation Oncology},
	publisher = {Springer International Publishing},
	author = {Japkowicz, Nathalie and Shah, Mohak},
	editor = {El Naqa, Issam and Li, Ruijiang and Murphy, Martin J.},
	urldate = {2024-03-15},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-18305-3_4},
}

@article{braun_independent_2023,
	title = {Independent validation as a validation method for classification},
	volume = {3},
	issn = {2699-8432},
	url = {https://qcmb.psychopen.eu/index.php/qcmb/article/view/12069},
	doi = {10.5964/qcmb.12069},
	abstract = {The use of classifiers provides an alternative to conventional statistical methods. This involves using the accuracy with which data is correctly assigned to a given group by the classifier to apply tests to compare the performance of classifiers. The conventional validation methods for determining the accuracy of classifiers have the disadvantage that the distribution of correct classifications does not follow any known distribution, and therefore, the application of statistical tests is problematic. Independent validation circumvents this problem and allows the use of binomial tests to assess the performance of classifiers. However, independent validation accuracy is subject to bias for small training datasets. The present study shows that a hyperbolic function can be used to estimate the loss in classifier accuracy for independent validation. This function is used to develop three new methods to estimate the classifier accuracy for small training sets more precisely. These methods are compared to two existing methods in a simulation study. The results indicate overall small errors in the estimation of classifier accuracy and indicate that independent validation can be used with small samples. A least square estimation approach seems best suited to estimate the classifier accuracy.},
	pages = {e12069},
	journaltitle = {Quantitative and Computational Methods in Behavioral Sciences},
	shortjournal = {Quant. Comput. Methods Behav. Sci.},
	author = {Braun, Tina and Eckert, Hannes and Von Oertzen, Timo},
	urldate = {2024-03-15},
	date = {2023-12-22},
	langid = {english},
}

@article{boulesteix_statistical_2015,
	title = {A Statistical Framework for Hypothesis Testing in Real Data Comparison Studies},
	volume = {69},
	issn = {0003-1305, 1537-2731},
	url = {http://www.tandfonline.com/doi/full/10.1080/00031305.2015.1005128},
	doi = {10.1080/00031305.2015.1005128},
	pages = {201--212},
	number = {3},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Boulesteix, Anne-Laure and Hable, Robert and Lauer, Sabine and Eugster, Manuel J. A.},
	urldate = {2024-03-15},
	date = {2015-07-03},
	langid = {english},
}

@article{berrar_confidence_2017,
	title = {Confidence curves: an alternative to null hypothesis significance testing for the comparison of classifiers},
	volume = {106},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-016-5612-6},
	doi = {10.1007/s10994-016-5612-6},
	shorttitle = {Confidence curves},
	abstract = {Null hypothesis signiﬁcance testing is routinely used for comparing the performance of machine learning algorithms. Here, we provide a detailed account of the major underrated problems that this common practice entails. For example, omnibus tests, such as the widely used Friedman test, are not appropriate for the comparison of multiple classiﬁers over diverse data sets. In contrast to the view that signiﬁcance tests are essential to a sound and objective interpretation of classiﬁcation results, our study suggests that no such tests are needed. Instead, greater emphasis should be placed on the magnitude of the performance difference and the investigator’s informed judgment. As an effective tool for this purpose, we propose conﬁdence curves, which depict nested conﬁdence intervals at all levels for the performance difference. These curves enable us to assess the compatibility of an inﬁnite number of null hypotheses with the experimental results. We benchmarked several classiﬁers on multiple data sets and analyzed the results with both signiﬁcance tests and conﬁdence curves. Our conclusion is that conﬁdence curves effectively summarize the key information needed for a meaningful interpretation of classiﬁcation results while avoiding the intrinsic pitfalls of signiﬁcance tests.},
	pages = {911--949},
	number = {6},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Berrar, Daniel},
	urldate = {2024-03-15},
	date = {2017-06},
	langid = {english},
}

@article{berrar_significance_2013,
	title = {Significance tests or confidence intervals: which are preferable for the comparison of classifiers?},
	volume = {25},
	issn = {0952-813X, 1362-3079},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0952813X.2012.680252},
	doi = {10.1080/0952813X.2012.680252},
	shorttitle = {Significance tests or confidence intervals},
	pages = {189--206},
	number = {2},
	journaltitle = {Journal of Experimental \& Theoretical Artificial Intelligence},
	shortjournal = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {Berrar, Daniel and Lozano, Jose A.},
	urldate = {2024-03-15},
	date = {2013-06},
	langid = {english},
}

@article{austin_iterative_2023,
	title = {The iterative bisection procedure: a useful tool for determining parameter values in data-generating processes in Monte Carlo simulations},
	volume = {23},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-023-01836-5},
	doi = {10.1186/s12874-023-01836-5},
	abstract = {Data-generating processes are key to the design of Monte Carlo simulations. It is important for investigators to be able to simulate data with specific characteristics.},
	pages = {45},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Medical Research Methodology},
	author = {Austin, Peter C.},
	date = {2023-02-17},
}

@article{goldfeld_simstudy_2020,
	title = {simstudy: Illuminating research methods through data generation},
	volume = {5},
	url = {https://doi.org/10.21105/joss.02763},
	doi = {10.21105/joss.02763},
	pages = {2763},
	number = {54},
	journaltitle = {Journal of Open Source Software},
	author = {Goldfeld, Keith and Wujciak-Jens, Jacob},
	date = {2020},
	note = {Publisher: The Open Journal},
}

@article{lang_batchtools_2017,
	title = {batchtools: Tools for R to work on batch systems},
	volume = {2},
	url = {https://doi.org/10.21105/joss.00135},
	doi = {10.21105/joss.00135},
	number = {10},
	journaltitle = {The Journal of Open Source Software},
	author = {Lang, Michel and Bischl, Bernd and Surmann, Dirk},
	date = {2017-02},
	note = {Publisher: The Open Journal},
}

@article{boulesteix_necessity_2018,
	title = {On the necessity and design of studies comparing statistical methods},
	volume = {60},
	issn = {0323-3847, 1521-4036},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/bimj.201700129},
	doi = {10.1002/bimj.201700129},
	pages = {216--218},
	number = {1},
	journaltitle = {Biometrical Journal},
	shortjournal = {Biometrical J},
	author = {Boulesteix, Anne‐Laure and Binder, Harald and Abrahamowicz, Michal and Sauerbrei, Willi and {for the Simulation Panel of the STRATOS Initiative}},
	urldate = {2024-03-20},
	date = {2018-01},
	langid = {english},
}

@article{hawinkel_out--sample_2023,
	title = {The out-of-sample R²: estimation and inference},
	issn = {0003-1305, 1537-2731},
	url = {http://arxiv.org/abs/2302.05131},
	doi = {10.1080/00031305.2023.2216252},
	shorttitle = {The out-of-sample \$R{\textasciicircum}2\$},
	abstract = {Out-of-sample prediction is the acid test of predictive models, yet an independent test dataset is often not available for assessment of the prediction error. For this reason, out-of-sample performance is commonly estimated using data splitting algorithms such as cross-validation or the bootstrap. For quantitative outcomes, the ratio of variance explained to total variance can be summarized by the coeﬃcient of determination or in-sample R2, which is easy to interpret and to compare across diﬀerent outcome variables. As opposed to the in-sample R2, the out-of-sample R2 has not been well deﬁned and the variability on the out-of-sample Rˆ2 has been largely ignored. Usually only its point estimate is reported, hampering formal comparison of predictability of diﬀerent outcome variables. Here we explicitly deﬁne the out-of-sample R2 as a comparison of two predictive models, provide an unbiased estimator and exploit recent theoretical advances on uncertainty of data splitting estimates to provide a standard error for the Rˆ2. The performance of the estimators for the R2 and its standard error are investigated in a simulation study. We demonstrate our new method by constructing conﬁdence intervals and comparing models for prediction of quantitative Brassica napus and Zea mays phenotypes based on gene expression data.},
	pages = {1--11},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {Hawinkel, Stijn and Waegeman, Willem and Maere, Steven},
	urldate = {2024-03-18},
	date = {2023-06-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.05131 [stat]},
	keywords = {Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{luijken_replicability_2023,
	title = {Replicability of Simulation Studies for the Investigation of Statistical Methods: The {RepliSims} Project},
	url = {https://www.semanticscholar.org/paper/Replicability-of-Simulation-Studies-for-the-of-The-Luijken-Lohmann/92c5679cdba1ecdb54ca8f19f78960c2eb34872c},
	shorttitle = {Replicability of Simulation Studies for the Investigation of Statistical Methods},
	abstract = {Results of simulation studies evaluating the performance of statistical methods are often considered actionable and thus can have a major impact on the way empirical research is implemented. However, so far there is limited evidence about the reproducibility and replicability of statistical simulation studies. Therefore, eight highly cited statistical simulation studies were selected, and their replicability was assessed by teams of replicators with formal training in quantitative methodology. The teams found relevant information in the original publications and used it to write simulation code with the aim of replicating the results. The primary outcome was the feasibility of replicability based on reported information in the original publications. Replicability varied greatly: Some original studies provided detailed information leading to almost perfect replication of results, whereas other studies did not provide enough information to implement any of the reported simulations. Replicators had to make choices regarding missing or ambiguous information in the original studies, error handling, and software environment. Factors facilitating replication included public availability of code, and descriptions of the data-generating procedure and methods in graphs, formulas, structured text, and publicly accessible additional resources such as technical reports. Replicability of statistical simulation studies was mainly impeded by lack of information and sustainability of information sources. Reproducibility could be achieved for simulation studies by providing open code and data as a supplement to the publication. Additionally, simulation studies should be transparently reported with all relevant information either in the research paper itself or in easily accessible supplementary material to allow for replicability.},
	author = {Luijken, K. and Lohmann, A. and Alter, U. and Gonzalez, J. C. and Clouth, F. and Fossum, J. and Hesen, L. and Huizing, A. and Ketelaar, J. and Montoya, A. and Nab, L. and Nijman, R. and Vries, Bas B. L. Penning De and Tibbe, T. and Wang, Y. A. and Groenwold, R.},
	urldate = {2024-03-18},
	date = {2023-07-05},
}

@report{white_how_2023,
	title = {How to check a simulation study},
	url = {https://osf.io/cbr72},
	abstract = {Simulation studies are powerful tools in epidemiology and biostatistics, but they can be hard to conduct successfully. Sometimes unexpected results are obtained. We offer advice on how to check a simulation study when this occurs, and how to design and conduct the study to give results that are easier to check. Simulation studies should be designed to include some settings in which answers are already known. They should be coded in stages, with data-generating mechanisms checked before simulated data are analysed. Results should be explored carefully, with scatterplots of standard error estimates against point estimates surprisingly powerful tools. Failed estimation and outlying estimates should be identiﬁed and dealt with by changing data-generating mechanisms or coding realistic hybrid analysis procedures. Finally, we give a series of ideas that have been useful to us in the past for checking unexpected results. Following our advice may help to prevent errors and to improve the quality of published simulation studies.},
	institution = {Open Science Framework},
	type = {preprint},
	author = {White, Ian R and Pham, Tra My and Quartagno, Matteo and Morris, Tim P},
	urldate = {2024-03-18},
	date = {2023-02-03},
	langid = {english},
	doi = {10.31219/osf.io/cbr72},
}

@article{pawel_pitfalls_2023,
	title = {Pitfalls and potentials in simulation studies: Questionable research practices in comparative simulation studies allow for spurious claims of superiority of any method},
	issn = {0323-3847, 1521-4036},
	url = {http://arxiv.org/abs/2203.13076},
	doi = {10.1002/bimj.202200091},
	shorttitle = {Pitfalls and potentials in simulation studies},
	abstract = {Comparative simulation studies are workhorse tools for benchmarking statistical methods. As with other empirical studies, the success of simulation studies hinges on the quality of their design, execution and reporting. If not conducted carefully and transparently, their conclusions may be misleading. In this paper we discuss various questionable research practices which may impact the validity of simulation studies, some of which cannot be detected or prevented by the current publication process in statistics journals. To illustrate our point, we invent a novel prediction method with no expected performance gain and benchmark it in a pre-registered comparative simulation study. We show how easy it is to make the method appear superior over well-established competitor methods if questionable research practices are employed. Finally, we provide concrete suggestions for researchers, reviewers and other academic stakeholders for improving the methodological quality of comparative simulation studies, such as pre-registering simulation protocols, incentivizing neutral simulation studies and code and data sharing.},
	pages = {2200091},
	journaltitle = {Biometrical Journal},
	shortjournal = {Biometrical J},
	author = {Pawel, Samuel and Kook, Lucas and Reeve, Kelly},
	urldate = {2024-03-18},
	date = {2023-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.13076 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@article{chalmers_writing_2020,
	title = {Writing Effective and Reliable Monte Carlo Simulations with the {SimDesign} Package},
	volume = {16},
	issn = {2292-1354},
	url = {http://www.tqmp.org/RegularArticles/vol16-4/p248},
	doi = {10.20982/tqmp.16.4.p248},
	abstract = {The purpose of this tutorial is to discuss and demonstrate how to write safe, effective, and intuitive computer code for Monte Carlo simulation experiments containing one or more simulation factors. Throughout this tutorial the {SimDesign} package (Chalmers, 2020), available within the R programming environment, will be adopted due to its ability to accommodate a number of desirable execution features. The article begins by discussing a selection of attractive coding strategies that should be present in Monte Carlo simulation experiments, showcases how the {SimDesign} package can satisfy many of these desirable strategies, and provides a worked mediation analysis simulation example to demonstrate the implementation of these features. To demonstrate how the package can be used for real-world experiments, the simulation explored by Flora and Curran (2004) pertaining to a conﬁrmatory factor analysis robustness study with ordinal response data is also presented and discussed.},
	pages = {248--280},
	number = {4},
	journaltitle = {The Quantitative Methods for Psychology},
	shortjournal = {{TQMP}},
	author = {Chalmers, R. Philip and Adkins, Mark C.},
	urldate = {2024-03-18},
	date = {2020-05-01},
	langid = {english},
}

@misc{cai_bootstrapping_2023,
	title = {Bootstrapping the Cross-Validation Estimate},
	url = {http://arxiv.org/abs/2307.00260},
	abstract = {Cross-validation is a widely used technique for evaluating the performance of prediction models. It helps avoid the optimism bias in error estimates, which can be significant for models built using complex statistical learning algorithms. However, since the cross-validation estimate is a random value dependent on observed data, it is essential to accurately quantify the uncertainty associated with the estimate. This is especially important when comparing the performance of two models using cross-validation, as one must determine whether differences in error estimates are a result of chance fluctuations. Although various methods have been developed for making inferences on cross-validation estimates, they often have many limitations, such as stringent model assumptions This paper proposes a fast bootstrap method that quickly estimates the standard error of the cross-validation estimate and produces valid confidence intervals for a population parameter measuring average model performance. Our method overcomes the computational challenge inherent in bootstrapping the cross-validation estimate by estimating the variance component within a random effects model. It is just as flexible as the cross-validation procedure itself. To showcase the effectiveness of our approach, we employ comprehensive simulations and real data analysis across three diverse applications.},
	number = {{arXiv}:2307.00260},
	publisher = {{arXiv}},
	author = {Cai, Bryan and Pellegrini, Fabio and Pang, Menglan and de Moor, Carl and Shen, Changyu and Charu, Vivek and Tian, Lu},
	urldate = {2024-03-18},
	date = {2023-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.00260 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{sun_confidence_2023,
	title = {Confidence intervals for the Cox model test error from cross-validation},
	volume = {42},
	issn = {0277-6715, 1097-0258},
	url = {http://arxiv.org/abs/2201.10770},
	doi = {10.1002/sim.9873},
	abstract = {Cross-validation ({CV}) is one of the most widely used techniques in statistical learning for estimating the test error of a model, but its behavior is not yet fully understood. It has been shown that standard confidence intervals for test error using estimates from {CV} may have coverage below nominal levels. This phenomenon occurs because each sample is used in both the training and testing procedures during {CV} and as a result, the {CV} estimates of the errors become correlated. Without accounting for this correlation, the estimate of the variance is smaller than it should be. One way to mitigate this issue is by estimating the mean squared error of the prediction error instead using nested {CV}. This approach has been shown to achieve superior coverage compared to intervals derived from standard {CV}. In this work, we generalize the nested {CV} idea to the Cox proportional hazards model and explore various choices of test error for this setting.},
	pages = {4532--4541},
	number = {25},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Statistics in Medicine},
	author = {Sun, Min Woo and Tibshirani, Robert},
	urldate = {2024-03-18},
	date = {2023-11-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2201.10770 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
}

@article{geroldinger_leave-one-out_2023,
	title = {Leave-one-out cross-validation, penalization, and differential bias of some prediction model performance measures—a simulation study},
	volume = {7},
	issn = {2397-7523},
	url = {https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-023-00146-0},
	doi = {10.1186/s41512-023-00146-0},
	abstract = {Background  The performance of models for binary outcomes can be described by measures such as the concordance statistic (c-statistic, area under the curve), the discrimination slope, or the Brier score. At internal validation, data resampling techniques, e.g., cross-validation, are frequently employed to correct for optimism in these model performance criteria. Especially with small samples or rare events, leave-one-out cross-validation is a popular choice.
Methods  Using simulations and a real data example, we compared the effect of different resampling techniques on the estimation of c-statistics, discrimination slopes, and Brier scores for three estimators of logistic regression models, including the maximum likelihood and two maximum penalized likelihood estimators.
Results  Our simulation study confirms earlier studies reporting that leave-one-out cross-validated c-statistics can be strongly biased towards zero. In addition, our study reveals that this bias is even more pronounced for model estimators shrinking estimated probabilities towards the observed event fraction, such as ridge regression. Leave-one-out cross-validation also provided pessimistic estimates of the discrimination slope but nearly unbiased estimates of the Brier score.
Conclusions  We recommend to use leave-pair-out cross-validation, fivefold cross-validation with repetitions, the enhanced or the .632+ bootstrap to estimate c-statistics, and leave-pair-out or fivefold cross-validation to estimate discrimination slopes.},
	pages = {9},
	number = {1},
	journaltitle = {Diagnostic and Prognostic Research},
	shortjournal = {Diagn Progn Res},
	author = {Geroldinger, Angelika and Lusa, Lara and Nold, Mariana and Heinze, Georg},
	urldate = {2024-03-18},
	date = {2023-05-02},
	langid = {english},
}

@misc{krupkin_prediction_2023,
	title = {Prediction Error Estimation in Random Forests},
	url = {http://arxiv.org/abs/2309.00736},
	abstract = {In this paper, error estimates of classiﬁcation Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classiﬁcation case, Random Forests’ estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the ﬁndings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across diﬀerent error estimation strategies such as cross-validation, bagging, and data splitting.},
	number = {{arXiv}:2309.00736},
	publisher = {{arXiv}},
	author = {Krupkin, Ian and Hardin, Johanna},
	urldate = {2024-03-18},
	date = {2023-09-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2309.00736 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xu_estimation_2022,
	title = {Estimation of prediction error with known covariate shift},
	url = {http://arxiv.org/abs/2205.01849},
	abstract = {In supervised learning, the estimation of prediction error on unlabeled test data is an important task. Existing methods are usually built on the assumption that the training and test data are sampled from the same distribution, which is often violated in practice. As a result, traditional estimators like cross-validation ({CV}) will be biased and this may result in poor model selection. In this paper, we assume that we have a test dataset in which the feature values are available but not the outcome labels, and focus on a particular form of distributional shift called “covariate shift”. We propose an alternative method based on parametric bootstrap of the target of conditional error {ErrX} [{BHT}21]. Empirically our method outperforms {CV} for both simulation and real data example across diﬀerent modeling tasks.},
	number = {{arXiv}:2205.01849},
	publisher = {{arXiv}},
	author = {Xu, Hui and Tibshirani, Robert},
	urldate = {2024-03-18},
	date = {2022-09-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2205.01849 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{xu_uncertainty_2023,
	title = {Uncertainty Intervals for Prediction Errors in Time Series Forecasting},
	url = {http://arxiv.org/abs/2309.07435},
	abstract = {Inference for prediction errors is critical in time series forecasting pipelines. However, providing statistically meaningful uncertainty intervals for prediction errors remains relatively under-explored. Practitioners often resort to forward cross-validation ({FCV}) for obtaining point estimators and constructing confidence intervals based on the Central Limit Theorem ({CLT}). The naive version assumes independence, a condition that is usually invalid due to time correlation. These approaches lack statistical interpretations and theoretical justifications even under stationarity.},
	number = {{arXiv}:2309.07435},
	publisher = {{arXiv}},
	author = {Xu, Hui and Mei, Song and Bates, Stephen and Taylor, Jonathan and Tibshirani, Robert},
	urldate = {2024-103-18},
	date = {2023-09-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2309.07435 [stat]},
	keywords = {Statistics - Methodology},
}

@article{tsamardinos_bootstrapping_2018,
	title = {Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-018-5714-4},
	doi = {10.1007/s10994-018-5714-4},
	abstract = {Cross-Validation ({CV}), and out-of-sample performance-estimation protocols in general, are often employed both for (a) selecting the optimal combination of algorithms and values of hyper-parameters (called a conﬁguration) for producing the ﬁnal predictive model, and (b) estimating the predictive performance of the ﬁnal model. However, the cross-validated performance of the best conﬁguration is optimistically biased. We present an efﬁcient bootstrap method that corrects for the bias, called Bootstrap Bias Corrected {CV} ({BBC}-{CV}). {BBC}-{CV}’s main idea is to bootstrap the whole process of selecting the best-performing conﬁguration on the out-of-sample predictions of each conﬁguration, without additional training of models. In comparison to the alternatives, namely the nested cross-validation (Varma and Simon in {BMC} Bioinform 7(1):91, 2006) and a method by Tibshirani and Tibshirani (Ann Appl Stat 822–829, 2009), {BBC}-{CV} is computationally more efﬁcient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, {AUC}, concordance index, mean squared error). Subsequently, we employ again the idea of bootstrapping the out-of-sample predictions to speed up the {CV} process. Speciﬁcally, using a bootstrap-based statistical criterion we stop training of models on new folds of inferior (with high probability) conﬁgurations. We name the method Bootstrap Bias Corrected with Dropping {CV} ({BBCD}-{CV}) that is both efﬁcient and provides accurate performance estimates.},
	pages = {1895--1922},
	number = {12},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Tsamardinos, Ioannis and Greasidou, Elissavet and Borboudakis, Giorgos},
	urldate = {2024-03-11},
	date = {2018-12},
	langid = {english},
}

@article{boulesteix_plea_2013,
	title = {A Plea for Neutral Comparison Studies in Computational Sciences},
	volume = {8},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0061562},
	doi = {10.1371/journal.pone.0061562},
	abstract = {In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of ‘‘new methods’’, while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-todate picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a ‘‘tidy neutral comparison study’’. R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/ organisation/mitarbeiter/020\_professuren/boulesteix/plea2013.},
	pages = {e61562},
	number = {4},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Boulesteix, Anne-Laure and Lauer, Sabine and Eugster, Manuel J. A.},
	editor = {Gasparini, Mauro},
	urldate = {2024-03-11},
	date = {2013-04-24},
	langid = {english},
}

@misc{bates_cross-validation_2022,
	title = {Cross-validation: what does it estimate and how well does it do it?},
	url = {http://arxiv.org/abs/2104.00673},
	shorttitle = {Cross-validation},
	abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, ﬁt to the training data. We prove that this is not the case for the linear model ﬁt by ordinary least squares; rather it estimates the average prediction error of models ﬁt on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow’s Cp. Next, the standard conﬁdence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and we show empirically that this modiﬁcation leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail.},
	number = {{arXiv}:2104.00673},
	publisher = {{arXiv}},
	author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
	urldate = {2024-03-11},
	date = {2022-07-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.00673 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

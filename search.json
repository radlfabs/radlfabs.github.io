[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Data scientist and developer. With my engineering background, I bring a cross-disciplinary perspective to data science. I am specialized in performance estimation and uncertainty quantification. Great at identifying the right tool for a problem, communicating it to the team, mastering it, and shipping it quickly."
  },
  {
    "objectID": "resume.html#profile",
    "href": "resume.html#profile",
    "title": "Resume",
    "section": "",
    "text": "Data scientist and developer. With my engineering background, I bring a cross-disciplinary perspective to data science. I am specialized in performance estimation and uncertainty quantification. Great at identifying the right tool for a problem, communicating it to the team, mastering it, and shipping it quickly."
  },
  {
    "objectID": "resume.html#section",
    "href": "resume.html#section",
    "title": "Resume",
    "section": "",
    "text": "Languages\n\n\n\nEnglish (C1)\nGerman (native)\n\n\n\n\n\nSoft skills\n\n\n\nMeticulous\nVersatile\nEmpathetic\nCreative"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\nProgramming\n\n\n\nPython, R, SQL\nLaTeX, Matlab\nRust (learning)\n\n\n\n\n\nFrameworks\n\n\n\nPyTorch, scikit-learn\nPandas, Polars, Arrow\nmlr3, data.table\ntidyverse, tidymodels\nBokeh, Shap, ggplot2\n\n\n\n\n\nTools\n\n\n\nQuarto, Tableau\nGit, Docker, Linux CLI, Parquet"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\n\n\n\n\n\n\nStudent Research Assistant\nFraunhofer MEVIS (Bremen) | 02/2024 ‚Äì 08/2024\n\n\n\nDeveloped statistical R-packages with uncertainty quantification techniques, giving clinicians a more robust understanding of ML models.\nDesigned unified APIs with consistent code styling and good unit tests.\n\n\n\n\n\n\n\n\n\n\nStudent Research Assistant\nUniversity of Applied Sciences D√ºsseldorf | 09/2020 ‚Äì 10/2023\n\n\n\nCo-published a data set with soundscapes from 100+ participants.\nPerformed data and feature engineering on 6000+ audio recordings.\nSaved days of compute by employing a faster hyperparameter tuning.\nIntroduced a solution to keep track of 1000+ experiments.\nKeynotes and posters on the use of stats & ML in acoustic research.\n\n\n\n\n\n\n\n\n\n\nVideo Recording Producer\nWDR (K√∂ln) | 09/2020 ‚Äì 02/2022\n\n\n\nCalled cameras for live-streaming concerts in a musically sensible way.\nLed technical teams and communicated cinematographic ideas efficiently.\n\n\n\n\n\n\n\n\n\n\nBike Messenger\nSelf-employed (D√ºsseldorf) | 11/2016 ‚Äì 09/2020\n\n\nCompleted time-critical and valuable deliveries in all weather conditions while optimizing complex urban routes in real-time."
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Resume",
    "section": "Projects",
    "text": "Projects\n\nflexcv\nUniversity of Applied Sciences D√ºsseldorf & Private Project\n\nA Python package simplifying nested cross-validation on grouped data.\nEnables reproducible multimodel workflows with few lines of code.\nReleased and maintained as OSS using GitHub Actions (CI/CD).\nProject page\n\n\n\nTime Series Forecasting\nUniversity of Applied Sciences Cologne\n\nPrediction of open bicycle counter data for Cologne and benchmarking of ML-algorithms with time series cross-validation.\nFine-tuned XGBoost with external weather data and seasonal trends beats the best linear models.\n\n\n\nGridNet White Balance\nUniversity of Applied Sciences Cologne\n\nAchieved improvement in color consistency metrics with GridNet.\nDeployed the model within a custom camera pipeline.\nStrong communication and co-operation in a 5-person team.\n\n\n\nSoundscape Viz App\nUniversity of Applied Sciences Cologne - Interactive app visualizing survey data on individual levels. - Enables intuitive exploration of the diversity of participants. - Deployed here on the Posit Connect Cloud."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\nMaster of Science: Media Technology\nUniversity of Applied Sciences Cologne | 2022 ‚Äì 2024\n\nThesis: Comparison of uncertainty quantification techniques for empirical prediction performance based on cross-validation. Optimized distributed ML pipeline to train 4.3M+ models on the Fraunhofer Edge Cloud. A writeup is available here.\nCourses: Machine learning; Deep learning and object recognition; Data visualization; Multivariate statistics.\nFinal grade: 1.2\n\n\n\nBachelor of Engineering: Sound and Video\nRSH & University of Applied Sciences D√ºsseldorf | 2010 ‚Äì 2022\n\nThesis: Audio feature extraction for predicting indoor soundscapes.\nFinal grade: 1.5"
  },
  {
    "objectID": "resume.html#other-accomplishments",
    "href": "resume.html#other-accomplishments",
    "title": "Resume",
    "section": "Other Accomplishments",
    "text": "Other Accomplishments\nOn the podium: Played solo horn in Bruckner‚Äôs 8th symphony with the D√ºsseldorf University Orchestra.\nOn two wheels: Cycled 300 km self-supported through Hessen."
  },
  {
    "objectID": "posts/thesis/tables/results_table_1st_full.html",
    "href": "posts/thesis/tables/results_table_1st_full.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Method\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ¬± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000"
  },
  {
    "objectID": "posts/soundscape-viz/index.html",
    "href": "posts/soundscape-viz/index.html",
    "title": "Soundscape Visualization App - Explore participants in a diverse dataset!",
    "section": "",
    "text": "Explore the app!\nVisualizing abstract data helps get an intuitive feel for the raw data that sometimes goes beyond summary statistics. In this post, I show how to visualize the D√ºsseldorf soundscape data set using Bokeh.\n\n\n\nRadar charts of static person information.\n\n\nThis project was part of the Data Visialization course at Technische Hochschule K√∂ln. The goal was to create an interactive visualization of the soundscape data set, that I helped collect during my time as a student research assistant at Hochschule D√ºsseldorf.\n\nData Set\nThe data set is published on Zenodo. It contains 6000+ soundscape rating from 100+ participants. The participants were asked to record and rate their soundscapes inside their private dwellings. Ther recordings are encoded to time-series of acoustic features and spectrograms to ensure privacy. However, the ratings, personal information, and situational context are included in the data set, which makes it a rich source for acoustic research.\n\n\nBokeh App\nYou can access the interactive app at the Posit Cloud. It allows you to explore the data set on an individual level.\nThe challenge with this app was, that I wanted to allow browsing through participants. We have to update the plots quickly, so we have to run a Bokeh server in the background. This makes deployment a bit more complicated, but the Posit Connect Cloud makes it easy to deploy Bokeh apps directly from the GitHub repository.\nFirst, I wanted to find a way to visualize the different personalities. I used a cluster of radar charts to show the personal factors in three fields: wellbeing, noise sensitivity, and trait. You can see an example of this at the top of this blog post.\nSecondly, person related meta data is visualized for example in the following bar plot to show composition of soundscape categories.\n\n\n\nBar charts of soundscape composition for a single participant.\n\n\nThe other part of the app visualizes the soundscape ratings directly as scatter plots. In the following example, we plot the ratings of personal valence against the ratings of personal arousal.\n\n\n\nScatter plot of personal arousal vs.¬†valence at the time of soundscape ratings.\n\n\nThis participant rated themself as either very aroused or very calm, but never in between. On the other hand, the valence ratings are more evenly distributed. Also, we can see a cluster of points in the top right corner, which indicates that the participant rated themself as very aroused and very happy at the same time.\n\n\nProject Report\nThe full project report is published at researchgate.net."
  },
  {
    "objectID": "posts/bicycle/index.html",
    "href": "posts/bicycle/index.html",
    "title": "Forecasting Bicycle Counts in Cologne",
    "section": "",
    "text": "A reliable forecast of the bicycle traffic is an important indicator for planning and optimizing the infrastructure of a city. In this post, I will predict the bicycle traffic in Cologne based on counting data from local bicycle counting stations.\nThis project was originally part of the course Machine Learning and Scientific Computing at TH K√∂ln and I worked on it in collaboration with my friend and collegue Felix Otte. While Felix supervised the model trainings and the evaluation of the models, I was responsible for data cleaning, exploration, and visualization. For this writeup, I performed re-runs of the original trainings and added a new class-based implementation of the whole pipeline that is closer to something useful in production.\nThis projects showcases the following skills:"
  },
  {
    "objectID": "posts/bicycle/index.html#evaluation",
    "href": "posts/bicycle/index.html#evaluation",
    "title": "Forecasting Bicycle Counts in Cologne",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we can evaluate the tuned pipeline of transformers and models on the evaluation data of the yet unseen year 2022.\n\n\nCode\nmetrics_df = ml_pipeline.eval(test_data)\n\n(\n    GT(metrics_df)\n    .tab_header(\"Metrics on Holdout Data\")\n    .cols_label(location=\"Location\", rmse=\"RMSE\", mape=\"MAPE\", count=\"Avg. Monthly Count\")\n    .cols_move(\"count\", after=\"location\")\n)\n\n\nC:\\Users\\rosen\\AppData\\Local\\Temp\\ipykernel_3060\\2838700285.py:194: CategoricalRemappingWarning:\n\nLocal categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n\n\n\n\n\n\n\n\n\nMetrics on Holdout Data\n\n\nLocation\nAvg. Monthly Count\nRMSE\nMAPE\nbaseline\n\n\n\n\nHohe Pforte\n92102\n7805\n0.08\ntrue\n\n\nVorgebirgswall\n92718\n10730\n0.1\nfalse\n\n\nA.-Silbermann-Weg\n83481\n11891\n0.11\nfalse\n\n\nNeumarkt\n126157\n15154\n0.11\nfalse\n\n\nZ√ºlpicher Stra√üe\n148239\n20062\n0.11\nfalse\n\n\nNiederl√§nder Ufer\n72004\n9610\n0.12\nfalse\n\n\nVenloer Stra√üe\n167610\n25447\n0.14\nfalse\n\n\nHohenzollernbr√ºcke\n73095\n12079\n0.15\nfalse\n\n\nA.-Sch√ºtte-Allee\n61827\n12985\n0.16\nfalse\n\n\nStadtwald\n74314\n15140\n0.16\nfalse\n\n\nNeusser Stra√üe\n98155\n17378\n0.16\nfalse\n\n\nDeutzer Br√ºcke\n142528\n26873\n0.17\nfalse\n\n\nSeverinsbr√ºcke\n48116\n10969\n0.19\nfalse\n\n\nBonner Stra√üe\n79392\n22168\n0.22\nfalse\n\n\nUniversit√§tsstra√üe\n133047\n35502\n0.23\nfalse\n\n\nVorgebirgspark\n31012\n9477\n0.27\nfalse\n\n\nRodenkirchener Br√ºcke\n52069\n14300\n0.27\nfalse\n\n\n\n\n\n\n        \n\n\n\nPlot the Predictions vs Actual Counts\nFirst, let‚Äôs start with some location that did really bad:\n\n\nCode\nml_pipeline.plot_predictions(test_data, \"Bonner Stra√üe\")\nml_pipeline.plot_predictions(test_data, \"Severinsbr√ºcke\")\n\n\n                                                \n\n\n                                                \n\n\nWhile our model for Severinsbr√ºcke seems to show the seasonal pattern, it is not able to predict the peaks with the correct magnitude. The model for Bonner Stra√üe is not able to capture the trend at all. Let‚Äôs have a look at the best location, on the other hand:\n\n\nCode\nml_pipeline.plot_predictions(test_data, \"Z√ºlpicher Stra√üe\")\nml_pipeline.plot_predictions(test_data, \"Hohe Pforte\")\nml_pipeline.plot_predictions(test_data, \"Venloer Stra√üe\")\n# def shap_single_loc(loc_str, data, pipeline_dict):\n#     data = data.filter(data[\"location\"] == loc_str)\n#     features = data.drop(\"count\").to_pandas()\n#     data = (\n#         data\n#         .with_columns(\n#             pl.Series(pipeline_dict[loc_str].predict(features)).alias(\"pred\")\n#         )\n#         .sort(\"date\")\n#     )\n\n#     pipe = pipeline_dict[loc_str]\n\n#     pred = model.predict(features, output_margin=True)\n\n#     explainer = shap.TreeExplainer(pipe[\"xgbregressor\"])\n#     explanation = explainer(pipe[\"columntransformer\"].transform(features))\n\n#     shap_values = explanation.values\n#     # make sure the SHAP values add up to marginal predictions\n#     print(f\"Max(Abs(ShapValues + BaseValues - Preds)) = {np.abs(shap_values.sum(axis=1) + explanation.base_values - pred).max()}\")\n#     shap.plots.beeswarm(explanation)\n\n# shap_single_loc(\"Z√ºlpicher Stra√üe\", test_data, pipeline_dict)\n# shap_single_loc(\"Bonner Stra√üe\", test_data, pipeline_dict)\n# shap_single_loc(\"Severinsbr√ºcke\", test_data, pipeline_dict)"
  },
  {
    "objectID": "posts/bicycle/index.html#future-extensions",
    "href": "posts/bicycle/index.html#future-extensions",
    "title": "Forecasting Bicycle Counts in Cologne",
    "section": "Future Extensions",
    "text": "Future Extensions\nAs a future addition, we can use the quantile regression feature of the XGBoost model to predict the upper and lower bounds of the counts. This can be useful for planning purposes and to get a better understanding of the uncertainty of the predictions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Hiüëã, I‚Äôm Fabian, a data scientist bridging machine learning and engineering.\nWith a M.Sc. in Media Technology, I specialize in developing robust machine learning solutions that perform reliably across simple and critical applications. My research on uncertainty quantification and cross-validation has equipped me with deep insights into performance estimation and statistical computing. I love package development and open-sourced a Python package for complex cross-validation workflows.\nMy interdisciplinary background‚Äîspanning audio engineering and data science‚Äîenables me to approach complex challenges with a holistic, end-to-end perspective. I‚Äôm passionate about working in cross-functional teams and collaborating with domain experts to deliver impactful solutions."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Forecasting Bicycle Counts in Cologne\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty quantification for cross-validation (Master Thesis Writeup)\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible cross validation and machine learning for regression on tabular data\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoundscape Visualization App - Explore participants in a diverse dataset!\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/flexcv/index.html",
    "href": "posts/flexcv/index.html",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "",
    "text": "flexcv is a Python package that implements flexible cross validation and machine learning for tabular data. It provides a range of features for comparing machine learning models on different datasets with different sets of predictors, customizing just about everything around cross validations. It supports both fixed and random effects, as well as random slopes.\nflexcv originated from the need to perform nested cross validation on grouped data. While I wrote most of the code during my time as a student research assistant at Hochschule D√ºsseldorf, I have continued to maintain and improve the package as a private open-source-project. Moreover, I decided to release it with full documentation, project page on GitHub pages, and continuous integration and deployment using GitHub Actions. Have a look at the project page!\nThe package is designed to be flexible and easy to use, with a focus on reproducibility and ease of use. It is built on top of popular machine learning libraries such as scikit-learn and pandas, and is designed to work seamlessly with these libraries.\nThis project showcases my"
  },
  {
    "objectID": "posts/flexcv/index.html#poster",
    "href": "posts/flexcv/index.html#poster",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Poster",
    "text": "Poster\n\n\nThis conference poster was a contribution to the Jahrestagung f√ºr Akustik DAGA 2024 and is published here."
  },
  {
    "objectID": "posts/flexcv/index.html#installation",
    "href": "posts/flexcv/index.html#installation",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Installation",
    "text": "Installation\nflexcv is on PyPI, so you can install it using pip:\npip install flexcv"
  },
  {
    "objectID": "posts/flexcv/index.html#usage",
    "href": "posts/flexcv/index.html#usage",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Usage",
    "text": "Usage\nHere is an example of how to use flexcv to perform nested cross validation on a regression problem. Let‚Äôs first load the modules and generate some sample data:\n\n# import the class interface, data generator and model\nfrom flexcv import CrossValidation\nfrom flexcv.synthesizer import generate_regression\nfrom flexcv.models import LinearModel\n\n# make sample data\nX, y, group, _ = generate_regression(10, 100, noise_level=0.01)\n\nNow the fun part about flexcv is its class interface. You can set up a complex configuration with just a few lines of code. Here is an example of how to set up a group cross validation with a linear model:\n\n# instantiate our cross validation class\ncv = CrossValidation()\n\n# now we can use method chaining to set up our configuration perform the cross validation\nresults = (\n    cv\n    .set_data(X, y, group, dataset_name=\"ExampleData\")\n    .set_splits(method_outer_split=\"GroupKFold\", method_inner_split=\"KFold\")\n    .add_model(LinearModel)\n    .perform()\n    .get_results()\n)\n\n# results has a summary property which returns a dataframe\n# we can simply call the pandas method \"to_excel\"\nresults.summary.to_excel(\"my_cv_results.xlsx\")\n\nI decided to use method chaining in the core interface, to make it easy to set up the configuration and perform the cross validation without having to remember a bunch of classes and functions. This approach leverages IDE hints and completion to guide the user through the process.\nVisit the project page to learn more. Feel free to reach out to me if you have any questions or suggestions, preferably using the issue tracker."
  },
  {
    "objectID": "posts/thesis/index.html",
    "href": "posts/thesis/index.html",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "",
    "text": "Cross-validation is a popular method to estimate the generalization performance of a machine learning model or algorithm. It also has its place in ML benchmarking experiments, where models or algorithms are compared based on their performance estimates. However, these estimates come with uncertainty, which is often neglected in practice. My master thesis explores different uncertainty quantification techniques for cross-validation estimates and evaluates their performance in terms of coverage and interval width. It is limited to the binary classification setting and focuses on the impact of sample size and machine learning algorithm choice on the performance of these techniques.\nI was extremely lucky to write this thesis with supervision of Dr.¬†Max Westphal at Fraunhofer MEVIS, Bremen. I totally enjoyed the process of developing this structure and writing this fairly mathematical thesis. Never have I thought I would choose such a topic when I failed 2 out of 3 math exams in my undergraduate studies. But here we are, and I am proud of the results. I hope you enjoy this writeup as much as I enjoyed writing the thesis."
  },
  {
    "objectID": "posts/thesis/index.html#introduction",
    "href": "posts/thesis/index.html#introduction",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "Introduction",
    "text": "Introduction\nYou have estimated you model performance using cross-validation (CV). So CV tells you something like ‚ÄúYour model accuracy is 85%‚Äù. Great job! But what does this number actually mean? Can we expect to see 85% accuracy on unseen data every time or does the model‚Äôs performance vary on new data? And how much does it vary? These are important questions that are often neglected in practice. Depending on the domain, it is extremly important, to know your model‚Äôs limits (Volovici et al. 2022). Think of medical applications, where a false negative can have severe consequences. But also in other domains, where it‚Äôs not about life and death, it is important to know the limits of your model. A model that predicts stock prices with an accuracy of 85% is not very useful if the uncertainty in this estimate is too large. Then the (unknown) real accuracy could be much lower (or higher), and you would not know it. A way to notate uncertainty is to construct confidence intervals (CIs) for the performance estimate. CIs have a frequentist interpretation. If we repeat the experiment including the CI construction many times, we would find the true performance to fall in the CI in 95% of the cases (when we set our nominal level to 95%). In my thesis I have performed experiments really often to explore the coverage probability of methods that yield CIs. The methods were selected by passing the following criteria:\n\ncompatible with binary classification,\nno additional model fits necessary,\nno alterations of the CV procedure.\n\nAs a side quest, I learned how to deal with over 200M CV validation predictions!\n\nResearch Questions\nThe following research questions guided me through my experiments and shaped how I set up the data generating processes:\n\nWhich uncertainty quantification technique provides the most accurate coverage for CV estimates in terms of deviation from nominal coverage?\nHow does the sample size affect the coverage and interval widths of different uncertainty quantification methods, particularly for smaller sample sizes?\nTo what extent does the choice of machine learning algorithm influence the coverage performance of uncertainty quantification techniques?"
  },
  {
    "objectID": "posts/thesis/index.html#theoretical-background",
    "href": "posts/thesis/index.html#theoretical-background",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "Theoretical Background",
    "text": "Theoretical Background\nFirst we have to make sure, that we‚Äôre talking about the same cross-validation scheme. There are variations and each of them can have stochastic implications, that do matter in this context.\n\n\\(k\\)-fold CV\n\nEvery observation is used once for validation\nMutually exclusive partitions\nIteratively use one partition for validation and the rest for training\nCV estimate is the average validation score\n\n\n\nProperties\nLet‚Äôs have a look at the key properties of CV that are important for the uncertainty quantification:\n\nCross-validation has an internal randomness due to the random splitting when \\(k_\\text{folds} &lt; n_\\text{obs}\\). When the number of folds equals the number of observations, we call it leave-one-out CV, and the estimate is deterministic.\nThere is an overlap of \\(\\frac{k-2}{k-1}\\) observations in 2 training sets of the cross-validation.\nAssuming model stability (Shalev-Shwartz and Ben-David 2014), i.e.¬†when a slight change of the training data does not change the model‚Äôs performance significantly, the validation scores obtained from the model are not i.i.d (Bengio and Grandvalet 2004; Markatou et al. 2005; Bates, Hastie, and Tibshirani 2023).\nNo unbiased estimator for the variance of the CV estimate (Bengio and Grandvalet 2004).\n\n\n\n\nCovariance matrix of k-fold CV as in (Bengio and Grandvalet 2004)."
  },
  {
    "objectID": "posts/thesis/index.html#methods-ademp",
    "href": "posts/thesis/index.html#methods-ademp",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "Methods: ADEMP",
    "text": "Methods: ADEMP\n\nWhat are we doing?\nIn short, we will compare the operating characteristics of different uncertainty quantification techniques for cross-validation estimates. We can observe the frerquentist properties of these methods by conducting a large number of simulations. Confidence intervals claim to include the ‚Äútrue‚Äù performance in a certain percentage of cases. We will simply count how often this holds and compare. That‚Äôs it, that‚Äôs the whole idea. Now, there are a few hidden challenges. For example, we have to come up with an efficient idea to calculate the ‚Äútrue‚Äù performance. I will talk about this in this part. Software-wise, I am using the great batchtools package for R to map data sets to machine learning algorithms and further to all the candidate methods. batchtools is a tremendous help when it comes to parallelizing tasks with so many factors. Actually, we will perform three separate studies sequentially to handle all the parts of this work.\nHere is an overview of the parts of the study before we dive into details: \nThere is a great framework that guides through simulation studies for comparing statistical methods and it‚Äôs called ADEMP (Morris, White, and Crowther 2019). ADEMP is a framework to plan, structure and report monte carlo simulation studies. In this ADEMP study I used pseudo random data to generate a large number of data sets. ADEMP also indicates, how to evaluate the coverage of the UQ techniques.\n\n\nA for AIMS\nThe aims in this study were guided by the research questions. We want to (1) find the best overall technique, (2) understand the influence of sample size and (3) understand the influence of learning algorithms.\n\n\nD for Data Generating Process\nNow it get‚Äôs more complex. It is important to describe how data is generated, so fellow readers can understand the intention and limitations. For my thesis I simulated a binary outcome from logistic models with std. normal covariates. In total, I used 12 different data generating processes (DGP) to cover a wide range of scenarios. Appropriate DGPs were determined by performing lots of pilot studies. Each DGP describes a distribution from which we can sample data sets. From each of the 12 DGPs I sampled data sets of sample sizes \\(n_\\text{obs} \\in \\{100, 500\\}\\). The number of covariates was varied. Some DGP also incorporated interaction terms to simulate non-linearities. I then used different learning algorithms inside of 5-fold cross-validation. The algorithms in use were Elastic Net (GLMNET), XGBoost and Support Vector Machine (SVM).\nAll these models have hyperparameters. When we tune these, we end up, with a distribution of performances that is skewed towards very good performances. This is why I instead sampled configurations randomly from appropriate search spaces. This way, we simulate a distribution of better or worse ML models, just as we would see in practice.\n\n\nE for Estimand\nA Monte Carlo simulation study is powerful, because we can know targets of inference by controlling the data generating process. The true performance can be approximated and is referred to as estimand. In my study, performance estimands were calculated for the AUC, accuracy, sensitivity and specificity metrics. From Bates, Hastie, and Tibshirani (2023) we know, that CV estimates are often close to the unconditional performance depending on training sets of size \\(n_\\text{train}\\). I will focus on that estimand in this post. Bates, Hastie, and Tibshirani (2023) is a great read and I totally recommend it to anyone interested in the topic. The paper says, that most often, the CV estimate is close to the performance of a algorithm trained on many training sets of size of the CV training folds drawn from the same population. So when we use this concept in the simulation, we either have to draw numerous data sets every time‚Ä¶ or we can use the classifiers trained in other simulations (using the same DGP).\nIn order to approximate the study estimands, we draw a large population data set from each DGP. In practice, we would never have the possibility to generate such a population. This is why controlling the DGP in a simulation study is so awesome. The approximation of the unconditional performance is defined as follows: It‚Äôs the average performance of classifiers trained in the CV iterations in repeated simulations as evaluated on the population data set. Each simulation is performed with a different random seed, so the data sets are different but drawn from the same DGP.\nAs a formal definition, we can write the estimand as: \\[\n\\tilde{\\psi}_{n_\\text{train}}= \\frac{1}{n_\\text{sim}k_\\text{folds}n_\\text{pop}} \\sum_{s=1}^{n_\\text{sim}}  \\sum_{k=1}^{k_\\text{folds}} \\sum_{i=1}^{n_\\text{pop}} L(\\phi_{k,s}(\\mathbf{x}_{i}), y_{i})\n\\]\nwhere \\(L\\) is the loss function, \\(\\phi_{k,s}\\) is the classifier trained in the \\(k\\)-th fold of the \\(s\\)-th simulation and evaluated on the \\(i\\)-th observation of the population data set. \\(n_\\text{sim}\\) is the number of simulations, \\(n_\\text{pop}\\) is the number of observations in the population data set.\nSo we average over all classifiers trained in CV iterations belonging to the same DGP + sample size + learning algorithm combination. This keeps the number of trainings, draws and evaluations manageable. The estimand is then used to evaluate the performance of the UQ techniques.\n\n\nM for Methods\n\nParametric approaches\n\nWe can construct Wald-intervals by pooling validation predictions (Baseline). We use the standard error of the proportion to calculate the confidence intervals. We define these Wald-intervals as our baseline method. All other methods are compared to this baseline and more complex methods should outperform the baseline.\nApproximation of the correlation similar to Bengio and Grandvalet (2004) (Param-CA) \\[\n\\hat{\\rho} = \\left(\\frac{n_\\text{folds} - 2}{n_\\text{folds} - 1}\\right)^r\n\\] \\[\nr \\in \\{0.1, 0.2, 0.5, 1, 2\\}\n\\]\n\n\nVariants: different distributions and transformation (normal or t; logit transformation possible) \n\n\n\nBootstrapping techniques\n\nNa√Øve parametric bootstrap (Boot-naive) is similar to Wald intervals. We simply bootstrap the pooled validation predictions and calculate the confidence intervals every time.\nHierarchical bootstrap (Boot-hier) (Davison and Hinkley 1997; Goldstein 2010)\n\nSample with replacement from the fold indices ‚Äì&gt;\nSample with replacement from the validation preds of the newly set of fold indices\nCalculate CV estimate on the new set of validation preds\n\n\n\nVariants: basic, normal, studentized, percentile\n\n\n\nGLMM\n\nModel fold information as random effect in a generalized linear mixed model (Eugster 2011): \\[\ng(\\mathbb{E}[Y|u_{k}])=Zu_{k}+\\epsilon_{k}\n\\]\n\nwhere \\(g(\\cdot)=\\ln(\\frac{p}{1-p})\\) is a link function, \\(Z\\) is the known random effects design matrix consisting of (validation) fold indices, \\(u_k\\) is the unknown random effect, and \\(\\epsilon_k\\) is an unknown vector of random errors, i.e., the residuals that are not explained by the model.\n\n\n\nP for Performance Measures\nFinally, we have to define how to measure the performance of the UQ techniques. Here are the key performance measures that we have to analyze: - Method failures (NA)\n\nCoverage probability\n\\(\\widehat{Cov} = \\frac{1}{n_\\text{sim}}\\sum_s 1(\\hat{\\theta}^l_s \\leq \\tilde{\\psi}_{n_\\text{train}} \\leq \\hat{\\theta}_s^u)\\)\nInterval width\n\\(\\hat{\\omega} = \\frac{1}{n_\\text{sim}}\\sum_s \\hat{\\theta}^u_s - \\hat{\\theta}^l_s\\)\nBoxplots can help identify general patterns of behaviour.\nMonte Carlo standard error (MCSE) indicate the precision of the Monte Carlo estimates.\n\n\\(n_\\text{sim}\\)=500 on Param\nWe have to use a much smaller \\(n_\\text{sim}\\) for Boot and GLMM due to computational constraints."
  },
  {
    "objectID": "posts/thesis/index.html#bottlenecks",
    "href": "posts/thesis/index.html#bottlenecks",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "Bottlenecks",
    "text": "Bottlenecks\nAs the previous part indicates, we have to deal with numerous different conditions, data sets, and models. Here are some of the bottlenecks I encountered during my thesis:\n\nTime\n\nTuning the DGP in early development took a lot of time. I wanted to make sure, to have meaningful data scenarios included, so I tweaked the DGPs a lot. This was necessary to ensure, that the scenarios are neither too easy nor too hard for the ML models. Lastly, this determins the distribution of estimands and therefore, the generalizability of the results.\nComputation times of candidate methods: bootstrapping and especcially the Bayesian implementation of GLMMs are computationally expensive. I had to optimize the code a lot to make it run in a reasonable time frame. However, I could not spend as much study repetitions on these longer running methods. This shows in higher Monte Carlo standard errors for these methods. However, by pooling scenarios, we can lower the SE sufficiently.\nAs some of the methods have note been implemented officially yet, debugging of implementations took also quite some time.\n\n\n\nMemory\n\nThe size of data sets from the machine learningare huge. Specifically, it is not possible to store all predicitons on the large population data set. This makes it necessary to apply all metrics as early as possible. Unfortunately, we can not compute metrics afterwards if desired.\nWhen the ML part is done, we end up with a large data set. This is passed to the UQ part, where the candidate methods go to work. When the full study is parallelized, it‚Äôs no longer possible to load the data set into memory across all workers. Instead, we have to filter it for the relevant portion that are used in the part. This was achieved by leveraging the power of Apache Arrow and parquet files. They can be filtered before loading the ML data and are much faster to load than the original data set.\nLastly, the data with all intervals is huge. I almost entirely used data.table from here on. It allows super fast joins by reference. This came in handy, when I merged the UQ data set with the estimand data that was calculated before."
  },
  {
    "objectID": "posts/thesis/index.html#evaluation",
    "href": "posts/thesis/index.html#evaluation",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "Evaluation",
    "text": "Evaluation\nWhile this does not show the frequentist properties, that I found, the following image luckily summarises my findings in a single case:\n\n\n\nExample coverage.\n\n\nWe can see, that the parametric methods yield too small intervals that don‚Äôt cover the estimand in this case. The GLMM methods cover, but have a width that is simply unusably wide. The bootstrap methods are just in between, often covering just right and having a width that is acceptable. The hierarchical bootstrap methods are the best in this case. The percentile and studentized versions are en par often.\nLet‚Äôs have a look at one of the many results tables in my thesis. This table shows results for the AUC metric. The average estimand and its standard deviation are shown in the grouping rows. The top part shows results for data sets of size 100, the bottom part of the table shows results for data sets of size 500. As you can see from the \\(n_\\text{sim}\\) column, I was able to perform a lot more Monte Carlo repetitions on the fast parametric methods as compared to bootstrap and GLMM. Note, how GLMM has at least nominal coverage but shows intervals that are extremly wide, on average. The baseline represents the naive Wald-intervals on the pooled validation predictions.\n\n\n\nMethod\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ¬± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000\n\n\nAUC (0.75 ¬± 0.16) - \\(n_\\text{obs}\\) = 500\n\n\nGLMM-UG\n95.9 (0.19)\n0.312 (0.0017)\n11520\n\n\nParam-CA-0.1-stud\n92.2 (0.04)\n0.182 (0.0001)\n360006\n\n\nBoot-hier-stud\n91.3 (0.20)\n0.127 (0.0008)\n19155\n\n\nBaseline\n79.5 (0.07)\n0.085 (0.0001)\n360000\n\n\n\nThe method ‚ÄòGLMM-UG‚Äô seems to do a great job at closing on the nominal coverage. However, the intervals are too wide to be useful. The hierarchical bootstrap methods are the best trade-off between coverage and width. The sample size is crucial for intervals with good coverage and width.\nTo get a more general idea of coverage and how it is dependent on the sample size and evaluation metric, we can look at the following boxplots: \nBoxplots for sample size vs.¬†width also support the finding, that a larger sample size is crucial for intervals with useable widths: \nAcross all four metrics, we see improved (narrower) widths in almost all methods under test. This is congruent with what we would expect from the theory. Unfortunately, this means, that we can not always have reliable UQ for all metrics, when the sample size is too small.\n\nKey findings\nTo keep this short, here are the key findings of my thesis:\n\nBaseline is not sufficient (supported by Bates, Hastie, and Tibshirani (2023))\nPoor coverage for sensitivity and specificity (all methods).\nIssues in edge case where \\(n_\\text{obs}\\) = 100 and prevalence = 0.2. Here, the effective sample size in the validation folds is too low.\nSkewness of estimand distribution is a problem (supported by Reed (2007)).\nGLMM intervals are reliable but unusably wide.\nBootstrap: hierarchical shows good trade-off between coverage and width.\nOur correlation approximation is promising but lacks consistency across the scenarios.\nSample size is crucial for intervals with good coverage and width.\nCoverage on sensitivity and specificity are problematic."
  },
  {
    "objectID": "posts/thesis/index.html#future-research",
    "href": "posts/thesis/index.html#future-research",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "Future research",
    "text": "Future research\n\nBraschel et al. (2016): higher number of clusters in the data increases coverage.\nTest hierarchical bootstrap with different number of folds.\nRun hierarchical bootstrap with higher \\(n_\\text{sim}\\).\nFind hyperparameter strategy for the correlation approximation."
  },
  {
    "objectID": "posts/thesis/index.html#my-thesis-in-numbers",
    "href": "posts/thesis/index.html#my-thesis-in-numbers",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "My thesis in numbers",
    "text": "My thesis in numbers\nDuring the main simulation, 1440 scenarios (12 DGP \\(\\times\\) 2 sample sizes \\(\\times\\) 3 learning algorithms \\(\\times\\) 20 configurations) are repeated 500 times resulting in \\(720000\\) ML unique jobs. In each job, the learning algorithm was trained 5 times in the CV and once on the available data set. This corresponds to \\(4.32 \\times 10^6\\) model fits. All trained models are used to predict on the population data set, resulting in \\(1.728\\times 10^{11}\\) predictions on the population data set. The table of validation predictions from CV has \\(2.16\\times 10^8\\) rows. After running the uncertainty quantification methods, the results table containing all the confidence intervals has 74 million rows. The study ran on two nodes of a high performance computing cluster. The data.table, mlr3, batchtools, snow, and arrow packages were used to manage the extreme computational load and the memory demands during asynchronuous parallel processing."
  },
  {
    "objectID": "posts/thesis/index.html#references",
    "href": "posts/thesis/index.html#references",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Writeup)",
    "section": "References",
    "text": "References\n\n\nBates, Stephen, Trevor Hastie, and Robert Tibshirani. 2023. ‚ÄúCross-Validation: What Does It Estimate and How Well Does It Do It?‚Äù Journal of the American Statistical Association, May. https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686.\n\n\nBengio, Yoshua, and Yves Grandvalet. 2004. ‚ÄúNo Unbiased Estimator of the Variance of k-Fold Cross-Validation.‚Äù Journal of Machine Learning Research, no. 5.\n\n\nBraschel, Melissa C, Ivana Svec, Gerarda A Darlington, and Allan Donner. 2016. ‚ÄúA Comparison of Confidence Interval Methods for the Intraclass Correlation Coefficient in Community-Based Cluster Randomization Trials with a Binary Outcome.‚Äù Clinical Trials 13 (2): 180‚Äì87. https://doi.org/10.1177/1740774515606377.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\n\nEugster, Manuel J A. 2011. ‚ÄúBenchmark Experiments ‚Äì a Tool for Analyzing Statistical Learning Algorithms.‚Äù PhD thesis, M√ºnchen: LMU M√ºnchen. https://edoc.ub.uni-muenchen.de/12990/1/Eugster_Manuel_J_A.pdf.\n\n\nGoldstein, Harvey. 2010. ‚ÄúBootstrapping in Multilevel Models.‚Äù In Handbook of Advanced Multilevel Analysis. Routledge.\n\n\nMarkatou, Marianthi, Hong Tian, Shameek Biswas, and George Hripcsak. 2005. ‚ÄúAnalysis of Variance of Cross-Validation Estimators of the Generalization Error.‚Äù Journal of Machine Learning Research 6 (39): 1127‚Äì68. http://jmlr.org/papers/v6/markatou05a.html.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2019. ‚ÄúUsing Simulation Studies to Evaluate Statistical Methods.‚Äù Statistics in Medicine 38 (11): 2074‚Äì2102. https://doi.org/10.1002/sim.8086.\n\n\nReed, James F. III. 2007. ‚ÄúBetter Binomial Confidence Intervals.‚Äù Journal of Modern Applied Statistical Methods 6 (May): 153‚Äì61. https://doi.org/10.56801/10.56801/v6.i.290.\n\n\nShalev-Shwartz, Shai, and Shai Ben-David. 2014. Understanding Machine Learning: From Theory to Algorithms. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9781107298019.\n\n\nVolovici, Victor, Nicholas L. Syn, Ari Ercole, Joseph J. Zhao, and Nan Liu. 2022. ‚ÄúSteps to Avoid Overuse and Misuse of Machine Learning in Clinical Research.‚Äù Nature Medicine 28 (10): 1996‚Äì99. https://doi.org/10.1038/s41591-022-01961-6."
  },
  {
    "objectID": "posts/thesis/tables/results_table_2nd_full.html",
    "href": "posts/thesis/tables/results_table_2nd_full.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Method\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ¬± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000\n\n\nAUC (0.75 ¬± 0.16) - \\(n_\\text{obs}\\) = 500\n\n\nGLMM-UG\n95.9 (0.19)\n0.312 (0.0017)\n11520\n\n\nParam-CA-0.1-stud\n92.2 (0.04)\n0.182 (0.0001)\n360006\n\n\nBoot-hier-stud\n91.3 (0.20)\n0.127 (0.0008)\n19155\n\n\nBaseline\n79.5 (0.07)\n0.085 (0.0001)\n360000"
  }
]
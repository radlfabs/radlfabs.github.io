[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "I‚Äôm Fabian Rosenthal, a Data Scientist and developer. With my engineering background, I bring a cross-disciplinary perspective to data science. I am specialized in performance estimation and uncertainty quantification. In my work and university projects, I have proven the ability to identify the right tool for a problem, communicate it to the team, master it, and ship it quickly.\n\n\n\n\n\n\n\n\n\nLanguages\n\n\n\nEnglish (C1)\nGerman (native)\n\n\n\n\n\nSoft skills\n\n\n\nMeticulous\nVersatile\nEmpathetic\nCreative"
  },
  {
    "objectID": "resume.html#profile",
    "href": "resume.html#profile",
    "title": "Resume",
    "section": "",
    "text": "I‚Äôm Fabian Rosenthal, a Data Scientist and developer. With my engineering background, I bring a cross-disciplinary perspective to data science. I am specialized in performance estimation and uncertainty quantification. In my work and university projects, I have proven the ability to identify the right tool for a problem, communicate it to the team, master it, and ship it quickly.\n\n\n\n\n\n\n\n\n\nLanguages\n\n\n\nEnglish (C1)\nGerman (native)\n\n\n\n\n\nSoft skills\n\n\n\nMeticulous\nVersatile\nEmpathetic\nCreative"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\nProgramming\n\n\n\nPython, R, SQL\nLaTeX, Matlab\nRust (learning)\n\n\n\n\n\nFrameworks\n\n\n\nPyTorch, scikit-learn\nPandas, Polars, Arrow\nmlr3, data.table\ntidyverse, tidymodels\nBokeh, Shap, ggplot2\n\n\n\n\n\nTools\n\n\n\nQuarto, Tableau\nGit, Docker, Linux CLI, Parquet"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\n\n\n\n\n\n\nStudent Research Assistant\nFraunhofer MEVIS (Bremen) | 02/2024 ‚Äì 08/2024\n\n\n\nDeveloped statistical R-packages with uncertainty quantification techniques, giving clinicians a more robust understanding of ML models.\nDesigned unified APIs with consistent code styling and good unit tests.\n\n\n\n\n\n\n\n\n\n\nStudent Research Assistant\nUniversity of Applied Sciences D√ºsseldorf | 09/2020 ‚Äì 10/2023\n\n\n\nCo-published a data set with soundscapes from 100+ participants.\nPerformed data and feature engineering on 6000+ audio recordings.\nSaved days of compute by employing a faster hyperparameter tuning.\nIntroduced a solution to keep track of 1000+ experiments.\nKeynotes and posters on the use of stats & ML in acoustic research.\n\n\n\n\n\n\n\n\n\n\nVideo Recording Producer\nWDR (K√∂ln) | 09/2020 ‚Äì 02/2022\n\n\n\nCalled cameras for live-streaming concerts in a musically sensible way.\nLed technical teams and communicated cinematographic ideas efficiently.\n\n\n\n\n\n\n\n\n\n\nBike Messenger\nSelf-employed (D√ºsseldorf) | 11/2016 ‚Äì 09/2020\n\n\nCompleted time-critical and valuable deliveries in all weather conditions while optimizing complex urban routes in real-time."
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Resume",
    "section": "Projects",
    "text": "Projects\n\nflexcv\nUniversity of Applied Sciences D√ºsseldorf & Private Project\n\nA Python package simplifying nested cross-validation on grouped data.\nEnables reproducible multimodel workflows with few lines of code.\nReleased and maintained as OSS using GitHub Actions (CI/CD).\nProject page\n\n\n\nTime Series Forecasting\nUniversity of Applied Sciences Cologne\n\nPrediction of open bicycle counter data for Cologne and benchmarking of ML-algorithms with time series cross-validation.\nFine-tuned XGBoost with external weather data and seasonal trends beats the best linear models.\n\n\n\nGridNet White Balance\nUniversity of Applied Sciences Cologne\n\nAchieved improvement in color consistency metrics with GridNet.\nDeployed the model within a custom camera pipeline.\nStrong communication and co-operation in a 5-person team.\n\n\n\nSoundscape Viz App\nUniversity of Applied Sciences Cologne - Interactive app visualizing survey data on individual levels. - Enables intuitive exploration of the diversity of participants. - Deployed here on the Posit Connect Cloud."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\nMaster of Science: Media Technology\nUniversity of Applied Sciences Cologne | 2022 ‚Äì 2024\n\nThesis: Comparison of uncertainty quantification techniques for empirical prediction performance based on cross-validation. Optimized distributed ML pipeline to train 4.3M+ models on the Fraunhofer Edge Cloud. A writeup is available here.\nCourses: Machine learning; Deep learning and object recognition; Data visualization; Multivariate statistics.\nFinal grade: 1.2\n\n\n\nBachelor of Engineering: Sound and Video\nRSH & University of Applied Sciences D√ºsseldorf | 2010 ‚Äì 2022\n\nThesis: Audio feature extraction for predicting indoor soundscapes.\nFinal grade: 1.5"
  },
  {
    "objectID": "resume.html#other-accomplishments",
    "href": "resume.html#other-accomplishments",
    "title": "Resume",
    "section": "Other Accomplishments",
    "text": "Other Accomplishments\nOn the podium: Played solo horn in Bruckner‚Äôs 8th symphony with the D√ºsseldorf University Orchestra.\nOn two wheels: Cycled 300 km self-supported through Hessen."
  },
  {
    "objectID": "posts/thesis/tables/results_table_2nd_full.html",
    "href": "posts/thesis/tables/results_table_2nd_full.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Method\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ¬± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000\n\n\nAUC (0.75 ¬± 0.16) - \\(n_\\text{obs}\\) = 500\n\n\nGLMM-UG\n95.9 (0.19)\n0.312 (0.0017)\n11520\n\n\nParam-CA-0.1-stud\n92.2 (0.04)\n0.182 (0.0001)\n360006\n\n\nBoot-hier-stud\n91.3 (0.20)\n0.127 (0.0008)\n19155\n\n\nBaseline\n79.5 (0.07)\n0.085 (0.0001)\n360000"
  },
  {
    "objectID": "posts/thesis/index.html",
    "href": "posts/thesis/index.html",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "",
    "text": "Cross-validation is a popular method to estimate the generalization performance of a machine learning model or algorithm. It also has its place in ML benchmarking experiments, where models or algorithms are compared based on their performance estimates. However, these estimates come with uncertainty, which is often neglected in practice. My master thesis explores different uncertainty quantification techniques for cross-validation estimates and evaluates their performance in terms of coverage and interval width. It is limited to the binary classification setting and focuses on the impact of sample size and machine learning algorithm choice on the performance of these techniques.\nI was extremely lucky to write this thesis with supervision of Dr.¬†Max Westphal at Fraunhofer MEVIS, Bremen, and Prof.¬†Dr.¬†Beate Rhein from TH K√∂ln. I totally enjoyed the process of developing this structure and writing this fairly mathematical thesis. Never have I thought I would choose such a topic when I failed 2 out of 3 math exams in my undergraduate studies. But here we are, and I am proud of the results. I hope you enjoy this write-up as much as I enjoyed writing the thesis."
  },
  {
    "objectID": "posts/thesis/index.html#introduction",
    "href": "posts/thesis/index.html#introduction",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Introduction",
    "text": "Introduction\nYou have estimated you model performance using cross-validation (CV). So CV tells you something like ‚ÄúYour model accuracy is 85%‚Äù. Great job! But what does this number actually mean? Can we expect to see 85% accuracy on unseen data every time or does the model‚Äôs performance vary on new data? And how much does it vary? These are important questions that are often neglected in practice. Depending on the domain, it is extremly important, to know your model‚Äôs limits (Volovici et al. 2022). Think of medical applications, where a false negative can have severe consequences. But also in other domains, where it‚Äôs not about life and death, it is important to know the limits of your model. A model that predicts stock prices with an accuracy of 85% is not very useful if the uncertainty in this estimate is too large. Then the (unknown) real accuracy could be much lower (or higher), and you would not know it. A way to notate uncertainty is to construct confidence intervals (CIs) for the performance estimate. CIs have a frequentist interpretation. If we repeat the experiment including the CI construction many times, we would find the true performance to fall in the CI in 95% of the cases (when we set our nominal level to 95%). In my thesis I have performed experiments really often to explore the coverage probability of methods that yield CIs. The methods were selected by passing the following criteria:\n\ncompatible with binary classification,\nno additional model fits necessary,\nno alterations of the CV procedure.\n\nAs a side quest, I learned how to deal with over 200M CV validation predictions!\n\nResearch Questions\nThe following research questions guided me through my experiments and shaped how I set up the data generating processes:\n\nWhich uncertainty quantification technique provides the most accurate coverage for CV estimates in terms of deviation from nominal coverage?\nHow does the sample size affect the coverage and interval widths of different uncertainty quantification methods, particularly for smaller sample sizes?\nTo what extent does the choice of machine learning algorithm influence the coverage performance of uncertainty quantification techniques?"
  },
  {
    "objectID": "posts/thesis/index.html#theoretical-background",
    "href": "posts/thesis/index.html#theoretical-background",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Theoretical Background",
    "text": "Theoretical Background\nFirst we have to make sure, that we‚Äôre talking about the same cross-validation scheme. There are variations and each of them can have stochastic implications, that do matter in this context.\n\n\\(k\\)-fold CV\n\nEvery observation is used once for validation\nMutually exclusive partitions\nIteratively use one partition for validation and the rest for training\nCV estimate is the average validation score\n\n\n\nProperties\nLet‚Äôs have a look at the key properties of CV that are important for the uncertainty quantification:\n\nCross-validation has an internal randomness due to the random splitting when \\(k_\\text{folds} &lt; n_\\text{obs}\\). When the number of folds equals the number of observations, we call it leave-one-out CV, and the estimate is deterministic.\nThere is an overlap of \\(\\frac{k-2}{k-1}\\) observations in 2 training sets of the cross-validation.\nAssuming model stability (Shalev-Shwartz and Ben-David 2014), i.e.¬†when a slight change of the training data does not change the model‚Äôs performance significantly, the validation scores obtained from the model are not i.i.d (Bengio and Grandvalet 2004; Markatou et al. 2005; Bates, Hastie, and Tibshirani 2023).\nNo unbiased estimator for the variance of the CV estimate (Bengio and Grandvalet 2004).\n\n\n\n\nCovariance matrix of k-fold CV as in (Bengio and Grandvalet 2004)."
  },
  {
    "objectID": "posts/thesis/index.html#methods-ademp",
    "href": "posts/thesis/index.html#methods-ademp",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Methods: ADEMP",
    "text": "Methods: ADEMP\n\nWhat are we doing?\nIn short, we will compare the operating characteristics of different uncertainty quantification techniques for cross-validation estimates. We can observe the frerquentist properties of these methods by conducting a large number of simulations. Confidence intervals claim to include the ‚Äútrue‚Äù performance in a certain percentage of cases. We will simply count how often this holds and compare. That‚Äôs it, that‚Äôs the whole idea. Now, there are a few hidden challenges. For example, we have to come up with an efficient idea to calculate the ‚Äútrue‚Äù performance. I will talk about this in this part. Software-wise, I am using the great batchtools package for R to map data sets to machine learning algorithms and further to all the candidate methods. batchtools is a tremendous help when it comes to parallelizing tasks with so many factors. Actually, we will perform three separate studies sequentially to handle all the parts of this work.\nHere is an overview of the parts of the study before we dive into details: \nThere is a great framework that guides through simulation studies for comparing statistical methods and it‚Äôs called ADEMP (Morris, White, and Crowther 2019). ADEMP is a framework to plan, structure and report monte carlo simulation studies. In this ADEMP study I used pseudo random data to generate a large number of data sets. ADEMP also indicates, how to evaluate the coverage of the UQ techniques.\n\n\nA for AIMS\nThe aims in this study were guided by the research questions. We want to (1) find the best overall technique, (2) understand the influence of sample size and (3) understand the influence of learning algorithms.\n\n\nD for Data Generating Process\nNow it get‚Äôs more complex. It is important to describe how data is generated, so fellow readers can understand the intention and limitations. For my thesis I simulated a binary outcome from logistic models with std. normal covariates. In total, I used 12 different data generating processes (DGP) to cover a wide range of scenarios. Appropriate DGPs were determined by performing lots of pilot studies. Each DGP describes a distribution from which we can sample data sets. From each of the 12 DGPs I sampled data sets of sample sizes \\(n_\\text{obs} \\in \\{100, 500\\}\\). The number of covariates was varied. Some DGP also incorporated interaction terms to simulate non-linearities. I then used different learning algorithms inside of 5-fold cross-validation. The algorithms in use were Elastic Net (GLMNET), XGBoost and Support Vector Machine (SVM).\nAll these models have hyperparameters. When we tune these, we end up, with a distribution of performances that is skewed towards very good performances. This is why I instead sampled configurations randomly from appropriate search spaces. This way, we simulate a distribution of better or worse ML models, just as we would see in practice.\n\n\nE for Estimand\nA Monte Carlo simulation study is powerful, because we can know targets of inference by controlling the data generating process. The true performance can be approximated and is referred to as estimand. In my study, performance estimands were calculated for the AUC, accuracy, sensitivity and specificity metrics. From Bates, Hastie, and Tibshirani (2023) we know, that CV estimates are often close to the unconditional performance depending on training sets of size \\(n_\\text{train}\\). I will focus on that estimand in this post. Bates, Hastie, and Tibshirani (2023) is a great read and I totally recommend it to anyone interested in the topic. The paper says, that most often, the CV estimate is close to the performance of a algorithm trained on many training sets of size of the CV training folds drawn from the same population. So when we use this concept in the simulation, we either have to draw numerous data sets every time‚Ä¶ or we can use the classifiers trained in other simulations (using the same DGP).\nIn order to approximate the study estimands, we draw a large population data set from each DGP. In practice, we would never have the possibility to generate such a population. This is why controlling the DGP in a simulation study is so awesome. The approximation of the unconditional performance is defined as follows: It‚Äôs the average performance of classifiers trained in the CV iterations in repeated simulations as evaluated on the population data set. Each simulation is performed with a different random seed, so the data sets are different but drawn from the same DGP.\nAs a formal definition, we can write the estimand as: \\[\n\\tilde{\\psi}_{n_\\text{train}}= \\frac{1}{n_\\text{sim}k_\\text{folds}n_\\text{pop}} \\sum_{s=1}^{n_\\text{sim}}  \\sum_{k=1}^{k_\\text{folds}} \\sum_{i=1}^{n_\\text{pop}} L(\\phi_{k,s}(\\mathbf{x}_{i}), y_{i})\n\\]\nwhere \\(L\\) is the loss function, \\(\\phi_{k,s}\\) is the classifier trained in the \\(k\\)-th fold of the \\(s\\)-th simulation and evaluated on the \\(i\\)-th observation of the population data set. \\(n_\\text{sim}\\) is the number of simulations, \\(n_\\text{pop}\\) is the number of observations in the population data set.\nSo we average over all classifiers trained in CV iterations belonging to the same DGP + sample size + learning algorithm combination. This keeps the number of trainings, draws and evaluations manageable. The estimand is then used to evaluate the performance of the UQ techniques.\n\n\nM for Methods\n\nParametric approaches\n\nWe can construct Wald-intervals by pooling validation predictions (Baseline). We use the standard error of the proportion to calculate the confidence intervals. We define these Wald-intervals as our baseline method. All other methods are compared to this baseline and more complex methods should outperform the baseline.\nApproximation of the correlation similar to Bengio and Grandvalet (2004) (Param-CA) \\[\n\\hat{\\rho} = \\left(\\frac{n_\\text{folds} - 2}{n_\\text{folds} - 1}\\right)^r\n\\] \\[\nr \\in \\{0.1, 0.2, 0.5, 1, 2\\}\n\\]\n\n\nVariants: different distributions and transformation (normal or t; logit transformation possible) \n\n\n\nBootstrapping techniques\n\nNa√Øve parametric bootstrap (Boot-naive) is similar to Wald intervals. We simply bootstrap the pooled validation predictions and calculate the confidence intervals every time.\nHierarchical bootstrap (Boot-hier) (Davison and Hinkley 1997; Goldstein 2010)\n\nSample with replacement from the fold indices ‚Äì&gt;\nSample with replacement from the validation preds of the newly set of fold indices\nCalculate CV estimate on the new set of validation preds\n\n\n\nVariants: basic, normal, studentized, percentile\n\n\n\nGLMM\n\nModel fold information as random effect in a generalized linear mixed model (Eugster 2011): \\[\ng(\\mathbb{E}[Y|u_{k}])=Zu_{k}+\\epsilon_{k}\n\\]\n\nwhere \\(g(\\cdot)=\\ln(\\frac{p}{1-p})\\) is a link function, \\(Z\\) is the known random effects design matrix consisting of (validation) fold indices, \\(u_k\\) is the unknown random effect, and \\(\\epsilon_k\\) is an unknown vector of random errors, i.e., the residuals that are not explained by the model.\n\n\n\nP for Performance Measures\nFinally, we have to define how to measure the performance of the UQ techniques. Here are the key performance measures that we have to analyze: - Method failures (NA)\n\nCoverage probability\n\\(\\widehat{Cov} = \\frac{1}{n_\\text{sim}}\\sum_s 1(\\hat{\\theta}^l_s \\leq \\tilde{\\psi}_{n_\\text{train}} \\leq \\hat{\\theta}_s^u)\\)\nInterval width\n\\(\\hat{\\omega} = \\frac{1}{n_\\text{sim}}\\sum_s \\hat{\\theta}^u_s - \\hat{\\theta}^l_s\\)\nBoxplots can help identify general patterns of behaviour.\nMonte Carlo standard error (MCSE) indicate the precision of the Monte Carlo estimates. The more repetitions we can do, the better (smaller) the MCSE. We have to use a much smaller \\(n_\\text{sim}\\) for Boot and GLMM due to computational constraints. So we will see higher MCSE in these Monte Carlo estimates of coverage or width.\n\n:::"
  },
  {
    "objectID": "posts/thesis/index.html#bottlenecks",
    "href": "posts/thesis/index.html#bottlenecks",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Bottlenecks",
    "text": "Bottlenecks\nAs the previous part indicates, we have to deal with numerous different conditions, data sets, and models. Here are some of the bottlenecks I encountered during my thesis:\n\nTime\n\nTuning the DGP in early development took a lot of time. I wanted to make sure, to have meaningful data scenarios included, so I tweaked the DGPs a lot. This was necessary to ensure, that the scenarios are neither too easy nor too hard for the ML models. Lastly, this determins the distribution of estimands and therefore, the generalizability of the results.\nComputation times of candidate methods: bootstrapping and especcially the Bayesian implementation of GLMMs are computationally expensive. I had to optimize the code a lot to make it run in a reasonable time frame. However, I could not spend as much study repetitions on these longer running methods. This shows in higher Monte Carlo standard errors for these methods. However, by pooling scenarios, we can lower the SE sufficiently.\nAs some of the methods have note been implemented officially yet, debugging of implementations took also quite some time.\n\n\n\nMemory\n\nThe size of data sets from the machine learningare huge. Specifically, it is not possible to store all predicitons on the large population data set. This makes it necessary to apply all metrics as early as possible. Unfortunately, we can not compute metrics afterwards if desired.\nWhen the ML part is done, we end up with a large data set. This is passed to the UQ part, where the candidate methods go to work. When the full study is parallelized, it‚Äôs no longer possible to load the data set into memory across all workers. Instead, we have to filter it for the relevant portion that are used in the part. This was achieved by leveraging the power of Apache Arrow and parquet files. They can be filtered before loading the ML data and are much faster to load than the original data set.\nLastly, the data with all intervals is huge. I almost entirely used data.table from here on. It allows super fast joins by reference. This came in handy, when I merged the UQ data set with the estimand data that was calculated before."
  },
  {
    "objectID": "posts/thesis/index.html#evaluation",
    "href": "posts/thesis/index.html#evaluation",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Evaluation",
    "text": "Evaluation\nWhile this does not show the frequentist properties, that I found, the following image luckily summarises my findings in a single case:\n\n\n\nExample coverage.\n\n\nWe can see, that the parametric methods yield too small intervals that don‚Äôt cover the estimand in this case. The GLMM methods cover, but have a width that is simply unusably wide. The bootstrap methods are just in between, often covering just right and having a width that is acceptable. The hierarchical bootstrap methods are the best in this case. The percentile and studentized versions are en par often.\nLet‚Äôs have a look at one of the many results tables in my thesis. This table shows results for the AUC metric. The average estimand and its standard deviation are shown in the grouping rows. The top part shows results for data sets of size 100, the bottom part of the table shows results for data sets of size 500. As you can see from the \\(n_\\text{sim}\\) column, I was able to perform a lot more Monte Carlo repetitions on the fast parametric methods as compared to bootstrap and GLMM. Note, how GLMM has at least nominal coverage but shows intervals that are extremly wide, on average. The baseline represents the naive Wald-intervals on the pooled validation predictions.\n\n\n\nMethod\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ¬± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000\n\n\nAUC (0.75 ¬± 0.16) - \\(n_\\text{obs}\\) = 500\n\n\nGLMM-UG\n95.9 (0.19)\n0.312 (0.0017)\n11520\n\n\nParam-CA-0.1-stud\n92.2 (0.04)\n0.182 (0.0001)\n360006\n\n\nBoot-hier-stud\n91.3 (0.20)\n0.127 (0.0008)\n19155\n\n\nBaseline\n79.5 (0.07)\n0.085 (0.0001)\n360000\n\n\n\nThe method ‚ÄòGLMM-UG‚Äô seems to do a great job at closing on the nominal coverage. However, the intervals are too wide to be useful. The hierarchical bootstrap methods are the best trade-off between coverage and width. The sample size is crucial for intervals with good coverage and width.\nTo get a more general idea of coverage and how it is dependent on the sample size and evaluation metric, we can look at the following boxplots: \nBoxplots for sample size vs.¬†width also support the finding, that a larger sample size is crucial for intervals with useable widths: \nAcross all four metrics, we see improved (narrower) widths in almost all methods under test. This is congruent with what we would expect from the theory. Unfortunately, this means, that we can not always have reliable UQ for all metrics, when the sample size is too small.\n\nKey findings\nTo keep this short, here are the key findings of my thesis:\n\nBaseline is not sufficient (supported by Bates, Hastie, and Tibshirani (2023))\nPoor coverage for sensitivity and specificity (all methods).\nIssues in edge case where \\(n_\\text{obs}\\) = 100 and prevalence = 0.2. Here, the effective sample size in the validation folds is too low.\nSkewness of estimand distribution is a problem (supported by Reed (2007)).\nGLMM intervals are reliable but unusably wide.\nBootstrap: hierarchical shows good trade-off between coverage and width.\nOur correlation approximation is promising but lacks consistency across the scenarios.\nSample size is crucial for intervals with good coverage and width.\nCoverage on sensitivity and specificity are problematic."
  },
  {
    "objectID": "posts/thesis/index.html#future-research",
    "href": "posts/thesis/index.html#future-research",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Future research",
    "text": "Future research\n\nBraschel et al. (2016): higher number of clusters in the data increases coverage.\nTest hierarchical bootstrap with different number of folds.\nRun hierarchical bootstrap with higher \\(n_\\text{sim}\\).\nFind hyperparameter strategy for the correlation approximation."
  },
  {
    "objectID": "posts/thesis/index.html#my-thesis-in-numbers",
    "href": "posts/thesis/index.html#my-thesis-in-numbers",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "My thesis in numbers",
    "text": "My thesis in numbers\nDuring the main simulation, 1440 scenarios (12 DGP \\(\\times\\) 2 sample sizes \\(\\times\\) 3 learning algorithms \\(\\times\\) 20 configurations) are repeated 500 times resulting in \\(720000\\) ML unique jobs. In each job, the learning algorithm was trained 5 times in the CV and once on the available data set. This corresponds to \\(4.32 \\times 10^6\\) model fits. All trained models are used to predict on the population data set, resulting in \\(1.728\\times 10^{11}\\) predictions on the population data set. The table of validation predictions from CV has \\(2.16\\times 10^8\\) rows. After running the uncertainty quantification methods, the results table containing all the confidence intervals has 74 million rows. The study ran on two nodes of a high performance computing cluster. The data.table, mlr3, batchtools, snow, and arrow packages were used to manage the extreme computational load and the memory demands during asynchronuous parallel processing."
  },
  {
    "objectID": "posts/thesis/index.html#references",
    "href": "posts/thesis/index.html#references",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "References",
    "text": "References\n\n\nBates, Stephen, Trevor Hastie, and Robert Tibshirani. 2023. ‚ÄúCross-Validation: What Does It Estimate and How Well Does It Do It?‚Äù Journal of the American Statistical Association, May. https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686.\n\n\nBengio, Yoshua, and Yves Grandvalet. 2004. ‚ÄúNo Unbiased Estimator of the Variance of k-Fold Cross-Validation.‚Äù Journal of Machine Learning Research, no. 5.\n\n\nBraschel, Melissa C, Ivana Svec, Gerarda A Darlington, and Allan Donner. 2016. ‚ÄúA Comparison of Confidence Interval Methods for the Intraclass Correlation Coefficient in Community-Based Cluster Randomization Trials with a Binary Outcome.‚Äù Clinical Trials 13 (2): 180‚Äì87. https://doi.org/10.1177/1740774515606377.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\n\nEugster, Manuel J A. 2011. ‚ÄúBenchmark Experiments ‚Äì a Tool for Analyzing Statistical Learning Algorithms.‚Äù PhD thesis, M√ºnchen: LMU M√ºnchen. https://edoc.ub.uni-muenchen.de/12990/1/Eugster_Manuel_J_A.pdf.\n\n\nGoldstein, Harvey. 2010. ‚ÄúBootstrapping in Multilevel Models.‚Äù In Handbook of Advanced Multilevel Analysis. Routledge.\n\n\nMarkatou, Marianthi, Hong Tian, Shameek Biswas, and George Hripcsak. 2005. ‚ÄúAnalysis of Variance of Cross-Validation Estimators of the Generalization Error.‚Äù Journal of Machine Learning Research 6 (39): 1127‚Äì68. http://jmlr.org/papers/v6/markatou05a.html.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2019. ‚ÄúUsing Simulation Studies to Evaluate Statistical Methods.‚Äù Statistics in Medicine 38 (11): 2074‚Äì2102. https://doi.org/10.1002/sim.8086.\n\n\nReed, James F. III. 2007. ‚ÄúBetter Binomial Confidence Intervals.‚Äù Journal of Modern Applied Statistical Methods 6 (May): 153‚Äì61. https://doi.org/10.56801/10.56801/v6.i.290.\n\n\nShalev-Shwartz, Shai, and Shai Ben-David. 2014. Understanding Machine Learning: From Theory to Algorithms. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9781107298019.\n\n\nVolovici, Victor, Nicholas L. Syn, Ari Ercole, Joseph J. Zhao, and Nan Liu. 2022. ‚ÄúSteps to Avoid Overuse and Misuse of Machine Learning in Clinical Research.‚Äù Nature Medicine 28 (10): 1996‚Äì99. https://doi.org/10.1038/s41591-022-01961-6."
  },
  {
    "objectID": "posts/flexcv/index.html",
    "href": "posts/flexcv/index.html",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "",
    "text": "flexcv is a Python package that implements flexible cross validation and machine learning for tabular data. It provides a range of features for comparing machine learning models on different datasets with different sets of predictors, customizing just about everything around cross validations. It supports both fixed and random effects, as well as random slopes.\nflexcv originated from the need to perform nested cross validation on grouped data. While I wrote most of the code during my time as a student research assistant at Hochschule D√ºsseldorf, I have continued to maintain and improve the package as a private open-source-project. Moreover, I decided to release it with full documentation, project page on GitHub pages, and continuous integration and deployment using GitHub Actions. Have a look at the project page!\nThe package is designed to be flexible and easy to use, with a focus on reproducibility and ease of use. It is built on top of popular machine learning libraries such as scikit-learn and pandas, and is designed to work seamlessly with these libraries.\nWhy would you need it? You can not simply perform a nested group cross validation with scikit-learn. This is where flexcv comes in. It provides a simple and flexible interface for performing nested cross validation on grouped data, and supports a wide range of machine learning models and evaluation metrics. It does a whole lot of other stuff, too, like scaling in the inner and outer CV independently and providing compatibility to neptune.ai for logging.\nThis project showcases my"
  },
  {
    "objectID": "posts/flexcv/index.html#poster",
    "href": "posts/flexcv/index.html#poster",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Poster",
    "text": "Poster\n\n\nThis conference poster was a contribution to the Jahrestagung f√ºr Akustik DAGA 2024 and is published here."
  },
  {
    "objectID": "posts/flexcv/index.html#installation",
    "href": "posts/flexcv/index.html#installation",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Installation",
    "text": "Installation\nflexcv is on PyPI, so you can install it using pip:\npip install flexcv"
  },
  {
    "objectID": "posts/flexcv/index.html#usage",
    "href": "posts/flexcv/index.html#usage",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Usage",
    "text": "Usage\nHere is an example of how to use flexcv to perform nested cross validation on a regression problem. Let‚Äôs first load the modules and generate some sample data:\n\n# import the class interface, data generator and model\nfrom flexcv import CrossValidation\nfrom flexcv.synthesizer import generate_regression\nfrom flexcv.models import LinearModel\n\n# make sample data\nX, y, group, _ = generate_regression(10, 100, noise_level=0.01)\n\nNow the fun part about flexcv is its class interface. You can set up a complex configuration with just a few lines of code. Here is an example of how to set up a group cross validation with a linear model:\n\n# instantiate our cross validation class\ncv = CrossValidation()\n\n# now we can use method chaining to set up our configuration perform the cross validation\nresults = (\n    cv\n    .set_data(X, y, group, dataset_name=\"ExampleData\")\n    .set_splits(method_outer_split=\"GroupKFold\", method_inner_split=\"KFold\")\n    .add_model(LinearModel)\n    .perform()\n    .get_results()\n)\n\n# results has a summary property which returns a dataframe\n# we can simply call the pandas method \"to_excel\"\nresults.summary.to_excel(\"my_cv_results.xlsx\")\n\nI decided to use method chaining in the core interface, to make it easy to set up the configuration and perform the cross validation without having to remember a bunch of classes and functions. This approach leverages IDE hints and completion to guide the user through the process.\nVisit the project page to learn more. Feel free to reach out to me if you have any questions or suggestions, preferably using the issue tracker."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Forecasting Bicycle Traffic in Cologne\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\n45 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty quantification for cross-validation (Master Thesis Write-up)\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible cross validation and machine learning for regression on tabular data\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoundscape Visualization App - Explore participants in a diverse dataset!\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nMixed-illuminant whitebalance with GridNet\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n31 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Hiüëã, I‚Äôm Fabian, a data scientist bridging machine learning and engineering.\nWith a M.Sc. in Media Technology, I specialize in developing robust machine learning solutions that perform reliably across simple and critical applications. My research on uncertainty quantification and cross-validation has equipped me with deep insights into performance estimation and statistical computing. I love package development and open-sourced a Python package for complex cross-validation workflows.\nMy interdisciplinary background‚Äîspanning audio engineering and data science‚Äîenables me to approach complex challenges with a holistic, end-to-end perspective. I‚Äôm passionate about working in cross-functional teams and collaborating with domain experts to deliver impactful solutions."
  },
  {
    "objectID": "posts/bicycle/index.html",
    "href": "posts/bicycle/index.html",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "",
    "text": "A reliable forecast of the bicycle traffic is an important indicator for planning and optimizing the infrastructure of a city. In this post, I will predict the bicycle traffic in Cologne based on counting data from local bicycle counting stations.\nThis project was originally part of the course Machine Learning and Scientific Computing at TH K√∂ln and I worked on it in collaboration with my friend and collegue Felix Otte. While Felix supervised the model trainings and the evaluation of the models, I was responsible for data cleaning, exploration, and visualization. For this writeup, I performed re-runs of the original trainings and added a new class-based implementation of the whole pipeline that is closer to something useful in production. As a newly added feature, I also included an implementation of prediction intervals using the mapie library.\nThis projects showcases the following skills:\nThis post is reproducible. Just follow along and execute the code chunks locally."
  },
  {
    "objectID": "posts/bicycle/index.html#evaluation",
    "href": "posts/bicycle/index.html#evaluation",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we can evaluate the tuned pipeline of transformers and models on the evaluation data of the yet unseen year 2022.\n\n\nCode\nmetrics_df = ml_pipeline.eval(test_data)\n\n(\n    GT(metrics_df)\n    .tab_header(\"Metrics on Holdout Data\")\n    .cols_label(location=\"Location\", rmse=\"RMSE\", mape=\"MAPE\", count=\"Avg. Monthly Count\")\n    .cols_move(\"count\", after=\"location\")\n)\n\n\n\n\n\n\n\n\nMetrics on Holdout Data\n\n\nLocation\nAvg. Monthly Count\nRMSE\nMAPE\nbaseline\n\n\n\n\nHohe Pforte\n92102\n7805\n0.08\ntrue\n\n\nVorgebirgswall\n92718\n10730\n0.1\nfalse\n\n\nZ√ºlpicher Stra√üe\n148239\n20062\n0.11\nfalse\n\n\nNeumarkt\n126157\n15154\n0.11\nfalse\n\n\nA.-Silbermann-Weg\n83481\n11891\n0.11\nfalse\n\n\nNiederl√§nder Ufer\n72004\n9610\n0.12\nfalse\n\n\nVenloer Stra√üe\n167610\n25447\n0.14\nfalse\n\n\nHohenzollernbr√ºcke\n73095\n12079\n0.15\nfalse\n\n\nA.-Sch√ºtte-Allee\n61827\n12985\n0.16\nfalse\n\n\nNeusser Stra√üe\n98155\n17378\n0.16\nfalse\n\n\nStadtwald\n74314\n15140\n0.16\nfalse\n\n\nDeutzer Br√ºcke\n142528\n26873\n0.17\nfalse\n\n\nSeverinsbr√ºcke\n48116\n10969\n0.19\nfalse\n\n\nBonner Stra√üe\n79392\n22168\n0.22\nfalse\n\n\nUniversit√§tsstra√üe\n133047\n35502\n0.23\nfalse\n\n\nRodenkirchener Br√ºcke\n52069\n14300\n0.27\nfalse\n\n\nVorgebirgspark\n31012\n9477\n0.27\nfalse\n\n\n\n\n\n\n        \n\n\n\nPlot the Predictions vs Actual Counts\nFirst, let‚Äôs start with some location that did really bad:\n\n\nCode\nml_pipeline.plot_predictions(test_data, \"Bonner Stra√üe\")\nml_pipeline.plot_predictions(test_data, \"Severinsbr√ºcke\")\n\n\n                                                \n\n\n                                                \n\n\nWhile our model for Severinsbr√ºcke seems to show the seasonal pattern, it is not able to predict the peaks with the correct magnitude. The model for Bonner Stra√üe is not able to capture the trend at all. Let‚Äôs have a look at the best location, on the other hand:\n\n\nCode\nml_pipeline.plot_predictions(test_data, \"Z√ºlpicher Stra√üe\")\nml_pipeline.plot_predictions(test_data, \"Hohe Pforte\")\nml_pipeline.plot_predictions(test_data, \"Venloer Stra√üe\")"
  },
  {
    "objectID": "posts/bicycle/index.html#extension-prediction-intervals",
    "href": "posts/bicycle/index.html#extension-prediction-intervals",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "Extension: Prediction Intervals",
    "text": "Extension: Prediction Intervals\nTo quantify the uncertainty in our predictions, we can use conformal prediction intervals. As a formal definition for general regression problems let‚Äôs assume that our training data \\((X,Y)=[(x_1,y_1),\\dots,(x_n,y_n)]\\) is drawn i.i.d. from an unknown \\(P_{X,Y}\\).Then the regression task is defined by \\(Y=\\mu(X)+\\epsilon\\) with our model \\(\\mu\\) and the residuals \\(\\epsilon_i ~ P_{Y|X}\\). We can define a nominal coverage level for the prediction intervals \\(1-\\alpha\\). So we want to get an interval \\(\\hat{C}_{n,a}\\) for each new prediction on the unseen feature vector \\(X_{n+1}\\) such that $\\(P[Y_{n+1}\\in \\hat{C}_{n,a}(X_{n+1})]\\ge 1-\\alpha\\) . We will use the MAPIE package to get performance intervals that are model agnostic. This is great, because we can plug in our existing pipeline and get the intervals with just a bit of wrapping. Now, there are different approaches to constructing prediction intervals. But since we are dealing with a timeseries, that violates the i.i.d assumption, we need to use ensemble batch prediction intervals (EnbPI). Read more about that here.\nWe will define a new class QuantileModels that inherits from BicyclePredictor and extends it with the necessary methods to fit the MapieTimeSeriesRegressorand predict the intervals. We will use the BlockBootstrap method to resample the data and get the intervals. We will also define a method to plot the intervals.\nThis code combines some of the examples here and here.\n\n\nCode\nfrom mapie.regression import MapieRegressor, MapieTimeSeriesRegressor\nfrom mapie.metrics import (regression_coverage_score,\n                           regression_mean_width_score)\nfrom mapie.subsample import BlockBootstrap\n\nclass QuantileModels(BicyclePredictor):\n    def _fit_mapie(self, train_data, location):\n        X_train = train_data.drop(\"count\").filter(train_data[\"location\"] == location).to_pandas()\n        y_train = train_data.filter(train_data[\"location\"] == location)[\"count\"]\n\n        pipe = make_pipeline(\n                make_feature_transformer(X_train),\n                XGBRegressor(\n                    **self.succesful_studies[location],\n                    **self.fixed_params\n                )\n            )\n\n        cv_mapietimeseries = BlockBootstrap(\n            n_resamplings=100, n_blocks=3, overlapping=False, random_state=42\n        )\n        mapie = MapieTimeSeriesRegressor(\n            pipe,\n            method=\"enbpi\",\n            cv=cv_mapietimeseries,\n            agg_function=\"mean\",\n            n_jobs=-1,\n        )\n        return mapie.fit(X_train, y_train)\n\n    def fit_mapie(self, train_data):\n        self._baseline_mapie = self._fit_mapie(train_data, location=self.best_location)\n\n        self.mapie_models = {}\n        for location in self.succesful_studies.keys():\n            self.mapie_models[location] = self._fit_mapie(train_data, location=location)\n\n        self.is_mapie_fitted = True\n\n    def check_mapie_fitted(self):\n        if not hasattr(self, \"is_mapie_fitted\") or not self.is_mapie_fitted:\n            raise ValueError(\"Model has not been fitted yet.\")\n        return True\n\n    def predict_mapie(self, X_test, loc_str=\"\", alpha=0.05):\n        self.check_mapie_fitted()\n\n        mapie = self.mapie_models.get(loc_str, self._baseline_mapie)\n        y_pred, y_pis = mapie.predict(X_test, alpha=alpha)\n\n        return y_pred, y_pis\n\n    def print_mapie_metrics(self, y_test, y_pis):\n        coverage = regression_coverage_score(y_test, y_pis[:, 0, 0], y_pis[:, 1, 0])\n        width = regression_mean_width_score(y_pis[:, 0, 0], y_pis[:, 1, 0])\n        print(\n            \"Coverage and prediction interval width mean for CV+: \"\n            f\"{coverage:.3f}, {width:.3f}\"\n        )\n\n    def plot_mapie(self, data, loc_str, alpha=0.05):\n        import plotly.graph_objects as go\n        self.check_mapie_fitted()\n        data = test_data\n\n        X_test = data.filter(data[\"location\"] == loc_str).drop(\"count\").to_pandas()\n        y_test = data.filter(data[\"location\"] == loc_str)[\"count\"].to_pandas()\n        y_pred, y_pis = self.predict_mapie(X_test, loc_str=loc_str, alpha=alpha)\n\n        fig = go.Figure([\n            go.Scatter(\n                name=f'{(1 - alpha)*100}% PI',\n                x=X_test['date'],\n                y=y_pis[:, 1, 0],\n                mode='lines',\n                marker=dict(color=\"#444\"),\n                line=dict(width=0),\n                showlegend=False\n            ),\n            go.Scatter(\n                name=f'{(1 - alpha)*100}% PI',\n                x=X_test['date'],\n                y=y_pis[:, 0, 0],\n                marker=dict(color=\"#444\"),\n                line=dict(width=0),\n                mode='lines',\n                fillcolor='rgba(68, 68, 68, 0.3)',\n                fill='tonexty',\n                showlegend=True\n            ),\n            go.Scatter(\n                x=X_test[\"date\"],\n                y=y_test,\n                name=\"Actual\",\n                line=dict(color='rgb(31, 119, 180)'),\n                mode='lines'\n            ),\n            go.Scatter(\n                x=X_test[\"date\"],\n                y=y_pred,\n                name=\"Predicted\",\n                line=dict(color='rgb(255, 127, 14)'),\n                mode='lines'\n            )\n        ])\n        fig.update_layout(\n            title=dict(text=f\"Prediction Intervals for {loc_str}\"),\n            hovermode=\"x\",\n            template=\"simple_white\"\n            )\n        fig.update_xaxes(title_text='Date')\n        fig.update_yaxes(title_text='Count')\n        fig.show()\n\n\nAs some of the attributes in our previous class instance are needed in the new instance of QuantileModels, we can simply copy them over. We will then fit the model and predict the intervals for the best location.\nFirst, we will look at one location, that show reasonably good predictions, and then at one location that shows bad predictions.\n\n\nCode\nquant_pipeline = QuantileModels()\nquant_pipeline.set_fixed_params({\n    \"n_estimators\": 2000,\n    \"enable_categorical\": True,\n    \"random_state\": 42\n})\nloc_str = \"Venloer Stra√üe\"\nquant_pipeline.best_location = loc_str\nquant_pipeline.succesful_studies = ml_pipeline.succesful_studies\nquant_pipeline.refit(train_data)\nquant_pipeline.fit_mapie(train_data)\n\nquant_pipeline.plot_mapie(test_data, loc_str, alpha=0.05)\nloc_str = \"Bonner Stra√üe\"\nquant_pipeline.plot_mapie(test_data, loc_str, alpha=0.05)\n\n\n                                                \n\n\n                                                \n\n\nWe now get a very clear idea of the uncertainty in our predictions. The prediction intervals for ‚ÄúBonner Stra√üe‚Äù indicate, that the model actually has no idea what to do. In order to predict the counts for this location, we would need more data or a different model. The prediction intervals for ‚ÄúVenloer Stra√üe‚Äù on the other hand, show that the model is quite confident in its predictions and always include the actual counts."
  },
  {
    "objectID": "posts/bicycle/index.html#extension-inference-using-a-weather-forecast-api",
    "href": "posts/bicycle/index.html#extension-inference-using-a-weather-forecast-api",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "Extension: Inference using a weather forecast API",
    "text": "Extension: Inference using a weather forecast API\nIn order to use our models in a real-time production setting, we need to think of which features can be available at inference. As the bicycle counter data is quantized in time, we can not use this data to predict, e.g., the bicycle traffic of the next hour. This is clearly a limitation of the original project idea and data set. Moreover, our prediction pipeline is based on knowing the weather data.\nHere is a example idea, how we can get the weather data from the tomorrow.io API. You can get an API key with a free account. We will store the key in a .env file and use the python-dotenv package to load it. Don‚Äôt forget to commit .env to your .gitignore file. Let‚Äôs look at one time point from the forecast API.\n\n\nCode\nimport os\nfrom pprint import pprint\nimport dotenv\nimport requests\nimport polars as pl\nfrom pathlib import Path\n\ndotenv.load_dotenv(Path(\".env\"))\ntomorrow_api_key = os.getenv('TOMORROW_API_KEY')\n\nlocation = '50.938361,6.959974'\nforecast_api = \"https://api.tomorrow.io/v4/weather/forecast\"\n\nparams = {\n    \"location\": location,\n    \"apikey\": tomorrow_api_key\n}\n\nheaders = {\"accept\": \"application/json\"}\n\nr = requests.get(forecast_api, params=params, headers=headers)\nr_data = r.json()\n\n\nSince the tomorrow.io API uses different feature names, we have to translate it, to be compatible with our data. However, a few columns will be missing. We can impute them by taking the mean of the training data.\n\n\nCode\ninference_data = (\n    pl.DataFrame(r_data[\"timelines\"][\"daily\"][0][\"values\"])\n    .with_columns(\n        pl.lit(\"Venloer Stra√üe\").alias(\"location\"),\n        pl.col(\"temperatureMax\").alias(\"air_temp_daymax_month_max\"),\n        pl.col(\"temperatureMax\").alias(\"air_temp_daymax_month_mean\"),\n        pl.col(\"temperatureMin\").alias(\"air_temp_daymin_month_min\"),\n        pl.col(\"temperatureMin\").alias(\"air_temp_daymin_month_mean\"),\n        pl.col(\"temperatureAvg\").alias(\"air_temp_daymean_month_mean\"),\n        pl.col(\"rainAccumulationMax\").alias(\"precipitation_daymax_month_max\"),\n        pl.col(\"windSpeedMax\").alias(\"wind_speed_daymax_month_max\"),\n        pl.col(\"windSpeedAvg\").alias(\"wind_speed_month_mean\"),\n        pl.col(\"cloudCoverAvg\").alias(\"sky_cov\"),\n        date = pl.lit(datetime.fromisoformat(r_data[\"timelines\"][\"daily\"][0][\"time\"])).cast(pl.Date),\n        sunshine_duration = train_data[\"sunshine_duration\"].mean(),\n        precipitation_month_sum = train_data[\"precipitation_month_sum\"].mean(),\n    )\n)\nloc_str = \"Venloer Stra√üe\"\ny_pred, y_pis = quant_pipeline.predict_mapie(inference_data.to_pandas(), loc_str=loc_str, alpha=0.05)\n\nGT(inference_data)\nGT(\n    pl.DataFrame(\n        {\n            \"Location\": loc_str,\n            \"Date\": inference_data[\"date\"],\n            \"Predicted count\": np.round(y_pred / 30, 0).astype(int).item(),\n            \"95% PI\": f\"[{np.round(y_pis[:, 0, 0] / 30, 0).astype(int).item()}, {np.round(y_pis[:, 1, 0] / 30, 0).astype(int).item()}]\"\n        }\n    )\n    \n).tab_header(\"Inference Results\")\n\n\n\n\n\n\n\n\nInference Results\n\n\nLocation\nDate\nPredicted count\n95% PI\n\n\n\n\nVenloer Stra√üe\n2024-12-06\n3880\n[2394, 5363]\n\n\n\n\n\n\n        \n\n\nAnd there you have it: a real-time extension for our ML-pipeline that we now can deploy!"
  },
  {
    "objectID": "posts/soundscape-viz/index.html",
    "href": "posts/soundscape-viz/index.html",
    "title": "Soundscape Visualization App - Explore participants in a diverse dataset!",
    "section": "",
    "text": "Visualizing abstract data helps get an intuitive feel for the raw data that sometimes goes beyond summary statistics. In this post, I show how to visualize the D√ºsseldorf soundscape data set using Bokeh. Explore the app here!\n\n\n\nRadar charts of static person information.\n\n\nThis project was part of the Data Visialization course at Technische Hochschule K√∂ln. The goal was to create an interactive visualization of the soundscape data set, that I helped collect during my time as a student research assistant at Hochschule D√ºsseldorf.\n\nData Set\nThe data set is published on Zenodo. It contains 6000+ soundscape rating from 100+ participants. The participants were asked to record and rate their soundscapes inside their private dwellings. Ther recordings are encoded to time-series of acoustic features and spectrograms to ensure privacy. However, the ratings, personal information, and situational context are included in the data set, which makes it a rich source for acoustic research.\n\n\nBokeh App\nYou can access the interactive app at the Posit Cloud. It allows you to explore the data set on an individual level.\nThe challenge with this app was, that I wanted to allow browsing through participants. We have to update the plots quickly, so we have to run a Bokeh server in the background. This makes deployment a bit more complicated, but the Posit Connect Cloud makes it easy to deploy Bokeh apps directly from the GitHub repository.\nFirst, I wanted to find a way to visualize the different personalities. I used a cluster of radar charts to show the personal factors in three fields: wellbeing, noise sensitivity, and trait. You can see an example of this at the top of this blog post.\nSecondly, person related meta data is visualized for example in the following bar plot to show composition of soundscape categories.\n\n\n\nBar charts of soundscape composition for a single participant.\n\n\nThe other part of the app visualizes the soundscape ratings directly as scatter plots. In the following example, we plot the ratings of personal valence against the ratings of personal arousal.\n\n\n\nScatter plot of personal arousal vs.¬†valence at the time of soundscape ratings.\n\n\nThis participant rated themself as either very aroused or very calm, but never in between. On the other hand, the valence ratings are more evenly distributed. Also, we can see a cluster of points in the top right corner, which indicates that the participant rated themself as very aroused and very happy at the same time.\n\n\nProject Report\nThe full project report is published at researchgate.net."
  },
  {
    "objectID": "posts/thesis/tables/results_table_1st_full.html",
    "href": "posts/thesis/tables/results_table_1st_full.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Method\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ¬± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000"
  },
  {
    "objectID": "posts/whitebalance/index.html",
    "href": "posts/whitebalance/index.html",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "",
    "text": "In this project I worked on benchmarking a Deep Learning approach to mixed-illuminant whitebalance in comparison with analytical methods based on images from a polarization camera. In a 5 person team of media technology students at TH K√∂ln, I was responsible for deploying a pre-trained Deep Learning model to our custom camera preprocessing pipeline, that was uniquely designed for the specific camera hardware.\nThis project showcases my abilities to"
  },
  {
    "objectID": "posts/whitebalance/index.html#the-deep-learning-part",
    "href": "posts/whitebalance/index.html#the-deep-learning-part",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "The Deep Learning part",
    "text": "The Deep Learning part\nFor the whitebalance predictions, we used the Deep Neural Network from Mahmoud Afifi. I wrote a custom class, that holds this model and can be used in our node-based image pipeline. This pipeline is written in Python and uses OpenCV for image processing. It is used to preprocess the raw images from the camera, apply a color profile, and to apply the whitebalance correction to the images.\nThe DNN has learned to generate mappings for 5 predefined white balance settings, that are commonly used in photography. That makes it possible to use the net in a modified camera. The camera has to render every image with 5 predefined white balance settings no matter what the scene actual demands. The network then creates mappings to correct the white balance in post-processing. When learning about this architecture and how the authors trained it, I was really buffled how they instrumentalized the loss function to achieve visually pleasing results. The overall loss function is defined as \\(\\mathcal{L}=\\mathcal{L}_r + \\lambda \\mathcal{L}_s\\). \\(\\mathcal{L}_r\\) is the following, relatively simple, reconstruction loss: \\[\n\\mathcal{L}_r=||P_{corr}-\\sum_i \\hat{W}_i \\odot P_{c_i} ||_F^2\n\\]\nIn this loss function, \\(||\\cdot||_F^2\\) computes the squared Frobenius norm, whereas \\(\\odot\\) is the Hadamard product. \\(P_{corr}\\) and \\(P_{c_i}\\) are extracted training patches from the ground truth sRGB images and input sRGB images rendered with the \\(C_i\\) WB setting, respectively. \\(W_i\\) is \\(i\\)-th blending weighting map as generated by the network for \\(P_{c_i}\\).\nTo further improve the results, additional steps have been taken by Afifi et al.: Firstly a cross-channel softmax operator has been applied before loss calculation in order to avoid out-of-gamut-colors. In this step, the exponential function is applied to each element and the output is then normalized by dividing by the sum of all new values. Secondly, a regularization term is introduced to the loss function. Hereby, GridNet is trained to produce rather smoothed weighting maps opposed to perfectly accurate maps. This may be due to reasons of generalization as well as visual observation by the researchers. The regularization is applied with \\[\n\\mathcal{L}_s = \\sum_i||\\hat{W}_i\\ast \\bigtriangledown _x||_F^2 + || \\hat{W}_i \\ast \\bigtriangledown _y ||_F^2\n\\]\nwith \\(\\bigtriangledown_x\\) and \\(\\bigtriangledown_y\\) \\(3\\times3\\) horizontal and vertical (edge detecting) Sobel filters and \\(\\ast\\) being the convolution operator. When all these terms are put together in \\(\\mathcal{L}=\\mathcal{L}_r + \\lambda \\mathcal{L}_s\\). \\(\\mathcal{L}_r\\), the contribution of the regularization/edge-preserving term is controlled by the hyperparameter \\(\\lambda\\). I think it‚Äôs really fascinating, how the authors applied image filtering techniques to the weighting maps to achieve a better generalization and visually improved outputs of the network.\n\n\n\nArchitecture of the GridNet deep neural network as used with six columns and four rows in the MixedIll WB method by Afifi, Brubaker, and Brown (2022). Blue row units are residual units, green column units are convolutional downscaling units (i.e.¬†reducing the dimensions of each feature received from the upper row reduced by two, while the number of output channels is duplicated), whereas orange column units are deconvolutional bilinear upscaling units (increasing the dimensions of the received features by two in the last three columns)."
  },
  {
    "objectID": "posts/whitebalance/index.html#benchmarking-experiments",
    "href": "posts/whitebalance/index.html#benchmarking-experiments",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "Benchmarking experiments",
    "text": "Benchmarking experiments\nNow, to compare this Deep Learning approach with the analytical methods, we produced a unique evaluation data set, that is super hard to white balance. We photographed images with extreme mixed illuminant scenarios where two light sources of opposing color temperatures were lighting the scene or sometimes were even visible in the image. We then compared the results of the Deep Learning approach with the analytical methods based on the polarization camera images by employing error metrices.\nLet‚Äôs see how the DNN works in one example of our benchmarking data set:\n\n\n\nComparison of ground truth (left) and final corrected image with the MixedIllWB method by Afifi, Brubaker, and Brown (2022) (right). The corrected image was generated by applying the weighting maps (bottom row) to the fixed WB images. The scene was illuminated with Skypanels set to \\(5928\\) K and \\(2768\\) K, respectively.\n\n\nWe can also inspect, what the DNN did under the hood by plotting the weighting maps and the Hadamard products of the weighting maps and the pre-rendered WB images:\n\n\n\nWeighting maps \\(W_i\\) (top row) and Hadamard products of \\(W_i\\) and pre-rendered WB images \\(P_{c_i}\\) (bottom row) for the corresponding WB settings (columns) for MIPo image A0023.\n\n\nTo get the full picture of how well the approach works on our custom evaluation data set, we calculated the \\(\\Delta E_{00}\\) error metric for the corrected images against the ground truth images. The \\(\\Delta E_{00}\\) metric is a color difference metric that is widely used in the industry to evaluate the color difference between two images. The lower the \\(\\Delta E_{00}\\) value, the better the color match between the two images. The Afifi approach is the one on the left; all other approaches are not deep learning based.\n\n\n\nMixed illuminants ‚Äì \\(\\Delta E_{00}\\): Boxplots of the \\(\\Delta E_{00}\\) metric for white balancing methods against ground truth for corrected images of the MIPo dataset. Boxplot properties: The horizontal line inside the box is the median value. The height of the box is the interquartile range (IQR) between lower and upper quartile. The whiskers mark the minimum and maximum datapoints within the range of \\(1.5*\\text{IQR}\\) from the nearest (lower/upper) quartile. Outliers are datapoints exceeding this definition and are marked with diamond symbols."
  },
  {
    "objectID": "posts/whitebalance/index.html#code",
    "href": "posts/whitebalance/index.html#code",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "Code",
    "text": "Code\nHere is my code, that I wrote for the custom class AfifiRenderer. The first part defines two custom dictionary classes, that prevent us from using the model in a wrong way.\n\n\nCode\nclass CameraDict(Dict[str, Camera]):\n    \"\"\"Custom dict class to ensure keys and values are valid to work in AfifiRenderer.\n    Keys are strings denoting the manual white balance setting.\n    Values are Camera objects.\n\n    Args:\n        Dict (_type_): Dict class we inherit from.\n\n    Raises:\n        TypeError: If key is not a str,\n        ValueError: If key is not in ['D', 'S', 'T', 'F', 'C'].\n        TypeError: If value is not of class Camera.\n    \"\"\"\n\n    ALLOWED_KEYS = [\"D\", \"S\", \"T\", \"F\", \"C\"]\n\n    def __setitem__(self, key, value):\n        if not isinstance(key, str):\n            raise TypeError(\"Keys must be strings\")\n        if key not in self.ALLOWED_KEYS:\n            raise ValueError(\"Key must be one of: \" + str(self.ALLOWED_KEYS))\n        if not isinstance(value, Camera):\n            raise TypeError(\"Values must be instances of the Camera class\")\n        super().__setitem__(key, value)\n\n\nclass ImageDict(Dict[str, np.ndarray]):\n    \"\"\"Custom dictionary to ensure keys and values are valid to work in AfifiRenderer.\n    Keys are strings denoting a manual white balance setting.\n    Values are images of type np.array.\n\n    Args:\n        Dict (_type_): Base class we inherit from.\n\n    Raises:\n        TypeError: If key is not a str,\n        ValueError: If key is not in ['D', 'S', 'T', 'F', 'C'].\n        TypeError: If value is not a numpy array.\n    \"\"\"\n\n    ALLOWED_KEYS = [\"D\", \"S\", \"T\", \"F\", \"C\"]\n\n    def __setitem__(self, key, value):\n        if not isinstance(key, str):\n            raise TypeError(\"Keys must be strings\")\n        if key not in self.ALLOWED_KEYS:\n            raise ValueError(\"Key must be one of: \" + str(self.ALLOWED_KEYS))\n        if not isinstance(value, np.ndarray):\n            raise TypeError(\"Values must be a numpy array.\")\n        super().__setitem__(key, value)\n\n\ndef array_to_image(img: np.ndarray) -&gt; Image:\n    return fromarray((np.clip(img, 0, 1) * (2**8 - 1)).astype(np.uint8))\n\n\ndef downsize_frame(img: np.ndarray, width=248, height=248) -&gt; np.ndarray:\n    return cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n\n\ndef move_weight_files(string_to_move: str = \"weight\"):\n    # Folder admin\n    move_from = Path(\"wb_algos\", \"Afifi\", \"images\", \"test_net_out\")\n    move_to = Path(\"wb_algos\", \"Afifi\", \"images\", \"test_net_out\", \"weights\")\n    # List the files in a directory\n    filenames = os.listdir(move_from)\n    # Remove the script's name from the list of files, if it is in there\n    try:\n        filenames.remove(os.path.basename(__file__))\n    except ValueError:\n        pass\n    # Move files from the input folder to the output folder\n    for filename in filenames:\n        if string_to_move in filename:\n            path_in = Path(move_from, filename)\n            path_out = Path(move_to, filename)\n            try:\n                shutil.move(path_in, path_out)\n            except PermissionError:\n                print(\n                    f\"An PermissionError occured moving {path_in} -&gt; {path_out}. Moving on.\"\n                )\n            except shutil.Error:\n                print(\n                    \"Error in shutil.move: I probably have tried to move a directory into itself.\"\n                )\n\n\nThe second part now defines the AfifiRenderer class, that holds the model and the camera pipeline. The render method applies the model to the images and the save method saves the results to disk.\n\n\nCode\nclass AfifiRenderer:\n    \"\"\"An AfifiRenderer instance renders a perfectly white-balanced image using Afifi's deep learning approach with 5 cameras.\n\n    Attributes:\n        camera_dict: A dictionary of five `Camera` objects, each with a preconfigured camera pipeline\n        with a manual white balance for a specific color temperature.\n\n    Methods:\n        __init__():\n            Instantiates the AfifiRenderer object with five `Camera` objects,\n            one for each of the white balance settings in `afifi_cct_map`.\n\n        instantiate_camera_with_nodes(wb_code: str) -&gt; Camera:\n            Instantiates a `Camera` object with a preconfigured camera pipeline\n            for manual white balance with a specific color temperature. This method is used in __init__.\n\n        render(img: np.array) -&gt; np.array:\n            Renders a perfectly white-balanced image using Afifi's deep learning approach with 5 cameras\n            and passes it to the data loader.\n\n            Args:\n                img: The raw input image as read in with `cv2.imread()`.\n\n            Returns:\n                The perfectly rendered Afifi output image.\n\n        get_weights() -&gt; ImageDict:\n            Returns the weighting maps of the Afifi net.\n\n            Returns:\n                A dictionary of weighting maps, one for each of the white balance settings in `afifi_cct_map`.\n    \"\"\"\n\n    img_dict: ImageDict\n    camera_dict: CameraDict\n\n    def __init__(self, downscale=True) -&gt; None:\n        \"\"\"\n        AfifiRenderer.__init__ instantiates 5 Camera objects\n        including necessary pipelines for manual WB with 5 cct values\n        taken from a dict according to Afifis paper.\n        afifi_cct_map = {\n                'D': {\n                    \"name\": \"daylight\",\n                    \"ext\": \"D_CS.png\",\n                    \"cct\": 5500},\n                'S': {\n                    \"name\": \"shade\",\n                    \"ext\": \"S_CS.png\",\n                    \"cct\": 7500\n                    },\n                'T': {\n                    \"name\": \"tungsten/incadescent\",\n                    \"ext\": \"T_CS.png\",\n                    \"cct\": 2850\n                    },\n                'F': {\n                    \"name\": \"fluorescent\",\n                    \"ext\": \"F_CS.png\",\n                    \"cct\": 3800\n                    },\n                'C': {\n                    \"name\": \"cloudy\",\n                    \"ext\": 'C_CS.png',\n                    \"cct\": 6500\n                    }\n            }\n        \"\"\"\n\n        self.camera_dict = CameraDict(\n            {\n                wb_code: self.instantiate_cameras(wb_code, downscale=downscale)\n                for wb_code in AFIFI_CCT_MAP.keys()\n            }\n        )\n        logger.debug(\n            f\"AfifiRenderer instantiated with {len(self.camera_dict)} camera objects.\"\n        )\n\n    def instantiate_cameras(self, wb_code: str, downscale=True) -&gt; Camera:\n        \"\"\"\n        Instantiates a Camera object with a preconfigured camera pipeline\n        for manual white balance with a specific color temperature/cct value.\n\n        Uses the following Nodes in the pipelines:\n            - Nodes.DebayeringNode,\n            - Nodes.ManualWhitebalanceNode,\n            - Nodes.ColorMatrixNode,\n            - Nodes.GammaCorrectionNode,\n            - Nodes.GainNode,\n            - Nodes.AveragePolarizationImagesNode\n\n        For images of \"S\", \"T\", \"F\" or \"C\" the image is downsampled to 384x384 pixels.\n\n        Default camera values:\n            - Gain = 0.0\n            - Bit depth = 12\n            - Edge Aware debayering mode\n\n        Args:\n            wb_code (str): White Balance code for Afifi. Must be either \"D\", \"S\", \"T\", \"F\" or \"C\".\n\n        Returns:\n            Camera: Preconfigured camera object with a ManualWhiteBalance Pipeline.\n        \"\"\"\n        self.downscale = downscale\n        camera = Camera()\n        camera.cct = AFIFI_CCT_MAP[wb_code][\"cct\"]\n        camera.gain = 1.0\n        camera.saveToDiskExtension = \".tiff\"\n        camera.saveToDiskName = f\"{AFIFI_CCT_MAP[wb_code]['ext']}\"\n        camera.bitdepth = 12\n        camera.debayerMethod = ColorConversionCode(\n            is_color=True, suffix=\"_EA\"\n        )  # Set debayering method to Edge Aware\n\n        # Daylight setting is passed in full size, all other images are downscaled\n        if wb_code == \"D\" or not downscale:\n            camera.makeNodes(\n                Nodes.DebayeringNode,\n                Nodes.AveragePolarizationImagesNode,\n                Nodes.ManualWhitebalanceNode,\n                Nodes.ColorMatrixNode,\n                Nodes.GammaCorrectionNode,\n                Nodes.GainNode,\n            )\n        else:\n            camera.makeNodes(\n                Nodes.DebayeringNode,\n                Nodes.AveragePolarizationImagesNode,\n                Nodes.DownscalingNode,\n                Nodes.ManualWhitebalanceNode,\n                Nodes.ColorMatrixNode,\n                Nodes.GammaCorrectionNode,\n                Nodes.GainNode,\n            )\n\n        return camera\n\n    def render(self, img: np.array, filename: str = \"\", neptune=False) -&gt; np.array:\n        \"\"\"AfifiRenderer.render will render the 5 camera pipelines and pass it to Afifis Dataloader.\n        The daylight setting is passed in full size, all other images are downsized.\n        Then Afifis approach will be applied in test mode/inference mode with the following default parameters:\n\n            - batch_size: int = 1\n            - norm: bool = False\n            - model_location: str = None\n            - wb_settings: list = field(default_factory=lambda: ['D', 'S', 'T', 'F', 'C'])\n            - save_weights: bool = True\n            - keep_aspect_ratio: bool = False\n            - multi_scale: bool = True\n            - post_process: bool = True\n            - tedir: str = os.path.join(os.getcwd(), \"wb_algos\", \"Afifi\",  \"images\", \"test_net_in\")\n            - outdir: str = os.path.join(os.getcwd(), \"wb_algos\", \"Afifi\",  \"images\", \"test_net_out\")\n            - gpu: int = 0\n            - t_size: int = 384 # 'Size before feeding images to the network. Typically, 128 or 256 give good results. If multi-scale is used, then 384 is recommended.'\n            - model_name: str = 'WB_model_p_128_D_S_T_F_C'\n\n        Args:\n            img (np.array): the raw input image as read in with cv2.imread().\n            downsize (bool): Flag to control if S, T, F and C images are rescaled before passing to net. Default is True.\n            neptune (bool): Flag to control if neptune logging is used. Default is False.\n\n        Returns:\n            np.array: The perfectly rendered Afifi output image.\n        \"\"\"\n        NEPTUNE_LOGGING = neptune\n        logger.debug(\"Preprocessing for Afifi: Render manual white balance settings.\")\n        self.img_dict = ImageDict(\n            {\n                key: self.camera_dict[key].pipeline.render(img)\n                for key in tqdm(AFIFI_CCT_MAP.keys(), desc=\"Preprocessing\")\n            }\n        )\n\n        # if 1:\n        #     return self.img_dict\n\n        if 0:\n            # Saving the dictionary to a file using pickle\n            with open(\"my_dict.pickle\", \"wb\") as handle:\n                pickle.dump(self.img_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        if 0:\n            # Loading the dictionary from the saved file using pickle\n            with open(\"my_dict.pickle\", \"rb\") as handle:\n                self.img_dict = pickle.load(handle)\n\n        logger.debug(\"Performing Mixed-Ill WB correction.\")\n\n        args = TestNetsArguments()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if device.type != \"cpu\":\n            torch.cuda.set_device(args.gpu)\n\n        logger.debug(f\"Using device {device}\")\n\n        net = wb_net.WBnet(\n            device=device, norm=args.norm, inchnls=3 * len(args.wb_settings)\n        )\n\n        model_path = os.path.join(\n            os.getcwd(),\n            \"wb_algos\",\n            \"Afifi\",\n            \"mixedillWB\",\n            \"models\",\n            args.model_name + \".pth\",\n        )\n        net.load_state_dict(torch.load(model_path, map_location=device))\n\n        logger.debug(f\"Model loaded from {model_path}\")\n\n        net.to(device=device)\n        net.eval()\n\n        if NEPTUNE_LOGGING:\n            log_neptune(f\"TestNetArgs/{filename}/model\", type(net).__name__)\n            log_neptune(f\"TestNetArgs/{filename}/model_path\", model_path)\n            log_neptune(f\"TestNetArgs/{filename}/args\", str(args.__dict__))\n            log_neptune(f\"TestNetArgs/{filename}/downscale\", str(self.downscale))\n\n        self.img, self.weights_list, self.hadamards = test_net_on_single_img(\n            net=net,\n            device=device,\n            input_files=self.img_dict,\n            batch_size=args.batch_size,\n            post_process=args.post_process,\n            keep_aspect_ratio=args.keep_aspect_ratio,\n            t_size=args.t_size,\n            multi_scale=args.multi_scale,\n            model_name=args.model_name,\n            save_weights=args.save_weights,\n            wb_settings=args.wb_settings,\n        )\n\n        return self.img\n\n    def get_weights(self) -&gt; ImageDict:\n        \"\"\"Method to get the weighting maps of the Afifi net.\n\n        Returns:\n            list[np.array]: List of weighting maps in order ['D', 'S', 'T', 'F', 'C'].\n        \"\"\"\n        return ImageDict(\n            {\n                key: value\n                for key, value in zip([\"D\", \"S\", \"T\", \"F\", \"C\"], self.weights_list)\n            }\n        )\n\n    def get_manual_wb_imgs(self) -&gt; ImageDict:\n        return self.img_dict\n\n    def save(\n        self,\n        filename: str = \"\",\n        save_weights: bool = True,\n        save_manual_wb_imgs: bool = True,\n        save_manual_wb_dump: bool = True,\n        save_hadamard_products: bool = True,\n    ):\n        \"\"\"Saves the perfectly rendered Afifi image to disk. File format is *.tiff.\n        Flags can be used to save the weight maps and the manual white balanced images as well.\n\n        Args:\n            savename_without_ext (str): Stem file name without extension.\n            \"_Afifi\" will be added automatically.\n\n        Args:\n            savename_without_ext (str): _description_\n            save_weights (bool): If True, weight maps will be saved. Default is True.\n            save_manual_wb_imgs (bool): If True, the manual white balanced images will be saved. Default is True.\n        \"\"\"\n\n        root_folder = Path(\".\")\n\n        img_folder = root_folder / \"AfifiCorrectedImages\"\n        os.makedirs(img_folder, exist_ok=True)\n        img_savepath = img_folder / f\"{filename}_Afifi_corrected.tiff\"\n        self.img.save(img_savepath, \"TIFF\")\n        log_neptune(f\"Corrected/{filename}_Afifi_corrected\", self.img)\n\n        mwb_folder = root_folder / \"AfifiManualWB\"\n        os.makedirs(mwb_folder, exist_ok=True)\n        if save_manual_wb_imgs:\n            for key, value in self.img_dict.items():\n                mwb_savepath = mwb_folder / f\"{filename}_MWB_{key}_.tiff\"\n                value = array_to_image(value)\n                value.save(mwb_savepath, \"TIFF\")\n                log_neptune(f\"ManualWB/{filename}/{filename}_MWB_{key}\", value)\n\n        if save_manual_wb_dump:\n            for key, value in self.camera_dict.items():\n                dump = value.getDump()\n                dump[\"Timestamp\"] = datetime.datetime.now().strftime(\n                    \"%d.%m.%Y %H:%M:%S\"\n                )\n\n                dump_savepath = mwb_folder / f\"{filename}_paramDump_{key}.json\"\n                dump = json.dumps(dump, indent=4, sort_keys=True)\n                with open(dump_savepath, \"w\") as f:\n                    f.write(dump)\n                log_neptune(f\"CamPipeParam/{filename}/{filename}_paramDump_{key}\", dump)\n\n        if save_weights:\n            weight_folder = root_folder / \"AfifiWeights\"\n            os.makedirs(weight_folder, exist_ok=True)\n            if save_weights:\n                for weight, wb in zip(self.weights_list, [\"D\", \"S\", \"T\", \"F\", \"C\"]):\n                    weight_savename = f\"{filename}_Afifi_weight_{wb}.tiff\"\n                    weight_savepath = os.path.join(weight_folder, weight_savename)\n                    weight.save(weight_savepath, \"TIFF\")\n                    log_neptune(\n                        f\"Weights/{filename}/{filename}_Afifi_weight_{wb}\", weight\n                    )\n\n        if save_hadamard_products:\n            hadamard_folder = root_folder / \"AfifiHadamard\"\n            os.makedirs(hadamard_folder, exist_ok=True)\n            for hadamard_img, wb in zip(self.hadamards, [\"D\", \"S\", \"T\", \"F\", \"C\"]):\n                had_savename = f\"{filename}_Afifi_hadamard_{wb}.tiff\"\n                had_savepath = os.path.join(hadamard_folder, had_savename)\n                hadamard_img.save(had_savepath, \"TIFF\")\n                log_neptune(\n                    f\"Hadamard/{filename}/{filename}_Afifi_hadamard_{wb}\", hadamard_img\n                )\n\n\nHere is an example how this would be used:\n\n# Load the raw image that should be processed\nimg_raw = cv2.imread(\"images/230119_MlPo_multi_illuminant_polarization_dataset/A0021_0002.tiff\", -1)  # -1 for uint16\n\n# Create an AfifiRenderer instance\nafifi_renderer = AfifiRenderer()\n\n# Render the image\nimg = afifi_renderer.render(img_raw)\n\n# Save the image and set flags to save the intermediate images and meta data\nafifi_renderer.save(\n    \"corrected_img\",\n    save_weights=True,\n    save_manual_wb_imgs=True,\n    save_manual_wb_dump=True,\n    save_hadamard_products=True,\n    )"
  }
]
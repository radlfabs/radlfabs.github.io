[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "I’m Fabian Rosenthal, a Data Scientist and developer. With my engineering background, I bring a cross-disciplinary perspective to data science. I am specialized in performance estimation and uncertainty quantification. In my work and university projects, I have proven the ability to identify the right tool for a problem, communicate it to the team, master it, and ship it quickly.\n\n\n\n\n\n\n\n\n\nLanguages\n\n\n\nEnglish (C1)\nGerman (native)\n\n\n\n\n\nSoft skills\n\n\n\nMeticulous\nVersatile\nEmpathetic\nCreative"
  },
  {
    "objectID": "resume.html#profile",
    "href": "resume.html#profile",
    "title": "Resume",
    "section": "",
    "text": "I’m Fabian Rosenthal, a Data Scientist and developer. With my engineering background, I bring a cross-disciplinary perspective to data science. I am specialized in performance estimation and uncertainty quantification. In my work and university projects, I have proven the ability to identify the right tool for a problem, communicate it to the team, master it, and ship it quickly.\n\n\n\n\n\n\n\n\n\nLanguages\n\n\n\nEnglish (C1)\nGerman (native)\n\n\n\n\n\nSoft skills\n\n\n\nMeticulous\nVersatile\nEmpathetic\nCreative"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\nProgramming\n\n\n\nPython, R, SQL\nLaTeX, Matlab\nRust (learning)\n\n\n\n\n\nFrameworks\n\n\n\nPyTorch, scikit-learn\nPandas, Polars, Arrow\nmlr3, data.table\ntidyverse, tidymodels\nBokeh, Shap, ggplot2\n\n\n\n\n\nTools\n\n\n\nQuarto, Tableau\nGit, Docker, Linux CLI, Parquet"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\n\n\n\n\n\n\nStudent Research Assistant\nFraunhofer MEVIS (Bremen) | 02/2024 – 08/2024\n\n\n\nDeveloped statistical R-packages with uncertainty quantification techniques, giving clinicians a more robust understanding of ML models.\nDesigned unified APIs with consistent code styling and good unit tests.\n\n\n\n\n\n\n\n\n\n\nStudent Research Assistant\nUniversity of Applied Sciences Düsseldorf | 09/2020 – 10/2023\n\n\n\nCo-published a data set with soundscapes from 100+ participants.\nPerformed data and feature engineering on 6000+ audio recordings.\nSaved days of compute by employing a faster hyperparameter tuning.\nIntroduced a solution to keep track of 1000+ experiments.\nKeynotes and posters on the use of stats & ML in acoustic research.\n\n\n\n\n\n\n\n\n\n\nVideo Recording Producer\nWDR (Köln) | 09/2020 – 02/2022\n\n\n\nCalled cameras for live-streaming concerts in a musically sensible way.\nLed technical teams and communicated cinematographic ideas efficiently.\n\n\n\n\n\n\n\n\n\n\nBike Messenger\nSelf-employed (Düsseldorf) | 11/2016 – 09/2020\n\n\nCompleted time-critical and valuable deliveries in all weather conditions while optimizing complex urban routes in real-time."
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Resume",
    "section": "Projects",
    "text": "Projects\n\nflexcv\nUniversity of Applied Sciences Düsseldorf & Private Project\n\nA Python package simplifying nested cross-validation on grouped data.\nEnables reproducible multimodel workflows with few lines of code.\nReleased and maintained as OSS using GitHub Actions (CI/CD).\nProject page\n\n\n\nTime Series Forecasting\nUniversity of Applied Sciences Cologne\n\nPrediction of open bicycle counter data for Cologne and benchmarking of ML-algorithms with time series cross-validation.\nFine-tuned XGBoost with external weather data and seasonal trends beats the best linear models.\n\n\n\nGridNet White Balance\nUniversity of Applied Sciences Cologne\n\nAchieved improvement in color consistency metrics with GridNet.\nDeployed the model within a custom camera pipeline.\nStrong communication and co-operation in a 5-person team.\n\n\n\nSoundscape Viz App\nUniversity of Applied Sciences Cologne - Interactive app visualizing survey data on individual levels. - Enables intuitive exploration of the diversity of participants. - Deployed here on the Posit Connect Cloud."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\nMaster of Science: Media Technology\nUniversity of Applied Sciences Cologne | 2022 – 2024\n\nThesis: Comparison of uncertainty quantification techniques for empirical prediction performance based on cross-validation. Optimized distributed ML pipeline to train 4.3M+ models on the Fraunhofer Edge Cloud. A writeup is available here.\nCourses: Machine learning; Deep learning and object recognition; Data visualization; Multivariate statistics.\nFinal grade: 1.2\n\n\n\nBachelor of Engineering: Sound and Video\nRSH & University of Applied Sciences Düsseldorf | 2010 – 2022\n\nThesis: Audio feature extraction for predicting indoor soundscapes.\nFinal grade: 1.5"
  },
  {
    "objectID": "resume.html#other-accomplishments",
    "href": "resume.html#other-accomplishments",
    "title": "Resume",
    "section": "Other Accomplishments",
    "text": "Other Accomplishments\nOn the podium: Played solo horn in Bruckner’s 8th symphony with the Düsseldorf University Orchestra.\nOn two wheels: Cycled 300 km self-supported through Hessen."
  },
  {
    "objectID": "posts/whitebalance/index.html",
    "href": "posts/whitebalance/index.html",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "",
    "text": "In this project I worked on benchmarking a Deep Learning approach to mixed-illuminant whitebalance in comparison with analytical methods based on images from a polarization camera. In a 5 person team of media technology students at TH Köln, I was responsible for deploying a pre-trained Deep Learning model to our custom camera preprocessing pipeline, that was uniquely designed for the specific camera hardware.\nThis project showcases my abilities to"
  },
  {
    "objectID": "posts/whitebalance/index.html#the-deep-learning-part",
    "href": "posts/whitebalance/index.html#the-deep-learning-part",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "The Deep Learning part",
    "text": "The Deep Learning part\nFor the whitebalance predictions, we used the Deep Neural Network from Mahmoud Afifi. I wrote a custom class, that holds this model and can be used in our node-based image pipeline. This pipeline is written in Python and uses OpenCV for image processing. It is used to preprocess the raw images from the camera, apply a color profile, and to apply the whitebalance correction to the images.\nThe DNN has learned to generate mappings for 5 predefined white balance settings, that are commonly used in photography. That makes it possible to use the net in a modified camera. The camera has to render every image with 5 predefined white balance settings no matter what the scene actual demands. The network then creates mappings to correct the white balance in post-processing. When learning about this architecture and how the authors trained it, I was really buffled how they instrumentalized the loss function to achieve visually pleasing results. The overall loss function is defined as \\(\\mathcal{L}=\\mathcal{L}_r + \\lambda \\mathcal{L}_s\\). \\(\\mathcal{L}_r\\) is the following, relatively simple, reconstruction loss: \\[\n\\mathcal{L}_r=||P_{corr}-\\sum_i \\hat{W}_i \\odot P_{c_i} ||_F^2\n\\]\nIn this loss function, \\(||\\cdot||_F^2\\) computes the squared Frobenius norm, whereas \\(\\odot\\) is the Hadamard product. \\(P_{corr}\\) and \\(P_{c_i}\\) are extracted training patches from the ground truth sRGB images and input sRGB images rendered with the \\(C_i\\) WB setting, respectively. \\(W_i\\) is \\(i\\)-th blending weighting map as generated by the network for \\(P_{c_i}\\).\nTo further improve the results, additional steps have been taken by Afifi et al.: Firstly a cross-channel softmax operator has been applied before loss calculation in order to avoid out-of-gamut-colors. In this step, the exponential function is applied to each element and the output is then normalized by dividing by the sum of all new values. Secondly, a regularization term is introduced to the loss function. Hereby, GridNet is trained to produce rather smoothed weighting maps opposed to perfectly accurate maps. This may be due to reasons of generalization as well as visual observation by the researchers. The regularization is applied with \\[\n\\mathcal{L}_s = \\sum_i||\\hat{W}_i\\ast \\bigtriangledown _x||_F^2 + || \\hat{W}_i \\ast \\bigtriangledown _y ||_F^2\n\\]\nwith \\(\\bigtriangledown_x\\) and \\(\\bigtriangledown_y\\) \\(3\\times3\\) horizontal and vertical (edge detecting) Sobel filters and \\(\\ast\\) being the convolution operator. When all these terms are put together in \\(\\mathcal{L}=\\mathcal{L}_r + \\lambda \\mathcal{L}_s\\). \\(\\mathcal{L}_r\\), the contribution of the regularization/edge-preserving term is controlled by the hyperparameter \\(\\lambda\\). I think it’s really fascinating, how the authors applied image filtering techniques to the weighting maps to achieve a better generalization and visually improved outputs of the network.\n\n\n\nArchitecture of the GridNet deep neural network as used with six columns and four rows in the MixedIll WB method by Afifi, Brubaker, and Brown (2022). Blue row units are residual units, green column units are convolutional downscaling units (i.e. reducing the dimensions of each feature received from the upper row reduced by two, while the number of output channels is duplicated), whereas orange column units are deconvolutional bilinear upscaling units (increasing the dimensions of the received features by two in the last three columns)."
  },
  {
    "objectID": "posts/whitebalance/index.html#benchmarking-experiments",
    "href": "posts/whitebalance/index.html#benchmarking-experiments",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "Benchmarking experiments",
    "text": "Benchmarking experiments\nNow, to compare this Deep Learning approach with the analytical methods, we produced a unique evaluation data set, that is super hard to white balance. We photographed images with extreme mixed illuminant scenarios where two light sources of opposing color temperatures were lighting the scene or sometimes were even visible in the image. We then compared the results of the Deep Learning approach with the analytical methods based on the polarization camera images by employing error metrices.\nLet’s see how the DNN works in one example of our benchmarking data set:\n\n\n\nComparison of ground truth (left) and final corrected image with the MixedIllWB method by Afifi, Brubaker, and Brown (2022) (right). The corrected image was generated by applying the weighting maps (bottom row) to the fixed WB images. The scene was illuminated with Skypanels set to \\(5928\\) K and \\(2768\\) K, respectively.\n\n\nWe can also inspect, what the DNN did under the hood by plotting the weighting maps and the Hadamard products of the weighting maps and the pre-rendered WB images:\n\n\n\nWeighting maps \\(W_i\\) (top row) and Hadamard products of \\(W_i\\) and pre-rendered WB images \\(P_{c_i}\\) (bottom row) for the corresponding WB settings (columns) for MIPo image A0023.\n\n\nTo get the full picture of how well the approach works on our custom evaluation data set, we calculated the \\(\\Delta E_{00}\\) error metric for the corrected images against the ground truth images. The \\(\\Delta E_{00}\\) metric is a color difference metric that is widely used in the industry to evaluate the color difference between two images. The lower the \\(\\Delta E_{00}\\) value, the better the color match between the two images. The Afifi approach is the one on the left; all other approaches are not deep learning based.\n\n\n\nMixed illuminants – \\(\\Delta E_{00}\\): Boxplots of the \\(\\Delta E_{00}\\) metric for white balancing methods against ground truth for corrected images of the MIPo dataset. Boxplot properties: The horizontal line inside the box is the median value. The height of the box is the interquartile range (IQR) between lower and upper quartile. The whiskers mark the minimum and maximum datapoints within the range of \\(1.5*\\text{IQR}\\) from the nearest (lower/upper) quartile. Outliers are datapoints exceeding this definition and are marked with diamond symbols."
  },
  {
    "objectID": "posts/whitebalance/index.html#code",
    "href": "posts/whitebalance/index.html#code",
    "title": "Mixed-illuminant whitebalance with GridNet",
    "section": "Code",
    "text": "Code\nHere is my code, that I wrote for the custom class AfifiRenderer. The first part defines two custom dictionary classes, that prevent us from using the model in a wrong way.\nThe second part now defines the AfifiRenderer class, that holds the model and the camera pipeline. The render method applies the model to the images and the save method saves the results to disk.\nHere is an example how this would be used:"
  },
  {
    "objectID": "posts/thesis/tables/results_table_1st_full.html",
    "href": "posts/thesis/tables/results_table_1st_full.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Method\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000"
  },
  {
    "objectID": "posts/soundscape-viz/index.html",
    "href": "posts/soundscape-viz/index.html",
    "title": "Soundscape Visualization App - Explore participants in a diverse dataset!",
    "section": "",
    "text": "Visualizing abstract data helps get an intuitive feel for the raw data that sometimes goes beyond summary statistics. In this post, I show how to visualize the Düsseldorf soundscape data set using Bokeh. Explore the app here!\n\n\n\nRadar charts of static person information.\n\n\nThis project was part of the Data Visialization course at Technische Hochschule Köln. The goal was to create an interactive visualization of the soundscape data set, that I helped collect during my time as a student research assistant at Hochschule Düsseldorf.\n\nData Set\nThe data set is published on Zenodo. It contains 6000+ soundscape rating from 100+ participants. The participants were asked to record and rate their soundscapes inside their private dwellings. Ther recordings are encoded to time-series of acoustic features and spectrograms to ensure privacy. However, the ratings, personal information, and situational context are included in the data set, which makes it a rich source for acoustic research.\n\n\nBokeh App\nYou can access the interactive app at the Posit Cloud. It allows you to explore the data set on an individual level.\nThe challenge with this app was, that I wanted to allow browsing through participants. We have to update the plots quickly, so we have to run a Bokeh server in the background. This makes deployment a bit more complicated, but the Posit Connect Cloud makes it easy to deploy Bokeh apps directly from the GitHub repository.\nFirst, I wanted to find a way to visualize the different personalities. I used a cluster of radar charts to show the personal factors in three fields: wellbeing, noise sensitivity, and trait. You can see an example of this at the top of this blog post.\nSecondly, person related meta data is visualized for example in the following bar plot to show composition of soundscape categories.\n\n\n\nBar charts of soundscape composition for a single participant.\n\n\nThe other part of the app visualizes the soundscape ratings directly as scatter plots. In the following example, we plot the ratings of personal valence against the ratings of personal arousal.\n\n\n\nScatter plot of personal arousal vs. valence at the time of soundscape ratings.\n\n\nThis participant rated themself as either very aroused or very calm, but never in between. On the other hand, the valence ratings are more evenly distributed. Also, we can see a cluster of points in the top right corner, which indicates that the participant rated themself as very aroused and very happy at the same time.\n\n\nProject Report\nThe full project report is published at researchgate.net."
  },
  {
    "objectID": "posts/flexcv/index.html",
    "href": "posts/flexcv/index.html",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "",
    "text": "flexcv is a Python package that implements flexible cross validation and machine learning for tabular data. It provides a range of features for comparing machine learning models on different datasets with different sets of predictors, customizing just about everything around cross validations. It supports both fixed and random effects, as well as random slopes.\nflexcv originated from the need to perform nested cross validation on grouped data. While I wrote most of the code during my time as a student research assistant at Hochschule Düsseldorf, I have continued to maintain and improve the package as a private open-source-project. Moreover, I decided to release it with full documentation, project page on GitHub pages, and continuous integration and deployment using GitHub Actions. Have a look at the project page!\nThe package is designed to be flexible and easy to use, with a focus on reproducibility and ease of use. It is built on top of popular machine learning libraries such as scikit-learn and pandas, and is designed to work seamlessly with these libraries.\nWhy would you need it? You can not simply perform a nested group cross validation with scikit-learn. This is where flexcv comes in. It provides a simple and flexible interface for performing nested cross validation on grouped data, and supports a wide range of machine learning models and evaluation metrics. It does a whole lot of other stuff, too, like scaling in the inner and outer CV independently and providing compatibility to neptune.ai for logging.\nThis project showcases my"
  },
  {
    "objectID": "posts/flexcv/index.html#poster",
    "href": "posts/flexcv/index.html#poster",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Poster",
    "text": "Poster\n\n\nThis conference poster was a contribution to the Jahrestagung für Akustik DAGA 2024 and is published here."
  },
  {
    "objectID": "posts/flexcv/index.html#installation",
    "href": "posts/flexcv/index.html#installation",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Installation",
    "text": "Installation\nflexcv is on PyPI, so you can install it using pip:\npip install flexcv"
  },
  {
    "objectID": "posts/flexcv/index.html#usage",
    "href": "posts/flexcv/index.html#usage",
    "title": "Flexible cross validation and machine learning for regression on tabular data",
    "section": "Usage",
    "text": "Usage\nHere is an example of how to use flexcv to perform nested cross validation on a regression problem. Let’s first load the modules and generate some sample data:\nNow the fun part about flexcv is its class interface. You can set up a complex configuration with just a few lines of code. Here is an example of how to set up a group cross validation with a linear model:\nI decided to use method chaining in the core interface, to make it easy to set up the configuration and perform the cross validation without having to remember a bunch of classes and functions. This approach leverages IDE hints and completion to guide the user through the process.\nVisit the project page to learn more. Feel free to reach out to me if you have any questions or suggestions, preferably using the issue tracker."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Recreating a temperature deviation chart for Wuppertal\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting bicycle traffic with a REST API using real-time weather data\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Bicycle Traffic in Cologne\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\n44 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty quantification for cross-validation (Master Thesis Write-up)\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible cross validation and machine learning for regression on tabular data\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSoundscape Visualization App - Explore participants in a diverse dataset!\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nMixed-illuminant whitebalance with GridNet\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n31 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Attribution-ShareAlike 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\n wiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More considerations\n for the public:\n wiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nAdditional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "Anbieter\nFabian Rosenthal\nc/o MDC Management#776\nWelserstraße 3\n87463 Dietmannsried\n\n\nKontakt\nE-Mail: radlfabs@tuta.io\nThreema: R5PREUEN\n\n\nVerantwortlich nach § 55 Abs.2 RStV\nFabian Rosenthal\nc/o MDC Management#776\nWelserstraße 3\n87463 Dietmannsried"
  },
  {
    "objectID": "datenschutz.html",
    "href": "datenschutz.html",
    "title": "Datenschutzerklärung",
    "section": "",
    "text": "Unser Hoster erhebt in sog. Logfiles folgende Daten, die Ihr Browser übermittelt: IP-Adresse, die Adresse der vorher besuchten Website (Referer Anfrage-Header), Datum und Uhrzeit der Anfrage, Zeitzonendifferenz zur Greenwich Mean Time, Inhalt der Anforderung, HTTP-Statuscode, übertragene Datenmenge, Website, von der die Anforderung kommt und Informationen zu Browser und Betriebssystem. Das ist erforderlich, um unsere Website anzuzeigen und die Stabilität und Sicherheit zu gewährleisten. Dies entspricht unserem berechtigten Interesse im Sinne des Art. 6 Abs. 1 S. 1 lit. f DSGVO. Es erfolgt kein Tracking und wir haben auf diese Daten keinen direkten Zugriff. Wir setzen für die Zurverfügungstellung unserer Website folgenden Hoster ein:\nGitHub Inc.\n88 Colin P Kelly Jr St\nSan Francisco, CA 94107\nUnited States\nDieser ist Empfänger Ihrer personenbezogenen Daten. Dies entspricht unserem berechtigten Interesse im Sinne des Art. 6 Abs. 1 S. 1 lit. f DSGVO, selbst keinen Server in unseren Räumlichkeiten vorhalten zu müssen. Serverstandort ist USA.\nWeitere Informationen zu Widerspruchs- und Beseitigungsmöglichkeiten gegenüber GitHub finden Sie unter: https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement#github-pages\nSie haben das Recht der Verarbeitung zu widersprechen. Ob der Widerspruch erfolgreich ist, ist im Rahmen einer Interessenabwägung zu ermitteln. Die Daten werden gelöscht, sobald der Zweck der Verarbeitung entfällt.\nDie Verarbeitung der unter diesem Abschnitt angegebenen Daten ist weder gesetzlich noch vertraglich vorgeschrieben. Die Funktionsfähigkeit der Website ist ohne die Verarbeitung nicht gewährleistet.\nGitHub hat Compliance-Maßnahmen für internationale Datenübermittlungen umgesetzt. Diese gelten für alle weltweiten Aktivitäten, bei denen GitHub personenbezogene Daten von natürlichen Personen in der EU verarbeitet. Diese Maßnahmen basieren auf den EU-Standardvertragsklauseln (SCCs). Weitere Informationen finden Sie unter: https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-data-protection-addendum#attachment-1–the-standard-contractual-clauses-processors"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Scroll down and join me on my journey.\n\n\n\n\n\n\n\n\n\n\nData Scientist \nWith a M.Sc. in Media Technology, I specialize in developing robust machine learning solutions that perform reliably across simple and critical applications.\n\n\n\n\n\nMy research on uncertainty quantification and cross-validation has equipped me with deep insights into performance estimation and statistical computing. This allows me to make data-driven decisions.\n\n\n\n\n\nIn my projects I like to incorporate my knowledge to build reliable solutions.\n\n\n\n\n\nIn this forecasting of bicycle demand in Cologne, I incorporated prediciton intervals to account for the uncertainty in the model. This gives more insights to the stakeholders and helps to make better decisions.\n\n\n\n\n\nIn my thesis at Fraunhofer MEVIS I trained 4+ million models to find appropriate methods to get confidence intervals for cross-validation estimates.\n\n\n\n\n\nThe complex study design allowed me to analyse the operation characteristics of statistical methods and to derive recommendations for the application in practice.\n\n\n\n\n\nI loved the development of my package for complex cross-validation workflows: flexcv. It uses a class interface for setting up the experiments.\n\n\n\n\n\nflexcv originated in my work at Hochschule Düsseldorf where I cross-validated a lot.\n\n\n\n\n\nIn my 3 years as a student research assistant I developed the ML pipelines that are used in a study on sound perception in private dwellings. I was also responsible for the processing and feature engineering of the 6000+ audio recordings of our study.\n\n\n\n\n\nA comprehensive study on feature selection including 2000+ audio features was part of my Bachelor thesis. I presented my work at DAGA 2022 in Stuttgart.\n\n\n\n\n\nDuring my studies in audio and video engineering, I transitioned to a more data focused path.\n\n\n\n\n\nLearning about machine learning in a project on music recommender systems at Hochschule Düsseldorf was a game changer for me and motivated me to pursue a career in data science.\n\n\n\n\n\n\n\nAnd on my journey I mastered a lot of languages, frameworks and tools.\n\n\n\n\n\n\nMy interdisciplinary background—spanning audio engineering and data science—enables me to approach complex challenges with a holistic, end-to-end perspective. I’m passionate about working in cross-functional teams and collaborating with domain experts to deliver impactful solutions.\n\n\n\n\n\n\n\n\n\n\n\nFind out more about me and my work on the portfolio page, read the resume and connect with me on Linkedin  or Github .  This page was created with Quarto and Closeread.\n\n\n\n\n\n\n\n\n\n\n\n\nLet \\(W\\) represent my work model \\[\nW = \\tau \\cdot m \\cdot ( b + P_i \\cdot \\gamma ) \\\\\n\\]\nObjective: \\(\\max_{m, \\gamma,P}\\)\nWhere:\n\n\\(m\\): Motivation with \\(m \\sim \\mathcal{U}[0.7, 1.0]\\)\n\\(b\\): Intercept denoting the engineering background\n\\(P_i\\): Matrix of \\(i\\) personal trait properties\n\\(\\gamma\\): Vector of learning rates corresponding to \\(P_i\\)\n\\(\\tau\\): Team spirit"
  },
  {
    "objectID": "index.html#hi-im-fabian-a-data-scientist-bridging-machine-learning-and-engineering.",
    "href": "index.html#hi-im-fabian-a-data-scientist-bridging-machine-learning-and-engineering.",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Scroll down and join me on my journey."
  },
  {
    "objectID": "index.html#fabian-rosenthal",
    "href": "index.html#fabian-rosenthal",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Data Scientist \nWith a M.Sc. in Media Technology, I specialize in developing robust machine learning solutions that perform reliably across simple and critical applications."
  },
  {
    "objectID": "mobile.html",
    "href": "mobile.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Hi👋, I’m Fabian, a data scientist bridging machine learning and engineering.\nWith a M.Sc. in Media Technology, I specialize in developing robust machine learning solutions that perform reliably across simple and critical applications. My research on uncertainty quantification and cross-validation has equipped me with deep insights into performance estimation and statistical computing. I love package development and open-sourced a Python package for complex cross-validation workflows.\nMy interdisciplinary background—spanning audio engineering and data science—enables me to approach complex challenges with a holistic, end-to-end perspective. I’m passionate about working in cross-functional teams and collaborating with domain experts to deliver impactful solutions."
  },
  {
    "objectID": "posts/bicycle/index.html",
    "href": "posts/bicycle/index.html",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "",
    "text": "A reliable forecast of the bicycle traffic is an important indicator for planning and optimizing the infrastructure of a city. In this post, I will predict the bicycle traffic in Cologne based on counting data from local bicycle counting stations.\nThis project was originally part of the course Machine Learning and Scientific Computing at TH Köln and I worked on it in collaboration with my friend and collegue Felix Otte. While Felix supervised the model trainings and the evaluation of the models, I was responsible for data cleaning, exploration, and visualization. For this writeup, I performed re-runs of the original trainings and added a new class-based implementation of the whole pipeline that is closer to something useful in production. As a newly added feature, I also included an implementation of prediction intervals using the mapie library.\nThis projects showcases the following skills:\nThis post is reproducible. Just follow along and execute the code chunks locally."
  },
  {
    "objectID": "posts/bicycle/index.html#evaluation",
    "href": "posts/bicycle/index.html#evaluation",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we can evaluate the tuned pipeline of transformers and models on the evaluation data of the yet unseen year 2022.\n\n\n\n\n\n\n\n\nMetrics on Holdout Data\n\n\nLocation\nAvg. Monthly Count\nRMSE\nMAPE\nbaseline\n\n\n\n\nNeumarkt\n126157\n11469\n0.08\nfalse\n\n\nVorgebirgswall\n92718\n11920\n0.09\nfalse\n\n\nHohenzollernbrücke\n73095\n11204\n0.1\nfalse\n\n\nHohe Pforte\n92102\n11028\n0.1\ntrue\n\n\nVenloer Straße\n167610\n23240\n0.11\nfalse\n\n\nA.-Silbermann-Weg\n83481\n10751\n0.11\nfalse\n\n\nNiederländer Ufer\n72004\n9990\n0.11\nfalse\n\n\nZülpicher Straße\n148239\n26699\n0.15\nfalse\n\n\nStadtwald\n74314\n15191\n0.16\nfalse\n\n\nNeusser Straße\n98155\n18376\n0.17\nfalse\n\n\nDeutzer Brücke\n142528\n35616\n0.19\nfalse\n\n\nBonner Straße\n79392\n18527\n0.19\nfalse\n\n\nSeverinsbrücke\n48116\n12039\n0.22\nfalse\n\n\nUniversitätsstraße\n133047\n35229\n0.23\nfalse\n\n\nVorgebirgspark\n31012\n10887\n0.25\nfalse\n\n\nA.-Schütte-Allee\n61827\n19031\n0.3\nfalse\n\n\nRodenkirchener Brücke\n52069\n18055\n0.37\nfalse\n\n\n\n\n\n\n        \n\n\n\nPlot the Predictions vs Actual Counts\nFirst, let’s start with some location that did really bad:\n\n\n                                                \n\n\n                                                \n\n\nWhile our model for Severinsbrücke seems to show the seasonal pattern, it is not able to predict the peaks with the correct magnitude. The model for Bonner Straße is not able to capture the trend at all. Let’s have a look at the best location, on the other hand:"
  },
  {
    "objectID": "posts/bicycle/index.html#extension-prediction-intervals",
    "href": "posts/bicycle/index.html#extension-prediction-intervals",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "Extension: Prediction Intervals",
    "text": "Extension: Prediction Intervals\nTo quantify the uncertainty in our predictions, we can use conformal prediction intervals. As a formal definition for general regression problems let’s assume that our training data \\((X,Y)=[(x_1,y_1),\\dots,(x_n,y_n)]\\) is drawn i.i.d. from an unknown \\(P_{X,Y}\\).Then the regression task is defined by \\(Y=\\mu(X)+\\epsilon\\) with our model \\(\\mu\\) and the residuals \\(\\epsilon_i ~ P_{Y|X}\\). We can define a nominal coverage level for the prediction intervals \\(1-\\alpha\\). So we want to get an interval \\(\\hat{C}_{n,a}\\) for each new prediction on the unseen feature vector \\(X_{n+1}\\) such that $\\(P[Y_{n+1}\\in \\hat{C}_{n,a}(X_{n+1})]\\ge 1-\\alpha\\) . We will use the MAPIE package to get performance intervals that are model agnostic. This is great, because we can plug in our existing pipeline and get the intervals with just a bit of wrapping. Now, there are different approaches to constructing prediction intervals. But since we are dealing with a timeseries, that violates the i.i.d assumption, we need to use ensemble batch prediction intervals (EnbPI). Read more about that here.\nWe will define a new class QuantileModels that inherits from BicyclePredictor and extends it with the necessary methods to fit the MapieTimeSeriesRegressorand predict the intervals. We will use the BlockBootstrap method to resample the data and get the intervals. We will also define a method to plot the intervals.\nThis code combines some of the examples here and here.\nNow, we can instantiate the MAPIE model and set some necessary attributes. We are basically transferring some optimization results to the new instance to not tune the models again. Instead, we are just fitting the underlying MAPIE time series models.\nNow let’s plot our new prediction intervals and their mean predictions for the location “Venloer Straße” for the test year 2022.\n\n\n                                                \n\n\nThe model is fairly uncertain, what the lower bound of bicycles counted is in the unseen time range. The upper bound at least includes all the actual counts. This is a good sign, but the model might be too conservative."
  },
  {
    "objectID": "posts/bicycle/index.html#extension-for-real-time-predictions",
    "href": "posts/bicycle/index.html#extension-for-real-time-predictions",
    "title": "Forecasting Bicycle Traffic in Cologne",
    "section": "Extension for Real-Time Predictions",
    "text": "Extension for Real-Time Predictions\nAn extension to this project allowing real-time predictions is described in this follow-up post. Here, I’ll use the Flask library to create a REST API that makes predictions using our pickled models. We will also use real-time weather data from the tomorrow.io API to get a inference feature vector."
  },
  {
    "objectID": "posts/realtime-bicycles/index.html",
    "href": "posts/realtime-bicycles/index.html",
    "title": "Predicting bicycle traffic with a REST API using real-time weather data",
    "section": "",
    "text": "This post extends the project on modelling bicycle traffic in Cologne. We will host a REST API with our trained models to allow real-time predictions by querying the tomorrow.io API. Additionally, we will build a REST API with Flask to serve our models. This will allow us to make predictions with our models in a real-time setting.\nThis post showcases my ability to: - Think end-to-end about a machine learning project - Use the tomorrow.io API to get real-time weather forecasts - Use Flask to use the models in a REST API."
  },
  {
    "objectID": "posts/realtime-bicycles/index.html#the-tomorrow.io-forecast-api",
    "href": "posts/realtime-bicycles/index.html#the-tomorrow.io-forecast-api",
    "title": "Predicting bicycle traffic with a REST API using real-time weather data",
    "section": "The tomorrow.io forecast API",
    "text": "The tomorrow.io forecast API\nIn order to use our models in a real-time production setting, we need to think of which features can be available at inference. As the bicycle counter data is quantized in time, we can not use this data to predict, e.g., the bicycle traffic of the next hour. This is clearly a limitation of the original project idea and data set. Moreover, our prediction pipeline is based on knowing the weather data.\nHere is an example, how we can get the weather data from the tomorrow.io API. You can get an API key with a free account. We will store the key in a .env file and use the python-dotenv package to load it. Don’t forget to commit .env to your .gitignore file. Let’s look at one time point from the forecast API."
  },
  {
    "objectID": "posts/realtime-bicycles/index.html#processing-the-forecast-data",
    "href": "posts/realtime-bicycles/index.html#processing-the-forecast-data",
    "title": "Predicting bicycle traffic with a REST API using real-time weather data",
    "section": "Processing the forecast data",
    "text": "Processing the forecast data\nSince the tomorrow.io API uses different feature names, we have to translate it, to be compatible with our data. However, a few columns will be missing. We can impute them by taking the mean of the training data. In practice, this will lead to worse predictions, but we will accept that for now to go through the whole process."
  },
  {
    "objectID": "posts/realtime-bicycles/index.html#using-the-models-in-a-rest-api",
    "href": "posts/realtime-bicycles/index.html#using-the-models-in-a-rest-api",
    "title": "Predicting bicycle traffic with a REST API using real-time weather data",
    "section": "Using the models in a REST API",
    "text": "Using the models in a REST API\nLet’s copy the files to another directory, where we use the models in a REST API. Flaskis a great choice to build a REST API with our trained models in a super simple way. We can follow the example provided by Muhammad Bilal Shinwari’s article on Medium (Code).\nWe will write two new files: app.py and client.py, the former to run the Flask app, the latter to post requests. Let’s assume, that the features are part of the request. In this way, clients can request predictions for time points of their liking. A possible other solution would be, that we assume, clients want alway predict the next possible time frame. Then we could move querying the tomorrow.io API to the Flask app. Here is what we need inside of app.py:\nAnd then client.py can look like this:"
  },
  {
    "objectID": "posts/realtime-bicycles/index.html#running-the-flask-app-and-the-client",
    "href": "posts/realtime-bicycles/index.html#running-the-flask-app-and-the-client",
    "title": "Predicting bicycle traffic with a REST API using real-time weather data",
    "section": "Running the Flask app and the client",
    "text": "Running the Flask app and the client\nNow, you can run the Flask app with python app.py and then run the client with python client.py. You should see the predictions in the console. The terminal will show something like this on the Flask side:\n* Serving Flask app 'app'\n * Debug mode: on\nINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:5000\nINFO:werkzeug:Press CTRL+C to quit\nINFO:werkzeug:Press CTRL+C to quit\nINFO:werkzeug: * Restarting with stat\nWARNING:werkzeug: * Debugger is active!\nINFO:werkzeug: * Debugger PIN: 248-803-927\nINFO:werkzeug:127.0.0.1 - - [10/Dec/2024 11:39:17] \"POST /predict HTTP/1.1\" 200 -\nINFO:werkzeug:127.0.0.1 - - [10/Dec/2024 11:39:50] \"POST /predict-mapie HTTP/1.1\" 200 -\nAnd the client will show the predictions:\n{'Prediction': 91762}\n{'PI': [75288, 171766], 'Prediction': 124490}\nSo this is an extremly easy and convenient way to use our custom models in a real-time setting, where the client doesn’t have to know how the model is implemented."
  },
  {
    "objectID": "posts/thesis/index.html",
    "href": "posts/thesis/index.html",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "",
    "text": "Cross-validation is a popular method to estimate the generalization performance of a machine learning model or algorithm. It also has its place in ML benchmarking experiments, where models or algorithms are compared based on their performance estimates. However, these estimates come with uncertainty, which is often neglected in practice. My master thesis explores different uncertainty quantification techniques for cross-validation estimates and evaluates their performance in terms of coverage and interval width. It is limited to the binary classification setting and focuses on the impact of sample size and machine learning algorithm choice on the performance of these techniques.\nI was extremely lucky to write this thesis with supervision of Dr. Max Westphal at Fraunhofer MEVIS, Bremen, and Prof. Dr. Beate Rhein from TH Köln. I totally enjoyed the process of developing this structure and writing this fairly mathematical thesis. Never have I thought I would choose such a topic when I failed 2 out of 3 math exams in my undergraduate studies. But here we are, and I am proud of the results. I hope you enjoy this write-up as much as I enjoyed writing the thesis."
  },
  {
    "objectID": "posts/thesis/index.html#introduction",
    "href": "posts/thesis/index.html#introduction",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Introduction",
    "text": "Introduction\nYou have estimated you model performance using cross-validation (CV). So CV tells you something like “Your model accuracy is 85%”. Great job! But what does this number actually mean? Can we expect to see 85% accuracy on unseen data every time or does the model’s performance vary on new data? And how much does it vary? These are important questions that are often neglected in practice. Depending on the domain, it is extremly important, to know your model’s limits (Volovici et al. 2022). Think of medical applications, where a false negative can have severe consequences. But also in other domains, where it’s not about life and death, it is important to know the limits of your model. A model that predicts stock prices with an accuracy of 85% is not very useful if the uncertainty in this estimate is too large. Then the (unknown) real accuracy could be much lower (or higher), and you would not know it. A way to notate uncertainty is to construct confidence intervals (CIs) for the performance estimate. CIs have a frequentist interpretation. If we repeat the experiment including the CI construction many times, we would find the true performance to fall in the CI in 95% of the cases (when we set our nominal level to 95%). In my thesis I have performed experiments really often to explore the coverage probability of methods that yield CIs. The methods were selected by passing the following criteria:\n\ncompatible with binary classification,\nno additional model fits necessary,\nno alterations of the CV procedure.\n\nAs a side quest, I learned how to deal with over 200M CV validation predictions!\n\nResearch Questions\nThe following research questions guided me through my experiments and shaped how I set up the data generating processes:\n\nWhich uncertainty quantification technique provides the most accurate coverage for CV estimates in terms of deviation from nominal coverage?\nHow does the sample size affect the coverage and interval widths of different uncertainty quantification methods, particularly for smaller sample sizes?\nTo what extent does the choice of machine learning algorithm influence the coverage performance of uncertainty quantification techniques?"
  },
  {
    "objectID": "posts/thesis/index.html#theoretical-background",
    "href": "posts/thesis/index.html#theoretical-background",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Theoretical Background",
    "text": "Theoretical Background\nFirst we have to make sure, that we’re talking about the same cross-validation scheme. There are variations and each of them can have stochastic implications, that do matter in this context.\n\n\\(k\\)-fold CV\n\nEvery observation is used once for validation\nMutually exclusive partitions\nIteratively use one partition for validation and the rest for training\nCV estimate is the average validation score\n\n\n\nProperties\nLet’s have a look at the key properties of CV that are important for the uncertainty quantification:\n\nCross-validation has an internal randomness due to the random splitting when \\(k_\\text{folds} &lt; n_\\text{obs}\\). When the number of folds equals the number of observations, we call it leave-one-out CV, and the estimate is deterministic.\nThere is an overlap of \\(\\frac{k-2}{k-1}\\) observations in 2 training sets of the cross-validation.\nAssuming model stability (Shalev-Shwartz and Ben-David 2014), i.e. when a slight change of the training data does not change the model’s performance significantly, the validation scores obtained from the model are not i.i.d (Bengio and Grandvalet 2004; Markatou et al. 2005; Bates, Hastie, and Tibshirani 2023).\nNo unbiased estimator for the variance of the CV estimate (Bengio and Grandvalet 2004).\n\n\n\n\nCovariance matrix of k-fold CV as in (Bengio and Grandvalet 2004)."
  },
  {
    "objectID": "posts/thesis/index.html#methods-ademp",
    "href": "posts/thesis/index.html#methods-ademp",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Methods: ADEMP",
    "text": "Methods: ADEMP\n\nWhat are we doing?\nIn short, we will compare the operating characteristics of different uncertainty quantification techniques for cross-validation estimates. We can observe the frerquentist properties of these methods by conducting a large number of simulations. Confidence intervals claim to include the “true” performance in a certain percentage of cases. We will simply count how often this holds and compare. That’s it, that’s the whole idea. Now, there are a few hidden challenges. For example, we have to come up with an efficient idea to calculate the “true” performance. I will talk about this in this part. Software-wise, I am using the great batchtools package for R to map data sets to machine learning algorithms and further to all the candidate methods. batchtools is a tremendous help when it comes to parallelizing tasks with so many factors. Actually, we will perform three separate studies sequentially to handle all the parts of this work.\nHere is an overview of the parts of the study before we dive into details: \nThere is a great framework that guides through simulation studies for comparing statistical methods and it’s called ADEMP (Morris, White, and Crowther 2019). ADEMP is a framework to plan, structure and report monte carlo simulation studies. In this ADEMP study I used pseudo random data to generate a large number of data sets. ADEMP also indicates, how to evaluate the coverage of the UQ techniques.\n\n\nA for AIMS\nThe aims in this study were guided by the research questions. We want to (1) find the best overall technique, (2) understand the influence of sample size and (3) understand the influence of learning algorithms.\n\n\nD for Data Generating Process\nNow it get’s more complex. It is important to describe how data is generated, so fellow readers can understand the intention and limitations. For my thesis I simulated a binary outcome from logistic models with std. normal covariates. In total, I used 12 different data generating processes (DGP) to cover a wide range of scenarios. Appropriate DGPs were determined by performing lots of pilot studies. Each DGP describes a distribution from which we can sample data sets. From each of the 12 DGPs I sampled data sets of sample sizes \\(n_\\text{obs} \\in \\{100, 500\\}\\). The number of covariates was varied. Some DGP also incorporated interaction terms to simulate non-linearities. I then used different learning algorithms inside of 5-fold cross-validation. The algorithms in use were Elastic Net (GLMNET), XGBoost and Support Vector Machine (SVM).\nAll these models have hyperparameters. When we tune these, we end up, with a distribution of performances that is skewed towards very good performances. This is why I instead sampled configurations randomly from appropriate search spaces. This way, we simulate a distribution of better or worse ML models, just as we would see in practice.\n\n\nE for Estimand\nA Monte Carlo simulation study is powerful, because we can know targets of inference by controlling the data generating process. The true performance can be approximated and is referred to as estimand. In my study, performance estimands were calculated for the AUC, accuracy, sensitivity and specificity metrics. From Bates, Hastie, and Tibshirani (2023) we know, that CV estimates are often close to the unconditional performance depending on training sets of size \\(n_\\text{train}\\). I will focus on that estimand in this post. Bates, Hastie, and Tibshirani (2023) is a great read and I totally recommend it to anyone interested in the topic. The paper says, that most often, the CV estimate is close to the performance of a algorithm trained on many training sets of size of the CV training folds drawn from the same population. So when we use this concept in the simulation, we either have to draw numerous data sets every time… or we can use the classifiers trained in other simulations (using the same DGP).\nIn order to approximate the study estimands, we draw a large population data set from each DGP. In practice, we would never have the possibility to generate such a population. This is why controlling the DGP in a simulation study is so awesome. The approximation of the unconditional performance is defined as follows: It’s the average performance of classifiers trained in the CV iterations in repeated simulations as evaluated on the population data set. Each simulation is performed with a different random seed, so the data sets are different but drawn from the same DGP.\nAs a formal definition, we can write the estimand as: \\[\n\\tilde{\\psi}_{n_\\text{train}}= \\frac{1}{n_\\text{sim}k_\\text{folds}n_\\text{pop}} \\sum_{s=1}^{n_\\text{sim}}  \\sum_{k=1}^{k_\\text{folds}} \\sum_{i=1}^{n_\\text{pop}} L(\\phi_{k,s}(\\mathbf{x}_{i}), y_{i})\n\\]\nwhere \\(L\\) is the loss function, \\(\\phi_{k,s}\\) is the classifier trained in the \\(k\\)-th fold of the \\(s\\)-th simulation and evaluated on the \\(i\\)-th observation of the population data set. \\(n_\\text{sim}\\) is the number of simulations, \\(n_\\text{pop}\\) is the number of observations in the population data set.\nSo we average over all classifiers trained in CV iterations belonging to the same DGP + sample size + learning algorithm combination. This keeps the number of trainings, draws and evaluations manageable. The estimand is then used to evaluate the performance of the UQ techniques.\n\n\nM for Methods\n\nParametric approaches\n\nWe can construct Wald-intervals by pooling validation predictions (Baseline). We use the standard error of the proportion to calculate the confidence intervals. We define these Wald-intervals as our baseline method. All other methods are compared to this baseline and more complex methods should outperform the baseline.\nApproximation of the correlation similar to Bengio and Grandvalet (2004) (Param-CA) \\[\n\\hat{\\rho} = \\left(\\frac{n_\\text{folds} - 2}{n_\\text{folds} - 1}\\right)^r\n\\] \\[\nr \\in \\{0.1, 0.2, 0.5, 1, 2\\}\n\\]\n\n\nVariants: different distributions and transformation (normal or t; logit transformation possible) \n\n\n\nBootstrapping techniques\n\nNaïve parametric bootstrap (Boot-naive) is similar to Wald intervals. We simply bootstrap the pooled validation predictions and calculate the confidence intervals every time.\nHierarchical bootstrap (Boot-hier) (Davison and Hinkley 1997; Goldstein 2010)\n\nSample with replacement from the fold indices –&gt;\nSample with replacement from the validation preds of the newly set of fold indices\nCalculate CV estimate on the new set of validation preds\n\n\n\nVariants: basic, normal, studentized, percentile\n\n\n\nGLMM\n\nModel fold information as random effect in a generalized linear mixed model (Eugster 2011): \\[\ng(\\mathbb{E}[Y|u_{k}])=Zu_{k}+\\epsilon_{k}\n\\]\n\nwhere \\(g(\\cdot)=\\ln(\\frac{p}{1-p})\\) is a link function, \\(Z\\) is the known random effects design matrix consisting of (validation) fold indices, \\(u_k\\) is the unknown random effect, and \\(\\epsilon_k\\) is an unknown vector of random errors, i.e., the residuals that are not explained by the model.\n\n\n\nP for Performance Measures\nFinally, we have to define how to measure the performance of the UQ techniques. Here are the key performance measures that we have to analyze: - Method failures (NA)\n\nCoverage probability\n\\(\\widehat{Cov} = \\frac{1}{n_\\text{sim}}\\sum_s 1(\\hat{\\theta}^l_s \\leq \\tilde{\\psi}_{n_\\text{train}} \\leq \\hat{\\theta}_s^u)\\)\nInterval width\n\\(\\hat{\\omega} = \\frac{1}{n_\\text{sim}}\\sum_s \\hat{\\theta}^u_s - \\hat{\\theta}^l_s\\)\nBoxplots can help identify general patterns of behaviour.\nMonte Carlo standard error (MCSE) indicate the precision of the Monte Carlo estimates. The more repetitions we can do, the better (smaller) the MCSE. We have to use a much smaller \\(n_\\text{sim}\\) for Boot and GLMM due to computational constraints. So we will see higher MCSE in these Monte Carlo estimates of coverage or width."
  },
  {
    "objectID": "posts/thesis/index.html#bottlenecks",
    "href": "posts/thesis/index.html#bottlenecks",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Bottlenecks",
    "text": "Bottlenecks\nAs the previous part indicates, we have to deal with numerous different conditions, data sets, and models. Here are some of the bottlenecks I encountered during my thesis:\n\nTime\n\nTuning the DGP in early development took a lot of time. I wanted to make sure, to have meaningful data scenarios included, so I tweaked the DGPs a lot. This was necessary to ensure, that the scenarios are neither too easy nor too hard for the ML models. Lastly, this determins the distribution of estimands and therefore, the generalizability of the results.\nComputation times of candidate methods: bootstrapping and especcially the Bayesian implementation of GLMMs are computationally expensive. I had to optimize the code a lot to make it run in a reasonable time frame. However, I could not spend as much study repetitions on these longer running methods. This shows in higher Monte Carlo standard errors for these methods. However, by pooling scenarios, we can lower the SE sufficiently.\nAs some of the methods have note been implemented officially yet, debugging of implementations took also quite some time.\n\n\n\nMemory\n\nThe size of data sets from the machine learningare huge. Specifically, it is not possible to store all predicitons on the large population data set. This makes it necessary to apply all metrics as early as possible. Unfortunately, we can not compute metrics afterwards if desired.\nWhen the ML part is done, we end up with a large data set. This is passed to the UQ part, where the candidate methods go to work. When the full study is parallelized, it’s no longer possible to load the data set into memory across all workers. Instead, we have to filter it for the relevant portion that are used in the part. This was achieved by leveraging the power of Apache Arrow and parquet files. They can be filtered before loading the ML data and are much faster to load than the original data set.\nLastly, the data with all intervals is huge. I almost entirely used data.table from here on. It allows super fast joins by reference. This came in handy, when I merged the UQ data set with the estimand data that was calculated before."
  },
  {
    "objectID": "posts/thesis/index.html#evaluation",
    "href": "posts/thesis/index.html#evaluation",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Evaluation",
    "text": "Evaluation\nWhile this does not show the frequentist properties, that I found, the following image luckily summarises my findings in a single case:\n\n\n\nExample coverage.\n\n\nWe can see, that the parametric methods yield too small intervals that don’t cover the estimand in this case. The GLMM methods cover, but have a width that is simply unusably wide. The bootstrap methods are just in between, often covering just right and having a width that is acceptable. The hierarchical bootstrap methods are the best in this case. The percentile and studentized versions are en par often.\nLet’s have a look at one of the many results tables in my thesis. This table shows results for the AUC metric. The average estimand and its standard deviation are shown in the grouping rows. The top part shows results for data sets of size 100, the bottom part of the table shows results for data sets of size 500. As you can see from the \\(n_\\text{sim}\\) column, I was able to perform a lot more Monte Carlo repetitions on the fast parametric methods as compared to bootstrap and GLMM. Note, how GLMM has at least nominal coverage but shows intervals that are extremly wide, on average. The baseline represents the naive Wald-intervals on the pooled validation predictions.\n\n\n\nMethod\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000\n\n\nAUC (0.75 ± 0.16) - \\(n_\\text{obs}\\) = 500\n\n\nGLMM-UG\n95.9 (0.19)\n0.312 (0.0017)\n11520\n\n\nParam-CA-0.1-stud\n92.2 (0.04)\n0.182 (0.0001)\n360006\n\n\nBoot-hier-stud\n91.3 (0.20)\n0.127 (0.0008)\n19155\n\n\nBaseline\n79.5 (0.07)\n0.085 (0.0001)\n360000\n\n\n\nThe method ‘GLMM-UG’ seems to do a great job at closing on the nominal coverage. However, the intervals are too wide to be useful. The hierarchical bootstrap methods are the best trade-off between coverage and width. The sample size is crucial for intervals with good coverage and width.\nTo get a more general idea of coverage and how it is dependent on the sample size and evaluation metric, we can look at the following boxplots: \nBoxplots for sample size vs. width also support the finding, that a larger sample size is crucial for intervals with useable widths: \nAcross all four metrics, we see improved (narrower) widths in almost all methods under test. This is congruent with what we would expect from the theory. Unfortunately, this means, that we can not always have reliable UQ for all metrics, when the sample size is too small.\n\nKey findings\nTo keep this short, here are the key findings of my thesis:\n\nBaseline is not sufficient (supported by Bates, Hastie, and Tibshirani (2023))\nPoor coverage for sensitivity and specificity (all methods).\nIssues in edge case where \\(n_\\text{obs}\\) = 100 and prevalence = 0.2. Here, the effective sample size in the validation folds is too low.\nSkewness of estimand distribution is a problem (supported by Reed (2007)).\nGLMM intervals are reliable but unusably wide.\nBootstrap: hierarchical shows good trade-off between coverage and width.\nOur correlation approximation is promising but lacks consistency across the scenarios.\nSample size is crucial for intervals with good coverage and width.\nCoverage on sensitivity and specificity are problematic."
  },
  {
    "objectID": "posts/thesis/index.html#future-research",
    "href": "posts/thesis/index.html#future-research",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "Future research",
    "text": "Future research\n\nBraschel et al. (2016): higher number of clusters in the data increases coverage.\nTest hierarchical bootstrap with different number of folds.\nRun hierarchical bootstrap with higher \\(n_\\text{sim}\\).\nFind hyperparameter strategy for the correlation approximation."
  },
  {
    "objectID": "posts/thesis/index.html#my-thesis-in-numbers",
    "href": "posts/thesis/index.html#my-thesis-in-numbers",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "My thesis in numbers",
    "text": "My thesis in numbers\nDuring the main simulation, 1440 scenarios (12 DGP \\(\\times\\) 2 sample sizes \\(\\times\\) 3 learning algorithms \\(\\times\\) 20 configurations) are repeated 500 times resulting in \\(720000\\) ML unique jobs. In each job, the learning algorithm was trained 5 times in the CV and once on the available data set. This corresponds to \\(4.32 \\times 10^6\\) model fits. All trained models are used to predict on the population data set, resulting in \\(1.728\\times 10^{11}\\) predictions on the population data set. The table of validation predictions from CV has \\(2.16\\times 10^8\\) rows. After running the uncertainty quantification methods, the results table containing all the confidence intervals has 74 million rows. The study ran on two nodes of a high performance computing cluster. The data.table, mlr3, batchtools, snow, and arrow packages were used to manage the extreme computational load and the memory demands during asynchronuous parallel processing."
  },
  {
    "objectID": "posts/thesis/index.html#references",
    "href": "posts/thesis/index.html#references",
    "title": "Uncertainty quantification for cross-validation (Master Thesis Write-up)",
    "section": "References",
    "text": "References\n\n\nBates, Stephen, Trevor Hastie, and Robert Tibshirani. 2023. “Cross-Validation: What Does It Estimate and How Well Does It Do It?” Journal of the American Statistical Association, May. https://www.tandfonline.com/doi/full/10.1080/01621459.2023.2197686.\n\n\nBengio, Yoshua, and Yves Grandvalet. 2004. “No Unbiased Estimator of the Variance of k-Fold Cross-Validation.” Journal of Machine Learning Research, no. 5.\n\n\nBraschel, Melissa C, Ivana Svec, Gerarda A Darlington, and Allan Donner. 2016. “A Comparison of Confidence Interval Methods for the Intraclass Correlation Coefficient in Community-Based Cluster Randomization Trials with a Binary Outcome.” Clinical Trials 13 (2): 180–87. https://doi.org/10.1177/1740774515606377.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\n\nEugster, Manuel J A. 2011. “Benchmark Experiments – a Tool for Analyzing Statistical Learning Algorithms.” PhD thesis, München: LMU München. https://edoc.ub.uni-muenchen.de/12990/1/Eugster_Manuel_J_A.pdf.\n\n\nGoldstein, Harvey. 2010. “Bootstrapping in Multilevel Models.” In Handbook of Advanced Multilevel Analysis. Routledge.\n\n\nMarkatou, Marianthi, Hong Tian, Shameek Biswas, and George Hripcsak. 2005. “Analysis of Variance of Cross-Validation Estimators of the Generalization Error.” Journal of Machine Learning Research 6 (39): 1127–68. http://jmlr.org/papers/v6/markatou05a.html.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2019. “Using Simulation Studies to Evaluate Statistical Methods.” Statistics in Medicine 38 (11): 2074–2102. https://doi.org/10.1002/sim.8086.\n\n\nReed, James F. III. 2007. “Better Binomial Confidence Intervals.” Journal of Modern Applied Statistical Methods 6 (May): 153–61. https://doi.org/10.56801/10.56801/v6.i.290.\n\n\nShalev-Shwartz, Shai, and Shai Ben-David. 2014. Understanding Machine Learning: From Theory to Algorithms. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9781107298019.\n\n\nVolovici, Victor, Nicholas L. Syn, Ari Ercole, Joseph J. Zhao, and Nan Liu. 2022. “Steps to Avoid Overuse and Misuse of Machine Learning in Clinical Research.” Nature Medicine 28 (10): 1996–99. https://doi.org/10.1038/s41591-022-01961-6."
  },
  {
    "objectID": "posts/thesis/tables/results_table_2nd_full.html",
    "href": "posts/thesis/tables/results_table_2nd_full.html",
    "title": "Fabian Rosenthal",
    "section": "",
    "text": "Method\n\\(\\widehat{Cov}\\) % (MCSE)\n\\(\\widehat{w}\\) (MCSE)\n\\(n_\\text{sim}\\)\n\n\n\n\nAUC (0.66 ± 0.14) - \\(n_\\text{obs}\\) = 100\n\n\nGLMM-UG\n98.1 (0.13)\n0.607 (0.0019)\n11520\n\n\nParam-CA-0.1-stud\n94.1 (0.04)\n0.496 (0.0003)\n360000\n\n\nBoot-hier-stud\n93.2 (0.20)\n0.336 (0.0012)\n16312\n\n\nBaseline\n77.5 (0.07)\n0.228 (0.0001)\n360000\n\n\nAUC (0.75 ± 0.16) - \\(n_\\text{obs}\\) = 500\n\n\nGLMM-UG\n95.9 (0.19)\n0.312 (0.0017)\n11520\n\n\nParam-CA-0.1-stud\n92.2 (0.04)\n0.182 (0.0001)\n360006\n\n\nBoot-hier-stud\n91.3 (0.20)\n0.127 (0.0008)\n19155\n\n\nBaseline\n79.5 (0.07)\n0.085 (0.0001)\n360000"
  },
  {
    "objectID": "posts/wupsi-weather/index.html",
    "href": "posts/wupsi-weather/index.html",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "",
    "text": "2024 will most likely be the first year to exceed 1.5°C above the pre-industrial average. To get a better understanding of the local temperature development, I decided to recreate a chart that shows the air temperature anomalies relative to a more recent period: 1991–2020. The chart that we will recreate is published by Copernicus Climate Change Sercive (C3S) here and looks like this:\nThe reference period for the pre-industrial age is typically 1850–1900. More information on this topic is available at C3S or in this post by Eli Wizevich.\nThe chosen C3S plot shows the deviation of the moving 12 months average temperature to the average temperature in the reference period 1991-2020. It uses a filled area plot with blue and red indicating averages below and above the reference temperature, respectively. The plot is themed minimalistic with a white background and only horizontal gridlines.\nSo to recreate this for the town I live in, we need to:"
  },
  {
    "objectID": "posts/wupsi-weather/index.html#general-information-on-wuppertal",
    "href": "posts/wupsi-weather/index.html#general-information-on-wuppertal",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "General information on Wuppertal",
    "text": "General information on Wuppertal\nBefore we dive into the code, here’s some information on Wuppertal: It’s is a town in the region Bergisches Land and is following the river Wupper. It is well known for its steep hills and the famous Schwebebahn, a suspension railway. The town is located in the state of North Rhine-Westphalia in Germany. A paper from Bergische Universität Wuppertal (2015) goes a bit into detail what makes the climate in Wuppertal special:\n\nAt the heights of the Bergisches Land region, moist Atlantic air masses meet an obstacle for the first time with the prevailing westerly air currents and are dammed up. As a result, the clouds rise into higher layers of air, which are usually colder, condense and rain down in the form of downpours. Around 1100 mm of precipitation is recorded in Elberfeld, rising to 1200 mm in Barmen/Oberbarmen."
  },
  {
    "objectID": "posts/wupsi-weather/index.html#packages",
    "href": "posts/wupsi-weather/index.html#packages",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "Packages",
    "text": "Packages\nFor getting the temperature data we’ll be using the meteostat package. The data wrangling can be performed in polars and we’ll use plotly to make interactive plots afterwards."
  },
  {
    "objectID": "posts/wupsi-weather/index.html#get-the-data",
    "href": "posts/wupsi-weather/index.html#get-the-data",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "Get the data",
    "text": "Get the data\nWe could get coordinates from the geopy package. In this case, for a single request, I found it quicker to just google it. We pass the coordinates to the Point class and then use it inside Daily to define the timespan for this location and the data frequency. For a quick overview we can just plot the data with the plot method.\nIn this post I simply load the pre-fetched data from a csv file, because I could not resolve a dependency conflict with the meteostat package in the context of my environment used for the website."
  },
  {
    "objectID": "posts/wupsi-weather/index.html#data-wrangling",
    "href": "posts/wupsi-weather/index.html#data-wrangling",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "Data wrangling",
    "text": "Data wrangling\nNext we move over to polars to do the data wrangling. We can throw away the time information and make a date column. Afterwards, we can apply our rolling average with a 12 month or 365 days window.\nTo not get totally confused, I did the reference calculations not in a chain but simply created a new object by subsetting the dataframe to the relevant time points. We also calculate the yearly average deviation to plot bars for each year. In the final assignment in this chunk, we split up the data into two columns, one for the positive deviations and one for the negative deviations. This makes it easier to plot different colors for the two cases."
  },
  {
    "objectID": "posts/wupsi-weather/index.html#plotting",
    "href": "posts/wupsi-weather/index.html#plotting",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "Plotting",
    "text": "Plotting\nNow comes the easy part. We just add two traces of go.Scatter to a go.Figure object. We could also add the yearly average deviation as bars. I decided to make this optional. You can click on the legend elements to view the bar plot. The update_yaxes method is used to add the °C to the y-axis ticks. In order to get a similar theme as in the original chart, we will use the “plotly_white” template and remove the horizontal gridlines with the update_xaxes method."
  },
  {
    "objectID": "posts/wupsi-weather/index.html#some-notes",
    "href": "posts/wupsi-weather/index.html#some-notes",
    "title": "Recreating a temperature deviation chart for Wuppertal",
    "section": "Some notes",
    "text": "Some notes\nCompared to the global data from the ERA5 reanalysis, Wuppertal shows longer periods of above-reference temperatures in the 2000s. This seems to be a bit earlier than the global average. However, the global chart shows high increase of temperature deviation from 2010 onwards, whereas Wuppertal saw a colder phase until around 2015. Spikes closing on 1.5°C were already noticeable in June 2007 and April 2019. March 2024 (corresponding to a 12 month moving average) was the first time point in the data at hand to exceed 1.5°C above the 1991-2020 average. While the 1980s were down to 2°C colder as the reference period, the colder as reference periods get slightly warmer over time (compare 1986, 1997, 2006, 2013)."
  }
]